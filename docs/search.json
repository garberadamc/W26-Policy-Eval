[
  {
    "objectID": "resources.html",
    "href": "resources.html",
    "title": "All Course Materials + Additional Readings & Resources Organized by Topic",
    "section": "",
    "text": "Lecture 1\nLecture 2\nLecture 3\nLecture 4\nLecture 5\nLecture 6\nLecture 7\nLecture 8\nLecture 9 (Part 1, Part 2)\nLecture 10\n\n\n\n\n\nLab 1: Generalize Fast! Replication of analyses from Buntaine et al., 2024\nLab 2: Code-along ü¶û ‚Üí Tables, Figures, & Lobsters (OH MY!)\nAdditional Lab - Illustration of ‚ÄòUnbiased Estimator‚Äô Using {gganimate} | View GIF\nWeek 8 - Lab\nWeek 9 - Lab Part 1\nWeek 9 - Lab Part 2\n\n\n\n\n\nAssignment 1: California Spiny Lobster Abundance (Panulirus Interruptus)\nAssignment 2: Investigating Causal Inference Designs Applied in Environmental Science\n\n\n\n\n\nFinal Project Instructions\nStudy Group Assignments\nFinal Presentation Grading Rubric\nPresentation Question Form"
  },
  {
    "objectID": "resources.html#all-course-materials",
    "href": "resources.html#all-course-materials",
    "title": "All Course Materials + Additional Readings & Resources Organized by Topic",
    "section": "",
    "text": "Lecture 1\nLecture 2\nLecture 3\nLecture 4\nLecture 5\nLecture 6\nLecture 7\nLecture 8\nLecture 9 (Part 1, Part 2)\nLecture 10\n\n\n\n\n\nLab 1: Generalize Fast! Replication of analyses from Buntaine et al., 2024\nLab 2: Code-along ü¶û ‚Üí Tables, Figures, & Lobsters (OH MY!)\nAdditional Lab - Illustration of ‚ÄòUnbiased Estimator‚Äô Using {gganimate} | View GIF\nWeek 8 - Lab\nWeek 9 - Lab Part 1\nWeek 9 - Lab Part 2\n\n\n\n\n\nAssignment 1: California Spiny Lobster Abundance (Panulirus Interruptus)\nAssignment 2: Investigating Causal Inference Designs Applied in Environmental Science\n\n\n\n\n\nFinal Project Instructions\nStudy Group Assignments\nFinal Presentation Grading Rubric\nPresentation Question Form"
  },
  {
    "objectID": "resources.html#applied-studies-referenced-in-eds-241---organized-by-topic",
    "href": "resources.html#applied-studies-referenced-in-eds-241---organized-by-topic",
    "title": "All Course Materials + Additional Readings & Resources Organized by Topic",
    "section": "Applied studies referenced in EDS 241 - organized by topic!",
    "text": "Applied studies referenced in EDS 241 - organized by topic!\n\nData science - general topics\nReproducibility\n\nLowndes et al., 2017 - Our path to better science in less time using open data science tools\nGehlman & Loken, 2013 - The garden of forking paths: Why multiple comparisons can be a problem, even when there is no ‚Äúfishing expedition‚Äù or ‚Äúp-hacking‚Äù\n\nData viz\n\nAnimated Visualizations - ‚ÄòR Psychologist‚Äô - Kristoffer Magnusson\nLarsen, Meng, & Kendall 2018 - Causal analysis in control‚Äìimpact ecological studies with observational data\n\n\n\nRegression\nOrdinary Least Squares (OLS)\nGeneralized Linear Models (GLMs)\n\nCarruthers et al., 2008 - Generalized linear models: model selection, diagnostics, and overdispersion\n\n\n\nCausal inference - Econometrics\nCausal inference research conducted at Bren & UCSB\n\nSarah Anderson - Monitoring ‚Üí Compliance\nMark Buntaine - Citizen participation ‚Üí Env. Policy\nJoan Dudney - Climate change ‚Üí Infectious tree disease\nLeah Stokes - Voting ‚Üí Climate policy\nOlivier Deschenes - Climate change ‚Üí Agricultural output\nMatto Mildenberger - Wildfires ‚Üí Voting\nKyle Meng - Marine Reserves, Inequality, Ground water rights\n\n\n\nExperimental Design\n\nBuntaine et al., 2024 - Social competition drives collective action to reduce informal waste burning in Uganda\n\nEDS 241 - Glossary of Econometric and Statistical Terms\nJ-PAL power calculation tool\nEGAP power calculation tool\n\n\n\n\nDifference-in-Difference Design\n\nMoland et al., 2013 - Lobster and cod benefit from small-scale northern marine protected areas: inference from an empirical before‚Äì after control-impact study\n\n\n\nRegression Discontinuity Design\n\n\n\n\n\n\n\nInstrumental Variable Design"
  },
  {
    "objectID": "course-materials/lecture-slides/lecture1.1-slides.html#title-slide",
    "href": "course-materials/lecture-slides/lecture1.1-slides.html#title-slide",
    "title": "EDS 241",
    "section": "",
    "text": "EDS 123: Lecture 1\nToday‚Äôs topic here\n\nWeek 1 | January 6th, 2025"
  },
  {
    "objectID": "course-materials/lecture-slides/lecture1.1-slides.html#welcome",
    "href": "course-materials/lecture-slides/lecture1.1-slides.html#welcome",
    "title": "EDS 241",
    "section": "",
    "text": "Welcome to EDS 123!\n\n\nThis is the default text size\n\n\n\n\nI can apply a class selector from meds-slides-styles.scss to change the size of this text\n\n\n\nSimilarly, I can apply a class selector(s) to modify the appearance of a subset of text\n\nCheck out this demo presentation for examples of all the cool capabilities of Revealjs"
  },
  {
    "objectID": "course-materials/lecture-slides/lecture1.1-slides.html#block-level-styling",
    "href": "course-materials/lecture-slides/lecture1.1-slides.html#block-level-styling",
    "title": "EDS 241",
    "section": "",
    "text": "You can also apply styles to blocks of content\n\nWe can use divs to apply styling to all content within the div gates (:::):\n::: {.selector1 .selector2}\nSome content\n:::\n\nFor example:\n\nThis text is bolded\nThis text is italicized"
  },
  {
    "objectID": "course-materials/lecture-slides/lecture1.1-slides.html#embedded-code",
    "href": "course-materials/lecture-slides/lecture1.1-slides.html#embedded-code",
    "title": "EDS 241",
    "section": "",
    "text": "Here is some embedded & rendered code\n\n\nlibrary(palmerpenguins)\nlibrary(ggplot2)\n\nggplot(data = penguins, aes(x = bill_length_mm, y = bill_depth_mm, color = species)) +\n  geom_point()"
  },
  {
    "objectID": "course-materials/lecture-slides/lecture1.1-slides.html#rendered-code",
    "href": "course-materials/lecture-slides/lecture1.1-slides.html#rendered-code",
    "title": "EDS 241",
    "section": "",
    "text": "Alternatively, we can render just the output"
  },
  {
    "objectID": "course-materials/lecture-slides/lecture1.1-slides.html#columns",
    "href": "course-materials/lecture-slides/lecture1.1-slides.html#columns",
    "title": "EDS 241",
    "section": "",
    "text": "You can arrange content in columns\n\n\n\nLearn more about the MEDS program on the Bren website."
  },
  {
    "objectID": "course-materials/week5.html",
    "href": "course-materials/week5.html",
    "title": "Week 5:",
    "section": "",
    "text": "Lecture slides\nResources\n\n\n\n\nLecture (TUES): Regression Discontinuity Designs\n\n\n\nLecture (THURS): RDD (Part 2)"
  },
  {
    "objectID": "course-materials/week5.html#lecture-materials",
    "href": "course-materials/week5.html#lecture-materials",
    "title": "Week 5:",
    "section": "",
    "text": "Lecture slides\nResources\n\n\n\n\nLecture (TUES): Regression Discontinuity Designs\n\n\n\nLecture (THURS): RDD (Part 2)"
  },
  {
    "objectID": "course-materials/week5.html#assignment-reminders",
    "href": "course-materials/week5.html#assignment-reminders",
    "title": "Week 5:",
    "section": " Assignment Reminders",
    "text": "Assignment Reminders\n\n\n\n\n\n\n\n\n\nAssignment Type\nAssignment Title\nDate Assigned\nDate Due\n\n\n\n\nReading - Mastering ‚ÄôMetrics\nA&P: Chapter 4\n01/29\n02/05\n\n\nReading - Forest Protection Study\nNeal, 2024\n01/29\n02/05"
  },
  {
    "objectID": "course-materials/week5.html#extra-resources",
    "href": "course-materials/week5.html#extra-resources",
    "title": "Week 5:",
    "section": " Extra resources",
    "text": "Extra resources\nCreate {gganimate} plot - Illustration of concept unbiased estimator:\n\n\nRepo - unbiased-estimator-viz\n\n\nExtra credit: Create an animated plot that illustrates a concept relevant to EDS 241 (i.e, causal inference, statistics, design-based methods). Take a look at the examples made by Kristoffer Magnusson for inspiration!\n\n\nCool animations - visualize statistical concepts: ‚ÄòR Psychologist‚Äô - Kristoffer Magnusson\nReproducibility: Lowndes et al., 2017 - Our path to better science in less time using open data science tools"
  },
  {
    "objectID": "course-materials/week6.html",
    "href": "course-materials/week6.html",
    "title": "Week 6: Instrumental Variable Design",
    "section": "",
    "text": "Lecture slides\nResources\n\n\n\n\nLecture (TUES): Instrumental Variables - Part 1\n\n\n\nLecture (THURS):"
  },
  {
    "objectID": "course-materials/week6.html#lecture-materials",
    "href": "course-materials/week6.html#lecture-materials",
    "title": "Week 6: Instrumental Variable Design",
    "section": "",
    "text": "Lecture slides\nResources\n\n\n\n\nLecture (TUES): Instrumental Variables - Part 1\n\n\n\nLecture (THURS):"
  },
  {
    "objectID": "course-materials/week6.html#assignment-reminders",
    "href": "course-materials/week6.html#assignment-reminders",
    "title": "Week 6: Instrumental Variable Design",
    "section": " Assignment Reminders",
    "text": "Assignment Reminders\n\n\n\n\n\n\n\n\n\nAssignment Type\nAssignment Title\nDate Assigned\nDate Due\n\n\n\n\nReading - Mastering ‚ÄôMetrics\nA&P: Chapter 3 (Instrumental Variables)\n02/09\n012/11\n\n\nReading - Focus on Instrumental Variable Analysis\nStokes, 2015\n02/09\n012/11"
  },
  {
    "objectID": "course-materials/week6.html#extra-resources",
    "href": "course-materials/week6.html#extra-resources",
    "title": "Week 6:",
    "section": " Extra resources",
    "text": "Extra resources\n\nRDD Lab (optional): Handout - Estimating Regression Discontinuity , RDD Lab Repository"
  },
  {
    "objectID": "course-materials/week3.html",
    "href": "course-materials/week3.html",
    "title": "Week 3: Difference-in-Difference",
    "section": "",
    "text": "Lecture slides\nResources\n\n\n\n\nLecture (TUES): Difference-in-Difference\n , \n\n\nLecture (THURS): M. Buntaine Experiment\n ,"
  },
  {
    "objectID": "course-materials/week3.html#lecture-materials",
    "href": "course-materials/week3.html#lecture-materials",
    "title": "Week 3: Difference-in-Difference",
    "section": "",
    "text": "Lecture slides\nResources\n\n\n\n\nLecture (TUES): Difference-in-Difference\n , \n\n\nLecture (THURS): M. Buntaine Experiment\n ,"
  },
  {
    "objectID": "course-materials/week3.html#assignment-reminders",
    "href": "course-materials/week3.html#assignment-reminders",
    "title": "Week 3: Difference-in-Difference",
    "section": " Assignment Reminders",
    "text": "Assignment Reminders\n\n\n\n\n\n\n\n\n\nAssignment Type\nAssignment Title\nDate Assigned\nDate Due\n\n\n\n\nFinish Reading Study\nBuntaine et al.¬†(2024) , ADD HERE -&gt; Qs for Prof.¬†Buntaine\n01/16\n01/22\n\n\nReading - Intro. to Natural Experiments\nLarsen et al., 2019\n01/16\n01/22\n\n\nReading - Mastering ‚ÄôMetrics\nA&P: Chapter 5\n01/16\n01/22"
  },
  {
    "objectID": "course-materials/week3.html#extra-resources",
    "href": "course-materials/week3.html#extra-resources",
    "title": "Week 3: Difference-in-Difference",
    "section": " Extra resources",
    "text": "Extra resources\n\nVideo - Interpreting regression coefficients & interactions - Assignment 1"
  },
  {
    "objectID": "course-materials/week1.html",
    "href": "course-materials/week1.html",
    "title": "Week 1:",
    "section": "",
    "text": "Welcome to the page for all things Week 1!"
  },
  {
    "objectID": "course-materials/week1.html#lecture-materials",
    "href": "course-materials/week1.html#lecture-materials",
    "title": "Week 1:",
    "section": " Lecture Materials",
    "text": "Lecture Materials\n\n\n\n\n\n\n\nLecture slides\nResources\n\n\n\n\nLecture 1 (TUE): Intro. to Casual Inference\n\n\n\nLecture 2 (THURS): Potenital Outcomes"
  },
  {
    "objectID": "course-materials/week1.html#assignment-reminders",
    "href": "course-materials/week1.html#assignment-reminders",
    "title": "Week 1:",
    "section": " Assignment Reminders",
    "text": "Assignment Reminders\n\n\n\n\n\n\n\n\n\nAssignment Type\nAssignment Title\nDate Assigned\nDate Due\n\n\n\n\nAssignment 1\nHandout: HW1-LOBSTRS , Repository: HW1-LOBSTRS\n01/08\n01/17\n\n\nMini-Assignment\nTopics of Interest - Spreadsheet (Add Topic/Study Here)\n01/08\n01/12\n\n\nReading - Mastering ‚ÄôMetrics\nA&P: Introduction & Chapter 1\n01/02\n01/07\n\n\nReading - Applied Policy Study\nPages 1-3: Buntaine et al.¬†(2024) , Glossary of Econometrics & Statistics Terms\n01/02\n01/07\n\n\nIn Class (Thursday)\nReading Quiz\n01/08\n01/08\n\n\nWeek 2 - Reading - Mastering ‚ÄôMetrics\nA&P: Chapter 2\n01/08\n01/15\n\n\nWeek 2 - Reading - Generalized Linear Models\nCarruthers et al., 2008\n01/13\n01/13\n\n\n\n\n\n\n\n\n\n\nAssignment Submission and Rubric\n\n\n\n\nSubmission: Submit assignments through GradeScope\nQuestion-specific guide: Each question has detailed grading criteria available on GradeScope. Be sure to review these guidelines before submitting your work!\nGeneral rubric outline: For an overview of the grading criteria, refer to the assignment rubric - general outline"
  },
  {
    "objectID": "course-materials/week1.html#discussion-materials",
    "href": "course-materials/week1.html#discussion-materials",
    "title": "Week 1:",
    "section": " Extra Resources",
    "text": "Extra Resources\n\n\nKey Concepts (videos)\nThese short videos that accompany Mastering ‚ÄôMetrics are a great resource for Chapter-1. The style is a bit extra, but they explain the key concepts clearly. If any of these concepts are unclear, I strongly recommend watching them (they may be relevant to the reading quiz tomorrow):\n\nCeteris Paribus & Counterfactuals - 6 minutes\nOmmitted Variable Bias & Selection Bias - 9 minutes\nExperimental Design & Random Assignment - 10 minutes\nHow to Read Econometrics Papers - 12 minutes\n\nReferences\n\nGelman, A., & Loken, E. (2013). The garden of forking paths: Why multiple comparisons can be a problem, even when there is no ‚Äúfishing expedition‚Äù or ‚Äúp-hacking‚Äù and the research hypothesis was posited ahead of time. Department of Statistics, Columbia University, 348(1-17), 3.\nBuntaine, M. T., Komakech, P., & Shen, S. V. (2024). Social competition drives collective action to reduce informal waste burning in Uganda. Proceedings of the National Academy of Sciences, 121(23), e2319712121."
  },
  {
    "objectID": "course-materials/week9.html",
    "href": "course-materials/week9.html",
    "title": "Week 9:",
    "section": "",
    "text": "Lecture slides\nLab\nAnswer key\n\n\n\n\nRepository - W9-Lab-EDS241\nRDD Lab Handout\n\n\n\n\nFixed Effects Lab Handout"
  },
  {
    "objectID": "course-materials/week9.html#lecture-materials",
    "href": "course-materials/week9.html#lecture-materials",
    "title": "Week 9:",
    "section": "",
    "text": "Lecture slides\nLab\nAnswer key\n\n\n\n\nRepository - W9-Lab-EDS241\nRDD Lab Handout\n\n\n\n\nFixed Effects Lab Handout"
  },
  {
    "objectID": "course-materials/week9.html#assignment-reminders",
    "href": "course-materials/week9.html#assignment-reminders",
    "title": "Week 9:",
    "section": " Assignment Reminders",
    "text": "Assignment Reminders\n\n\n\n\n\n\n\n\n\nAssignment Type\nAssignment Title\nDate Assigned\nDate Due\n\n\n\n\nEnd of Class Survey\nEOC-Week9\n03/05/2025\n03/05/2025, 11:55pm\n\n\nReading - Article\nNeal, 2024 - Estimating the Effectiveness of Forest Protection Using Regression Discontinuity\n03/05/2025"
  },
  {
    "objectID": "course-materials/week9.html#extra-resources",
    "href": "course-materials/week9.html#extra-resources",
    "title": "Week 9:",
    "section": " Extra resources",
    "text": "Extra resources"
  },
  {
    "objectID": "index.html#course-description",
    "href": "index.html#course-description",
    "title": "Environmental Policy Evaluation (EDS241 / ESM244)",
    "section": "Course Description",
    "text": "Course Description\nThis course will present state of the art program evaluation techniques necessary to evaluate the impact of environmental policies. The program evaluation methods presented will aim at identifying and measuring the causal effect of policies, regulations, and interventions on environmental outcomes of interest. Students will learn the research designs and methods for estimating causal effects with experimental and non-experimental data. This will prepare the students for interpreting and conducting high-quality empirical research, with applications in cross-sectional data and panel data settings."
  },
  {
    "objectID": "index.html#teaching-team",
    "href": "index.html#teaching-team",
    "title": "Environmental Policy Evaluation (EDS241 / ESM244)",
    "section": "Teaching Team",
    "text": "Teaching Team\n\n\n\n\n\n\nInstructor\n\n\n\n\n\n\n\n\n\n\n\nAdam Garber\nEmail: agarber@ucsb.edu\nOffice hours (subject to change):\nTuesday 12-2pm @ BH 4329 (Manzanita Room)\nAlternate instructor availability:\nMonday - Friday (by appointment)\n\n\n\n\n\n\nCo- Instructor\n\n\n\n\n\n\n\n\n\n\n\nAnnie Adams\nEmail: aradams@ucsb.edu\nOffice hours (subject to change):\nThursday 3 pm - 4 pm @ BH 3418 (Office)"
  },
  {
    "objectID": "index.html#meeting-time-location",
    "href": "index.html#meeting-time-location",
    "title": "Environmental Policy Evaluation (EDS241 / ESM244)",
    "section": "Meeting Time & Location",
    "text": "Meeting Time & Location\n\nTime\nTuesday & Thursday, 8:00-9:15 am\n\n\n\nLocation\nBren Hall 1414"
  },
  {
    "objectID": "index.html#acknowledgements",
    "href": "index.html#acknowledgements",
    "title": "Environmental Policy Evaluation (EDS241 / ESM244)",
    "section": "Acknowledgements",
    "text": "Acknowledgements\nThis website was developed from a template authored by Sam Csik. The EDS241/ESM244 course website houses materials which are heavily reused, adapted from, and inspired by Sam Csik‚Äôs original work. This website was made using Quarto and the repository used to make this website can be found here."
  },
  {
    "objectID": "assignments.html",
    "href": "assignments.html",
    "title": "Assignments",
    "section": "",
    "text": "Important: Email group presentation slides to instructor (Adam) before 7am on presentation day! (3/19)\n\n\n\n\nPresentation slides must be emailed to your instructor (Adam) before 7am (3/19)\nAcceptable submission formats include google slides or a shared link of the slides in pdf format (i.e., a Google Drive link)\nIf slides are available in advance please send early!\n\n\n\n\n\n\nFinal Project Instructions\nStudy Group Assignments\nFinal Presentation Grading Rubric\nPresentation Question Form\n\n\n\n\n\n\n\n\n\n\nPresentations will start at 8am sharp (please do not be late)\n\n\n\nPresentation order will follow the numerical order of your assigned group study.\n\n\n\nFirst 4 presentations (~60 minutes)\nShort break (~5-10 minutes)\nNext 3 presentations (~45 minutes)\nShort break (~5-10 minutes)\nFinal 3 presentations (~45 minutes)\n\nüçø Snacks are encouraged! Bring food for sustenance/survival or to share with the class! (totally optional)"
  },
  {
    "objectID": "assignments.html#final-project-links",
    "href": "assignments.html#final-project-links",
    "title": "Assignments",
    "section": "",
    "text": "Final Project Instructions\nStudy Group Assignments\nFinal Presentation Grading Rubric\nPresentation Question Form"
  },
  {
    "objectID": "assignments.html#presentation-day-schedule-8-11am-wednesday-319",
    "href": "assignments.html#presentation-day-schedule-8-11am-wednesday-319",
    "title": "Assignments",
    "section": "",
    "text": "Presentations will start at 8am sharp (please do not be late)\n\n\n\nPresentation order will follow the numerical order of your assigned group study.\n\n\n\nFirst 4 presentations (~60 minutes)\nShort break (~5-10 minutes)\nNext 3 presentations (~45 minutes)\nShort break (~5-10 minutes)\nFinal 3 presentations (~45 minutes)\n\nüçø Snacks are encouraged! Bring food for sustenance/survival or to share with the class! (totally optional)"
  },
  {
    "objectID": "course-materials/labs/lab1.1.html",
    "href": "course-materials/labs/lab1.1.html",
    "title": "Lab 1.1 TEMPLATE",
    "section": "",
    "text": "# load packages ----"
  },
  {
    "objectID": "course-materials/labs/lab1.1.html#setup",
    "href": "course-materials/labs/lab1.1.html#setup",
    "title": "Lab 1.1 TEMPLATE",
    "section": "",
    "text": "# load packages ----"
  },
  {
    "objectID": "course-materials/labs/lab1.1.html#tidy-data-review",
    "href": "course-materials/labs/lab1.1.html#tidy-data-review",
    "title": "Lab 1.1 TEMPLATE",
    "section": "Tidy Data Review",
    "text": "Tidy Data Review\nExample untidy / wide data:\n\n# create some untidy temperature data ----\n\n# print it out ----\n\nUsing pivot_longer() to ‚Äúlengthen‚Äù / tidy our data:\n\n# convert data from wide &gt; long ----\n\n# print it out ----"
  },
  {
    "objectID": "course-materials/week8.html",
    "href": "course-materials/week8.html",
    "title": "Week 8:",
    "section": "",
    "text": "Lecture slides\nLab\nAnswer key\n\n\n\n\nRepository - W8-Lab-EDS241\nLab-Handout\nRMD Updated!"
  },
  {
    "objectID": "course-materials/week8.html#lecture-materials",
    "href": "course-materials/week8.html#lecture-materials",
    "title": "Week 8:",
    "section": "",
    "text": "Lecture slides\nLab\nAnswer key\n\n\n\n\nRepository - W8-Lab-EDS241\nLab-Handout\nRMD Updated!"
  },
  {
    "objectID": "course-materials/week8.html#assignment-reminders",
    "href": "course-materials/week8.html#assignment-reminders",
    "title": "Week 8:",
    "section": " Assignment Reminders",
    "text": "Assignment Reminders\n\n\n\n\n\n\n\n\n\nAssignment Type\nAssignment Title\nDate Assigned\nDate Due\n\n\n\n\nEnd of Class Survey\nEOC-Week8\n02/26/2025\n02/26/2025, 11:55pm\n\n\nReading - Article\nStatistical matching for conservation science - Schleicher et al., 2019 -\n02/16/2025\n\n\n\nReview chapter - Mastering Metrics\nChapter 3 - Instrumental Variables\n02/26/2025"
  },
  {
    "objectID": "course-materials/discussion-materials/week1-discussion-slides.html#title-slide",
    "href": "course-materials/discussion-materials/week1-discussion-slides.html#title-slide",
    "title": "EDS 241",
    "section": "",
    "text": "EDS 240: Discussion 1\nToday‚Äôs topic here\n\nWeek 1 | January 7th, 2024"
  },
  {
    "objectID": "course-materials/discussion-materials/week1-discussion-slides.html#slide-slug-here",
    "href": "course-materials/discussion-materials/week1-discussion-slides.html#slide-slug-here",
    "title": "EDS 241",
    "section": "",
    "text": "Slide title"
  },
  {
    "objectID": "course-materials/week2.html",
    "href": "course-materials/week2.html",
    "title": "Week 2:",
    "section": "",
    "text": "Lecture slides\nResources\n\n\n\n\nLecture (TUES): Generalized Linear Models\n , \n\n\nLecture (THURS): Modeling Count Outcomes"
  },
  {
    "objectID": "course-materials/week2.html#lecture-materials",
    "href": "course-materials/week2.html#lecture-materials",
    "title": "Week 2:",
    "section": "",
    "text": "Lecture slides\nResources\n\n\n\n\nLecture (TUES): Generalized Linear Models\n , \n\n\nLecture (THURS): Modeling Count Outcomes"
  },
  {
    "objectID": "course-materials/week2.html#assignment-reminders",
    "href": "course-materials/week2.html#assignment-reminders",
    "title": "Week 2:",
    "section": " Assignment Reminders",
    "text": "Assignment Reminders\n\n\n\n\n\n\n\n\n\nAssignment Type\nAssignment Title\nDate Assigned\nDate Due\n\n\n\n\nMini-Assignment\nTopics of Interest - Spreadsheet (Add Topic/Study Here)\n01/08\n01/12\n\n\nReading - Mastering ‚ÄôMetrics\nA&P: Chapter 2\n01/08\n01/15\n\n\nReading - Generalized Linear Models\nCarruthers et al., 2008\n01/13\n01/13\n\n\nIn Class (Thursday)\nReading Quiz\n01/15\n01/15\n\n\nAssignment 1\nHandout: HW1-LOBSTRS , Repository: HW1-LOBSTRS\n01/08\n01/17\n\n\n\n\n\n\n\n\n\n\nTipAssignment Submission and Rubric\n\n\n\n\nSubmission: Submit assignments through GradeScope\nQuestion-specific guide: Each question has detailed grading criteria available on GradeScope. Be sure to review these guidelines before submitting your work!\nGeneral rubric outline: For an overview of the grading criteria, refer to the assignment rubric - general outline"
  },
  {
    "objectID": "course-materials/week7.html",
    "href": "course-materials/week7.html",
    "title": "Week 7:",
    "section": "",
    "text": "Lecture slides\nResources\n\n\n\n\nLecture (TUES):\n\n\n\nLecture (THURS):"
  },
  {
    "objectID": "course-materials/week7.html#lecture-materials",
    "href": "course-materials/week7.html#lecture-materials",
    "title": "Week 7:",
    "section": "",
    "text": "Lecture slides\nResources\n\n\n\n\nLecture (TUES):\n\n\n\nLecture (THURS):"
  },
  {
    "objectID": "course-materials/week7.html#assignment-reminders",
    "href": "course-materials/week7.html#assignment-reminders",
    "title": "Week 7:",
    "section": " Assignment Reminders",
    "text": "Assignment Reminders\n\n\n\n\n\n\n\n\n\nAssignment Type\nAssignment Title\nDate Assigned\nDate Due\n\n\n\n\nReading -\n\n\n\n\n\nReading - Mastering ‚ÄôMetrics\nA&P:"
  },
  {
    "objectID": "course-materials/week4.html",
    "href": "course-materials/week4.html",
    "title": "Week 4: Fixed Effects & Matching Methods",
    "section": "",
    "text": "Lecture slides\nResources\n\n\n\n\nLecture (TUES): Fixed Effects + Matching\n\n\n\nLecture (THURS): Matching/FE (Lab Exercise)"
  },
  {
    "objectID": "course-materials/week4.html#lecture-materials",
    "href": "course-materials/week4.html#lecture-materials",
    "title": "Week 4: Fixed Effects & Matching Methods",
    "section": "",
    "text": "Lecture slides\nResources\n\n\n\n\nLecture (TUES): Fixed Effects + Matching\n\n\n\nLecture (THURS): Matching/FE (Lab Exercise)"
  },
  {
    "objectID": "course-materials/week4.html#assignment-reminders",
    "href": "course-materials/week4.html#assignment-reminders",
    "title": "Week 4: Fixed Effects & Matching Methods",
    "section": " Assignment Reminders",
    "text": "Assignment Reminders\n\n\n\n\n\n\n\n\n\nAssignment Type\nAssignment Title\nDate Assigned\nDate Due\n\n\n\n\nAssignment 2\nHandout: HW2-Matching-DiD , Repository: HW2-Matching-DiD\n01/26\n02/04\n\n\nReading - Matching in Env. Sci.\nSchleicher et al., 2019\n01/26\n01/29\n\n\nReading - Review ‚ÄúWithin Estimator‚Äù Section of Article\nLarsen et al., 2019\n01/26\n01/29"
  },
  {
    "objectID": "course-materials/keys/key1.1.html",
    "href": "course-materials/keys/key1.1.html",
    "title": "Lab 1.1 KEY",
    "section": "",
    "text": "# load packages ----\nlibrary(tidyverse)\nlibrary(palmerpenguins)"
  },
  {
    "objectID": "course-materials/keys/key1.1.html#setup",
    "href": "course-materials/keys/key1.1.html#setup",
    "title": "Lab 1.1 KEY",
    "section": "",
    "text": "# load packages ----\nlibrary(tidyverse)\nlibrary(palmerpenguins)"
  },
  {
    "objectID": "course-materials/keys/key1.1.html#tidy-data-review",
    "href": "course-materials/keys/key1.1.html#tidy-data-review",
    "title": "Lab 1.1 KEY",
    "section": "Tidy Data Review",
    "text": "Tidy Data Review\nExample untidy / wide data:\n\n# create some untidy temperature data ----\ntemp_data_wide &lt;- tribble(\n  ~date, ~station1, ~station2,  ~station3,\n  \"2023-10-01\", 30.1, 29.8,  31.2,\n  \"2023-11-01\", 28.6, 29.1,  33.4,\n  \"2023-12-01\", 29.9, 28.5,  32.3\n)\n\n# print it out ----\nprint(temp_data_wide)\n\n# A tibble: 3 √ó 4\n  date       station1 station2 station3\n  &lt;chr&gt;         &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;\n1 2023-10-01     30.1     29.8     31.2\n2 2023-11-01     28.6     29.1     33.4\n3 2023-12-01     29.9     28.5     32.3\n\n\nUsing pivot_longer() to ‚Äúlengthen‚Äù / tidy our data:\n\n# convert data from wide &gt; long ----\ntemp_data_long &lt;- temp_data_wide %&gt;%  \n  pivot_longer(cols = starts_with(\"station\"),\n               names_to = \"station_id\",\n               values_to = \"temp_c\")\n\n# print it out ----\nprint(temp_data_long)\n\n# A tibble: 9 √ó 3\n  date       station_id temp_c\n  &lt;chr&gt;      &lt;chr&gt;       &lt;dbl&gt;\n1 2023-10-01 station1     30.1\n2 2023-10-01 station2     29.8\n3 2023-10-01 station3     31.2\n4 2023-11-01 station1     28.6\n5 2023-11-01 station2     29.1\n6 2023-11-01 station3     33.4\n7 2023-12-01 station1     29.9\n8 2023-12-01 station2     28.5\n9 2023-12-01 station3     32.3"
  },
  {
    "objectID": "course-materials/week10.html",
    "href": "course-materials/week10.html",
    "title": "Week 10:",
    "section": "",
    "text": "Lecture slides\nLab\nAnswer key\n\n\n\n\nWeek 10 Slides (Updated after class)"
  },
  {
    "objectID": "course-materials/week10.html#lecture-materials",
    "href": "course-materials/week10.html#lecture-materials",
    "title": "Week 10:",
    "section": "",
    "text": "Lecture slides\nLab\nAnswer key\n\n\n\n\nWeek 10 Slides (Updated after class)"
  },
  {
    "objectID": "course-materials/week10.html#assignment-reminders",
    "href": "course-materials/week10.html#assignment-reminders",
    "title": "Week 10:",
    "section": " Assignment Reminders",
    "text": "Assignment Reminders\n\n\n\n\n\n\n\n\n\nAssignment Type\nAssignment Title\nDate Assigned\nDate Due\n\n\n\n\n\nReminder: Please fill out the EDS 241 course evaluations. ¬†No EOC survey this week!\n03/12/2025\n03/12/2025, 11:55pm\n\n\nReading - Article\nGertler, 2004 - High Impact Randomized Experiment - PROGRESA Intervention\n03/12/2025"
  },
  {
    "objectID": "course-materials/resources.html",
    "href": "course-materials/resources.html",
    "title": "All Course Materials + Additional Readings & Resources Organized by Topic",
    "section": "",
    "text": "Lecture 1\nLecture 2\nLecture 3\nLecture 4\nLecture 5\nLecture 6\nLecture 7\nLecture 8\nLecture 9 (Part 1, Part 2)\nLecture 10\n\n\n\n\n\nLab 1: Generalize Fast! Replication of analyses from Buntaine et al., 2024\nLab 2: Code-along ü¶û ‚Üí Tables, Figures, & Lobsters (OH MY!)\nAdditional Lab - Illustration of ‚ÄòUnbiased Estimator‚Äô Using {gganimate} | View GIF\nWeek 8 - Lab\nWeek 9 - Lab Part 1\nWeek 9 - Lab Part 2\n\n\n\n\n\nAssignment 1: California Spiny Lobster Abundance (Panulirus Interruptus)\nAssignment 2: Investigating Causal Inference Designs Applied in Environmental Science\n\n\n\n\n\nFinal Project Instructions\nStudy Group Assignments\nFinal Presentation Grading Rubric\nPresentation Question Form"
  },
  {
    "objectID": "course-materials/resources.html#all-course-materials",
    "href": "course-materials/resources.html#all-course-materials",
    "title": "All Course Materials + Additional Readings & Resources Organized by Topic",
    "section": "",
    "text": "Lecture 1\nLecture 2\nLecture 3\nLecture 4\nLecture 5\nLecture 6\nLecture 7\nLecture 8\nLecture 9 (Part 1, Part 2)\nLecture 10\n\n\n\n\n\nLab 1: Generalize Fast! Replication of analyses from Buntaine et al., 2024\nLab 2: Code-along ü¶û ‚Üí Tables, Figures, & Lobsters (OH MY!)\nAdditional Lab - Illustration of ‚ÄòUnbiased Estimator‚Äô Using {gganimate} | View GIF\nWeek 8 - Lab\nWeek 9 - Lab Part 1\nWeek 9 - Lab Part 2\n\n\n\n\n\nAssignment 1: California Spiny Lobster Abundance (Panulirus Interruptus)\nAssignment 2: Investigating Causal Inference Designs Applied in Environmental Science\n\n\n\n\n\nFinal Project Instructions\nStudy Group Assignments\nFinal Presentation Grading Rubric\nPresentation Question Form"
  },
  {
    "objectID": "course-materials/resources.html#applied-studies-referenced-in-eds-241---organized-by-topic",
    "href": "course-materials/resources.html#applied-studies-referenced-in-eds-241---organized-by-topic",
    "title": "All Course Materials + Additional Readings & Resources Organized by Topic",
    "section": "Applied studies referenced in EDS 241 - organized by topic!",
    "text": "Applied studies referenced in EDS 241 - organized by topic!\n\nData science - general topics\nReproducibility\n\nLowndes et al., 2017 - Our path to better science in less time using open data science tools\nGehlman & Loken, 2013 - The garden of forking paths: Why multiple comparisons can be a problem, even when there is no ‚Äúfishing expedition‚Äù or ‚Äúp-hacking‚Äù\n\nData viz\n\nAnimated Visualizations - ‚ÄòR Psychologist‚Äô - Kristoffer Magnusson\nLarsen, Meng, & Kendall 2018 - Causal analysis in control‚Äìimpact ecological studies with observational data\n\n\n\nRegression\nOrdinary Least Squares (OLS)\nGeneralized Linear Models (GLMs)\n\nCarruthers et al., 2008 - Generalized linear models: model selection, diagnostics, and overdispersion\n\n\n\nCausal inference - Econometrics\nCausal inference research conducted at Bren & UCSB\n\nSarah Anderson - Monitoring ‚Üí Compliance\nMark Buntaine - Citizen participation ‚Üí Env. Policy\nJoan Dudney - Climate change ‚Üí Infectious tree disease\nLeah Stokes - Voting ‚Üí Climate policy\nOlivier Deschenes - Climate change ‚Üí Agricultural output\nMatto Mildenberger - Wildfires ‚Üí Voting\nKyle Meng - Marine Reserves, Inequality, Ground water rights\n\n\n\nExperimental Design\n\nBuntaine et al., 2024 - Social competition drives collective action to reduce informal waste burning in Uganda\n\nEDS 241 - Glossary of Econometric and Statistical Terms\nJ-PAL power calculation tool\nEGAP power calculation tool\n\n\n\n\nDifference-in-Difference Design\n\nMoland et al., 2013 - Lobster and cod benefit from small-scale northern marine protected areas: inference from an empirical before‚Äì after control-impact study\n\n\n\nRegression Discontinuity Design\n\n\n\n\n\n\n\nInstrumental Variable Design"
  },
  {
    "objectID": "course-materials/assignments.html",
    "href": "course-materials/assignments.html",
    "title": "Assignments",
    "section": "",
    "text": "Important: Email group presentation slides to instructor (Adam) before 7am on presentation day! (3/19)\n\n\n\n\nPresentation slides must be emailed to your instructor (Adam) before 7am (3/19)\nAcceptable submission formats include google slides or a shared link of the slides in pdf format (i.e., a Google Drive link)\nIf slides are available in advance please send early!\n\n\n\n\n\n\nFinal Project Instructions\nStudy Group Assignments\nFinal Presentation Grading Rubric\nPresentation Question Form\n\n\n\n\n\n\n\n\n\n\nPresentations will start at 8am sharp (please do not be late)\n\n\n\nPresentation order will follow the numerical order of your assigned group study.\n\n\n\nFirst 4 presentations (~60 minutes)\nShort break (~5-10 minutes)\nNext 3 presentations (~45 minutes)\nShort break (~5-10 minutes)\nFinal 3 presentations (~45 minutes)\n\nüçø Snacks are encouraged! Bring food for sustenance/survival or to share with the class! (totally optional)"
  },
  {
    "objectID": "course-materials/assignments.html#final-project-links",
    "href": "course-materials/assignments.html#final-project-links",
    "title": "Assignments",
    "section": "",
    "text": "Final Project Instructions\nStudy Group Assignments\nFinal Presentation Grading Rubric\nPresentation Question Form"
  },
  {
    "objectID": "course-materials/assignments.html#presentation-day-schedule-8-11am-wednesday-319",
    "href": "course-materials/assignments.html#presentation-day-schedule-8-11am-wednesday-319",
    "title": "Assignments",
    "section": "",
    "text": "Presentations will start at 8am sharp (please do not be late)\n\n\n\nPresentation order will follow the numerical order of your assigned group study.\n\n\n\nFirst 4 presentations (~60 minutes)\nShort break (~5-10 minutes)\nNext 3 presentations (~45 minutes)\nShort break (~5-10 minutes)\nFinal 3 presentations (~45 minutes)\n\nüçø Snacks are encouraged! Bring food for sustenance/survival or to share with the class! (totally optional)"
  },
  {
    "objectID": "index.html#section",
    "href": "index.html#section",
    "title": "Environmental Policy Evaluation (EDS241 / ESM244)",
    "section": "",
    "text": "////////   $$$$$$$$   ||\\      /|        @@@@@@     ^      ^    ^      ^   \n  /          $          || \\    / |       @      @    ^      ^    ^      ^   \n  /          $          ||  \\  /  |              @    ^      ^    ^      ^   \n  /          $          ||   \\/  ||             @     ^      ^    ^      ^   \n  ////////   $$$$$$$$   ||       ||            @      ^^^^^^^^    ^^^^^^^^   \n  /                 $   ||       ||           @              ^           ^   \n  /                 $   ||       ||          @               ^           ^   \n  /                 $   ||       ||         @                ^           ^   \n   ////////   $$$$$$$$   ||       ||       @@@@@@@            ^           ^"
  },
  {
    "objectID": "course-materials/labs/week1.html",
    "href": "course-materials/labs/week1.html",
    "title": "Lab 1",
    "section": "",
    "text": "There are many different ways to create tidy tables in R. You might be familiar with the kable function from {knitr} that creates tables for rectangular data. Kable tables don‚Äôt have a ton of flexibility, but are great at producing clean, simple tables. As we move into creating tables for the many different statistical models we will learn in this course, we will need to move beyond a simple kable table. That is where {gt} comes in! A {gt} table allows for the following structure, making it ideal for displaying different statistical outcomes.\n\n\n\n\n\n\n\n\n\nWe are going to use our class survey data to create some tables!\n\n# Load packages\nlibrary(tidyverse)\nlibrary(gtsummary)\nlibrary(gt)\nlibrary(janitor)\n\n\n\n# Read in class survey data and split into two random groups\nclass_data &lt;- read_csv(\"https://raw.github.com/garberadamc/W26-Policy-Eval/main/course-materials/labs/data/W26_class_survey.csv\") %&gt;%\n  mutate(random_groups = sample(rep(c(\"control\", \"treatment\"), each = n()/2)))\n\nLet‚Äôs create a base table that we will use with both kable and gt.\n\n# Create the base summary object\nbalance_summary &lt;- class_data %&gt;%\n  gtsummary::tbl_summary(\n    # Select how you want to group columns\n    by = random_groups,\n    # Variables to include\n    include = c(height, pets, dominant_hand, fav_number), \n    # Display mean and standard deviation for all continuous variables\n    statistic = list(all_continuous() ~ \"{mean} ({sd})\")\n  ) %&gt;% \n  # Add p value\n  add_p()\n\nbalance_summary\n\n\n\n\n\n\n\n\n\n\n\n\n\nCharacteristic\ncontrol\nN = 171\ntreatment\nN = 171\np-value2\n\n\n\n\nheight\n66.4 (4.2)\n63.5 (3.1)\n0.022\n\n\npets\n8 (47%)\n14 (82%)\n0.031\n\n\ndominant_hand\n\n\n\n\n0.6\n\n\n¬†¬†¬†¬†Left\n3 (18%)\n1 (5.9%)\n\n\n\n\n¬†¬†¬†¬†Right\n14 (82%)\n16 (94%)\n\n\n\n\nfav_number\n\n\n\n\n0.9\n\n\n¬†¬†¬†¬†2\n1 (5.9%)\n4 (24%)\n\n\n\n\n¬†¬†¬†¬†3\n2 (12%)\n2 (12%)\n\n\n\n\n¬†¬†¬†¬†4\n3 (18%)\n1 (5.9%)\n\n\n\n\n¬†¬†¬†¬†5\n3 (18%)\n2 (12%)\n\n\n\n\n¬†¬†¬†¬†6\n1 (5.9%)\n1 (5.9%)\n\n\n\n\n¬†¬†¬†¬†7\n4 (24%)\n4 (24%)\n\n\n\n\n¬†¬†¬†¬†8\n1 (5.9%)\n1 (5.9%)\n\n\n\n\n¬†¬†¬†¬†9\n2 (12%)\n1 (5.9%)\n\n\n\n\n¬†¬†¬†¬†10\n0 (0%)\n1 (5.9%)\n\n\n\n\n\n1 Mean (SD); n (%)\n\n\n2 Wilcoxon rank sum test; Pearson‚Äôs Chi-squared test; Fisher‚Äôs exact test\n\n\n\n\n\n\n\n\nNow lets output our balance_summary with a kable table!\n\nbalance_summary %&gt;%\n  as_kable_extra(caption = \"Class Survey Balance Table\") %&gt;% \n  kableExtra::kable_styling(\n    bootstrap_options = c(\"striped\", \"condensed\", \"hover\")\n  )\n\n\nClass Survey Balance Table\n\n\n\n\n\n\n\n\nCharacteristic\ncontrol\nN = 17\ntreatment\nN = 17\np-value\n\n\n\n\nheight\n66.4 (4.2)\n63.5 (3.1)\n0.022\n\n\npets\n8 (47%)\n14 (82%)\n0.031\n\n\ndominant_hand\n\n\n0.6\n\n\nLeft\n3 (18%)\n1 (5.9%)\n\n\n\nRight\n14 (82%)\n16 (94%)\n\n\n\nfav_number\n\n\n0.9\n\n\n2\n1 (5.9%)\n4 (24%)\n\n\n\n3\n2 (12%)\n2 (12%)\n\n\n\n4\n3 (18%)\n1 (5.9%)\n\n\n\n5\n3 (18%)\n2 (12%)\n\n\n\n6\n1 (5.9%)\n1 (5.9%)\n\n\n\n7\n4 (24%)\n4 (24%)\n\n\n\n8\n1 (5.9%)\n1 (5.9%)\n\n\n\n9\n2 (12%)\n1 (5.9%)\n\n\n\n10\n0 (0%)\n1 (5.9%)\n\n\n\n\n1 Mean (SD); n (%)\n\n\n\n\n\n2 Wilcoxon rank sum test; Pearson's Chi-squared test; Fisher's exact test\n\n\n\n\n\n\n\n\n\nIt looks fine‚Ä¶ But we can make it a lot nicer with {gt}!\n\n# Convert our balance summary table to a gt table\nbalance_summary %&gt;%\n  as_gt() %&gt;%\n  \n  # Add a Title and Subtitle\n  tab_header(\n    title = \"Class Survey Balance Table\",\n    subtitle = \"With Randomly Assigned Groups\"\n  ) %&gt;%\n  \n  # Add a Spanner to group the data columns\n  tab_spanner(\n    label = \"Randomized Groups\",\n    columns = c(stat_1, stat_2)\n  ) %&gt;%\n  \n  # Change column labels\n  cols_label(\n    label = \"Variable\",\n    p.value = \"P-Value\"\n  ) %&gt;%\n  \n  # Add a source note at the bottom\n  tab_source_note(\n    source_note = \"Note: Data from the Winter 2026 Class Survey.\"\n  )\n\n\n\n\n\n\n\n\n\n\n\n\n\nClass Survey Balance Table\n\n\nWith Randomly Assigned Groups\n\n\nVariable\n\nRandomized Groups\n\nP-Value2\n\n\ncontrol\nN = 171\ntreatment\nN = 171\n\n\n\n\nheight\n66.4 (4.2)\n63.5 (3.1)\n0.022\n\n\npets\n8 (47%)\n14 (82%)\n0.031\n\n\ndominant_hand\n\n\n\n\n0.6\n\n\n¬†¬†¬†¬†Left\n3 (18%)\n1 (5.9%)\n\n\n\n\n¬†¬†¬†¬†Right\n14 (82%)\n16 (94%)\n\n\n\n\nfav_number\n\n\n\n\n0.9\n\n\n¬†¬†¬†¬†2\n1 (5.9%)\n4 (24%)\n\n\n\n\n¬†¬†¬†¬†3\n2 (12%)\n2 (12%)\n\n\n\n\n¬†¬†¬†¬†4\n3 (18%)\n1 (5.9%)\n\n\n\n\n¬†¬†¬†¬†5\n3 (18%)\n2 (12%)\n\n\n\n\n¬†¬†¬†¬†6\n1 (5.9%)\n1 (5.9%)\n\n\n\n\n¬†¬†¬†¬†7\n4 (24%)\n4 (24%)\n\n\n\n\n¬†¬†¬†¬†8\n1 (5.9%)\n1 (5.9%)\n\n\n\n\n¬†¬†¬†¬†9\n2 (12%)\n1 (5.9%)\n\n\n\n\n¬†¬†¬†¬†10\n0 (0%)\n1 (5.9%)\n\n\n\n\n\n1 Mean (SD); n (%)\n\n\n2 Wilcoxon rank sum test; Pearson‚Äôs Chi-squared test; Fisher‚Äôs exact test\n\n\nNote: Data from the Winter 2026 Class Survey.\n\n\n\n\n\n\n\n\nThe {gt} table looks a lot cleaner! Let‚Äôs move on with creating some more {gt} tables!\nWe are going to use data from the Moland et al.¬†2013 study on Lobster MPAS.\nThe data we will be working with has the following variables:\n\n\n\n\n\n\n\n\nVariable\nData Type\nDescriptions\n\n\n\n\nyear\nNumeric (5-levels)\nYears measured from 2006 to 2010\n\n\nregion\nCharacter (3-levels)\nbol= Bol√¶rne , kve = Kvernskj√¶r , flo = Fl√∏devigen\n\n\ntreat\nCharacter (2-levels)\nmpa = treatment , con = control\n\n\ncpue\nNumeric\nCatch per unit effort\n\n\n\nLet‚Äôs read in our data to get started!\n\nlobsters &lt;- read_csv(\"https://raw.github.com/garberadamc/Lab2-EDS241-Moland13/main/data/moland13_lobsters.csv\")\n\nWe will start with creating a table for the total CPUE for each year and region.\nTo create a table with a column for each region, we need to untidy our data! We will do so by pivoting our data into wide format, with a column for each region, and the CPU for each year in that specific region.\nThis will be a 2 way table, since we are displaying data for two variables.\n\ntbl_2way &lt;- lobsters %&gt;%\n  \n  # Calculate total cpue for each year/region\n  group_by(year, region) %&gt;%\n  summarize(\n      total_cpue = sum(cpue, na.rm = TRUE),\n            .groups = \"drop\") %&gt;% # Same as `ungroup()`\n  \n  # Pivot to create column for each region\n  pivot_wider(\n      names_from = region, \n      values_from = total_cpue) %&gt;%\n  arrange(year) %&gt;%\n  \n  # Add a row for total cpue \n  adorn_totals(\"row\")\n\nTime to use {gt} to make this into a nice looking table!!\n\ntbl_2way %&gt;% \n  gt(rowname_col = \"year\") %&gt;%\n  tab_header(\n    title = \"European Lobster Catch by Region and Year\",\n    subtitle = \"Total Catch Per Unit Effort (CPUE) by year and region\") %&gt;% \n  cols_label(\n    bol = \"Bol√¶rne\",\n    flo = \"Kvernskj√¶r\",\n    kve = \"Fl√∏devigen\") %&gt;%\n  tab_source_note(\n      \"Source: Moland et al., 2013\")\n\n\n\n\n\n\n\nEuropean Lobster Catch by Region and Year\n\n\nTotal Catch Per Unit Effort (CPUE) by year and region\n\n\n\nBol√¶rne\nKvernskj√¶r\nFl√∏devigen\n\n\n\n\n2006\n127\n122\n177\n\n\n2007\n269\n93\n276\n\n\n2008\n249\n151\n367\n\n\n2009\n484\n168\n466\n\n\n2010\n463\n175\n449\n\n\nTotal\n1592\n709\n1735\n\n\n\nSource: Moland et al., 2013\n\n\n\n\n\n\n\n\nLet‚Äôs now add our treat variable into the table, so we can see how lobster catch varied within our control and MPA groups. This will be a 3 way table!\n\ntbl_3way &lt;- lobsters %&gt;%\n  # Calculate total cpue for each year/region/treatment group\n  group_by(year, region, treat) %&gt;%\n  summarize(total_cpue = sum(cpue, na.rm = TRUE),\n            .groups = \"drop\") %&gt;%\n  \n  # Pivot to create column for each trt/ control group within each region\n  pivot_wider(names_from = c(region, treat), values_from = total_cpue) %&gt;%\n  \n  arrange(year)\n\nTime to use {gt} to make this into a nice looking table!!\n\nfancy_table &lt;- tbl_3way %&gt;%\n  gt(rowname_col = \"year\") %&gt;%\n  tab_header(\n    title = \"European Lobster Catch by Year, Region and Treatment\",\n    subtitle = \"Total Catch Per Unit Effort (CPUE)\"\n  ) %&gt;%\n  tab_spanner(\n    label = \"Bol√¶rne\",\n    columns = c(\"bol_con\", \"bol_mpa\")\n  ) %&gt;%\n  tab_spanner(\n    label = \"Fl√∏devigen\",\n    columns = c(\"flo_con\", \"flo_mpa\")\n  ) %&gt;%\n  tab_spanner(\n    label = \"Kvernskj√¶r\",\n    columns = c(\"kve_con\", \"kve_mpa\")\n  ) %&gt;%\n  cols_label(\n    bol_con = \"Control\",\n    bol_mpa = \"MPA\",\n    flo_con = \"Control\",\n    flo_mpa = \"MPA\",\n    kve_con = \"Control\",\n    kve_mpa = \"MPA\")\n\nfancy_table\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nEuropean Lobster Catch by Year, Region and Treatment\n\n\nTotal Catch Per Unit Effort (CPUE)\n\n\n\n\nBol√¶rne\n\n\nFl√∏devigen\n\n\nKvernskj√¶r\n\n\n\nControl\nMPA\nControl\nMPA\nControl\nMPA\n\n\n\n\n2006\n52\n75\n54\n68\n125\n52\n\n\n2007\n98\n171\n33\n60\n114\n162\n\n\n2008\n78\n171\n55\n96\n178\n189\n\n\n2009\n187\n297\n51\n117\n244\n222\n\n\n2010\n148\n315\n64\n111\n198\n251\n\n\n\n\n\n\n\n\n\nWe can add plots within our table as well! While maybe not completely necessary in this instance, it can be a helpful tool to have!\n\ntable_w_plots &lt;- lobsters %&gt;%\n  # Calculate total cpue for each year\n  group_by(year) %&gt;%\n  summarize(\n      total_cpue = sum(cpue, na.rm = TRUE),\n      dist_cpue = list(cpue),\n      .groups = \"drop\") %&gt;% \n  arrange(year) %&gt;% \n  # Create gt table\n    gt() %&gt;% \n     tab_header(\n    title = \"European Lobster Catch Totals and Distribution (2006-2010)\",\n    subtitle = \"Total Catch Per Unit Effort (CPUE)\") %&gt;% \n    cols_label(\n    year = \"Year\",\n    total_cpue = \"Total CPUE\",\n    dist_cpue = \"Density CPUE\") %&gt;%\n    # Add in line density plots\n    gtExtras::gt_plt_dist( \n        dist_cpue,\n        type = \"density\", \n        line_color = \"blue\", \n        fill_color = \"red\")\n\n\ntable_w_plots \n\n\n\n\n\n\n\n\n\n\n\n\nEuropean Lobster Catch Totals and Distribution (2006-2010)\n\n\nTotal Catch Per Unit Effort (CPUE)\n\n\nYear\nTotal CPUE\nDensity CPUE\n\n\n\n\n2006\n426\n\n\n\n   \n\n\n\n2007\n638\n\n\n\n   \n\n\n\n2008\n767\n\n\n\n   \n\n\n\n2009\n1118\n\n\n\n   \n\n\n\n2010\n1087\n\n\n\n   \n\n\n\n\n\n\n\n\n\n#install.packages(\"praise\")\n#install.packages(\"cowsay\")\n#install.packages(\"beepr\")\nlibrary(praise)\nlibrary(cowsay)\nlibrary(beepr)\nsay(\"All done making some beautiful tables! :) \", \"whale\"); beep(3)  \n\n\n ___________________________________________ \n&lt; All done making some beautiful tables! :) &gt;\n ------------------------------------------- \n  \\\n   \\\n\n     .-'\n'--./ /     _.---.\n'-,  (__..-`       \\\n   \\          .     |\n    `,.__.   ,__.--/\n     '._/_.'___.-`"
  },
  {
    "objectID": "course-materials/labs/week1.html#activity-1-creating-tidy-tables",
    "href": "course-materials/labs/week1.html#activity-1-creating-tidy-tables",
    "title": "Lab 1",
    "section": "",
    "text": "There are many different ways to create tidy tables in R. You might be familiar with the kable function from {knitr} that creates tables for rectangular data. Kable tables don‚Äôt have a ton of flexibility, but are great at producing clean, simple tables. As we move into creating tables for the many different statistical models we will learn in this course, we will need to move beyond a simple kable table. That is where {gt} comes in! A {gt} table allows for the following structure, making it ideal for displaying different statistical outcomes.\n\n\n\n\n\n\n\n\n\nWe are going to use our class survey data to create some tables!\n\n# Load packages\nlibrary(tidyverse)\nlibrary(gtsummary)\nlibrary(gt)\nlibrary(janitor)\n\n\n\n# Read in class survey data and split into two random groups\nclass_data &lt;- read_csv(\"https://raw.github.com/garberadamc/W26-Policy-Eval/main/course-materials/labs/data/W26_class_survey.csv\") %&gt;%\n  mutate(random_groups = sample(rep(c(\"control\", \"treatment\"), each = n()/2)))\n\nLet‚Äôs create a base table that we will use with both kable and gt.\n\n# Create the base summary object\nbalance_summary &lt;- class_data %&gt;%\n  gtsummary::tbl_summary(\n    # Select how you want to group columns\n    by = random_groups,\n    # Variables to include\n    include = c(height, pets, dominant_hand, fav_number), \n    # Display mean and standard deviation for all continuous variables\n    statistic = list(all_continuous() ~ \"{mean} ({sd})\")\n  ) %&gt;% \n  # Add p value\n  add_p()\n\nbalance_summary\n\n\n\n\n\n\n\n\n\n\n\n\n\nCharacteristic\ncontrol\nN = 171\ntreatment\nN = 171\np-value2\n\n\n\n\nheight\n66.4 (4.2)\n63.5 (3.1)\n0.022\n\n\npets\n8 (47%)\n14 (82%)\n0.031\n\n\ndominant_hand\n\n\n\n\n0.6\n\n\n¬†¬†¬†¬†Left\n3 (18%)\n1 (5.9%)\n\n\n\n\n¬†¬†¬†¬†Right\n14 (82%)\n16 (94%)\n\n\n\n\nfav_number\n\n\n\n\n0.9\n\n\n¬†¬†¬†¬†2\n1 (5.9%)\n4 (24%)\n\n\n\n\n¬†¬†¬†¬†3\n2 (12%)\n2 (12%)\n\n\n\n\n¬†¬†¬†¬†4\n3 (18%)\n1 (5.9%)\n\n\n\n\n¬†¬†¬†¬†5\n3 (18%)\n2 (12%)\n\n\n\n\n¬†¬†¬†¬†6\n1 (5.9%)\n1 (5.9%)\n\n\n\n\n¬†¬†¬†¬†7\n4 (24%)\n4 (24%)\n\n\n\n\n¬†¬†¬†¬†8\n1 (5.9%)\n1 (5.9%)\n\n\n\n\n¬†¬†¬†¬†9\n2 (12%)\n1 (5.9%)\n\n\n\n\n¬†¬†¬†¬†10\n0 (0%)\n1 (5.9%)\n\n\n\n\n\n1 Mean (SD); n (%)\n\n\n2 Wilcoxon rank sum test; Pearson‚Äôs Chi-squared test; Fisher‚Äôs exact test\n\n\n\n\n\n\n\n\nNow lets output our balance_summary with a kable table!\n\nbalance_summary %&gt;%\n  as_kable_extra(caption = \"Class Survey Balance Table\") %&gt;% \n  kableExtra::kable_styling(\n    bootstrap_options = c(\"striped\", \"condensed\", \"hover\")\n  )\n\n\nClass Survey Balance Table\n\n\n\n\n\n\n\n\nCharacteristic\ncontrol\nN = 17\ntreatment\nN = 17\np-value\n\n\n\n\nheight\n66.4 (4.2)\n63.5 (3.1)\n0.022\n\n\npets\n8 (47%)\n14 (82%)\n0.031\n\n\ndominant_hand\n\n\n0.6\n\n\nLeft\n3 (18%)\n1 (5.9%)\n\n\n\nRight\n14 (82%)\n16 (94%)\n\n\n\nfav_number\n\n\n0.9\n\n\n2\n1 (5.9%)\n4 (24%)\n\n\n\n3\n2 (12%)\n2 (12%)\n\n\n\n4\n3 (18%)\n1 (5.9%)\n\n\n\n5\n3 (18%)\n2 (12%)\n\n\n\n6\n1 (5.9%)\n1 (5.9%)\n\n\n\n7\n4 (24%)\n4 (24%)\n\n\n\n8\n1 (5.9%)\n1 (5.9%)\n\n\n\n9\n2 (12%)\n1 (5.9%)\n\n\n\n10\n0 (0%)\n1 (5.9%)\n\n\n\n\n1 Mean (SD); n (%)\n\n\n\n\n\n2 Wilcoxon rank sum test; Pearson's Chi-squared test; Fisher's exact test\n\n\n\n\n\n\n\n\n\nIt looks fine‚Ä¶ But we can make it a lot nicer with {gt}!\n\n# Convert our balance summary table to a gt table\nbalance_summary %&gt;%\n  as_gt() %&gt;%\n  \n  # Add a Title and Subtitle\n  tab_header(\n    title = \"Class Survey Balance Table\",\n    subtitle = \"With Randomly Assigned Groups\"\n  ) %&gt;%\n  \n  # Add a Spanner to group the data columns\n  tab_spanner(\n    label = \"Randomized Groups\",\n    columns = c(stat_1, stat_2)\n  ) %&gt;%\n  \n  # Change column labels\n  cols_label(\n    label = \"Variable\",\n    p.value = \"P-Value\"\n  ) %&gt;%\n  \n  # Add a source note at the bottom\n  tab_source_note(\n    source_note = \"Note: Data from the Winter 2026 Class Survey.\"\n  )\n\n\n\n\n\n\n\n\n\n\n\n\n\nClass Survey Balance Table\n\n\nWith Randomly Assigned Groups\n\n\nVariable\n\nRandomized Groups\n\nP-Value2\n\n\ncontrol\nN = 171\ntreatment\nN = 171\n\n\n\n\nheight\n66.4 (4.2)\n63.5 (3.1)\n0.022\n\n\npets\n8 (47%)\n14 (82%)\n0.031\n\n\ndominant_hand\n\n\n\n\n0.6\n\n\n¬†¬†¬†¬†Left\n3 (18%)\n1 (5.9%)\n\n\n\n\n¬†¬†¬†¬†Right\n14 (82%)\n16 (94%)\n\n\n\n\nfav_number\n\n\n\n\n0.9\n\n\n¬†¬†¬†¬†2\n1 (5.9%)\n4 (24%)\n\n\n\n\n¬†¬†¬†¬†3\n2 (12%)\n2 (12%)\n\n\n\n\n¬†¬†¬†¬†4\n3 (18%)\n1 (5.9%)\n\n\n\n\n¬†¬†¬†¬†5\n3 (18%)\n2 (12%)\n\n\n\n\n¬†¬†¬†¬†6\n1 (5.9%)\n1 (5.9%)\n\n\n\n\n¬†¬†¬†¬†7\n4 (24%)\n4 (24%)\n\n\n\n\n¬†¬†¬†¬†8\n1 (5.9%)\n1 (5.9%)\n\n\n\n\n¬†¬†¬†¬†9\n2 (12%)\n1 (5.9%)\n\n\n\n\n¬†¬†¬†¬†10\n0 (0%)\n1 (5.9%)\n\n\n\n\n\n1 Mean (SD); n (%)\n\n\n2 Wilcoxon rank sum test; Pearson‚Äôs Chi-squared test; Fisher‚Äôs exact test\n\n\nNote: Data from the Winter 2026 Class Survey.\n\n\n\n\n\n\n\n\nThe {gt} table looks a lot cleaner! Let‚Äôs move on with creating some more {gt} tables!\nWe are going to use data from the Moland et al.¬†2013 study on Lobster MPAS.\nThe data we will be working with has the following variables:\n\n\n\n\n\n\n\n\nVariable\nData Type\nDescriptions\n\n\n\n\nyear\nNumeric (5-levels)\nYears measured from 2006 to 2010\n\n\nregion\nCharacter (3-levels)\nbol= Bol√¶rne , kve = Kvernskj√¶r , flo = Fl√∏devigen\n\n\ntreat\nCharacter (2-levels)\nmpa = treatment , con = control\n\n\ncpue\nNumeric\nCatch per unit effort\n\n\n\nLet‚Äôs read in our data to get started!\n\nlobsters &lt;- read_csv(\"https://raw.github.com/garberadamc/Lab2-EDS241-Moland13/main/data/moland13_lobsters.csv\")\n\nWe will start with creating a table for the total CPUE for each year and region.\nTo create a table with a column for each region, we need to untidy our data! We will do so by pivoting our data into wide format, with a column for each region, and the CPU for each year in that specific region.\nThis will be a 2 way table, since we are displaying data for two variables.\n\ntbl_2way &lt;- lobsters %&gt;%\n  \n  # Calculate total cpue for each year/region\n  group_by(year, region) %&gt;%\n  summarize(\n      total_cpue = sum(cpue, na.rm = TRUE),\n            .groups = \"drop\") %&gt;% # Same as `ungroup()`\n  \n  # Pivot to create column for each region\n  pivot_wider(\n      names_from = region, \n      values_from = total_cpue) %&gt;%\n  arrange(year) %&gt;%\n  \n  # Add a row for total cpue \n  adorn_totals(\"row\")\n\nTime to use {gt} to make this into a nice looking table!!\n\ntbl_2way %&gt;% \n  gt(rowname_col = \"year\") %&gt;%\n  tab_header(\n    title = \"European Lobster Catch by Region and Year\",\n    subtitle = \"Total Catch Per Unit Effort (CPUE) by year and region\") %&gt;% \n  cols_label(\n    bol = \"Bol√¶rne\",\n    flo = \"Kvernskj√¶r\",\n    kve = \"Fl√∏devigen\") %&gt;%\n  tab_source_note(\n      \"Source: Moland et al., 2013\")\n\n\n\n\n\n\n\nEuropean Lobster Catch by Region and Year\n\n\nTotal Catch Per Unit Effort (CPUE) by year and region\n\n\n\nBol√¶rne\nKvernskj√¶r\nFl√∏devigen\n\n\n\n\n2006\n127\n122\n177\n\n\n2007\n269\n93\n276\n\n\n2008\n249\n151\n367\n\n\n2009\n484\n168\n466\n\n\n2010\n463\n175\n449\n\n\nTotal\n1592\n709\n1735\n\n\n\nSource: Moland et al., 2013\n\n\n\n\n\n\n\n\nLet‚Äôs now add our treat variable into the table, so we can see how lobster catch varied within our control and MPA groups. This will be a 3 way table!\n\ntbl_3way &lt;- lobsters %&gt;%\n  # Calculate total cpue for each year/region/treatment group\n  group_by(year, region, treat) %&gt;%\n  summarize(total_cpue = sum(cpue, na.rm = TRUE),\n            .groups = \"drop\") %&gt;%\n  \n  # Pivot to create column for each trt/ control group within each region\n  pivot_wider(names_from = c(region, treat), values_from = total_cpue) %&gt;%\n  \n  arrange(year)\n\nTime to use {gt} to make this into a nice looking table!!\n\nfancy_table &lt;- tbl_3way %&gt;%\n  gt(rowname_col = \"year\") %&gt;%\n  tab_header(\n    title = \"European Lobster Catch by Year, Region and Treatment\",\n    subtitle = \"Total Catch Per Unit Effort (CPUE)\"\n  ) %&gt;%\n  tab_spanner(\n    label = \"Bol√¶rne\",\n    columns = c(\"bol_con\", \"bol_mpa\")\n  ) %&gt;%\n  tab_spanner(\n    label = \"Fl√∏devigen\",\n    columns = c(\"flo_con\", \"flo_mpa\")\n  ) %&gt;%\n  tab_spanner(\n    label = \"Kvernskj√¶r\",\n    columns = c(\"kve_con\", \"kve_mpa\")\n  ) %&gt;%\n  cols_label(\n    bol_con = \"Control\",\n    bol_mpa = \"MPA\",\n    flo_con = \"Control\",\n    flo_mpa = \"MPA\",\n    kve_con = \"Control\",\n    kve_mpa = \"MPA\")\n\nfancy_table\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nEuropean Lobster Catch by Year, Region and Treatment\n\n\nTotal Catch Per Unit Effort (CPUE)\n\n\n\n\nBol√¶rne\n\n\nFl√∏devigen\n\n\nKvernskj√¶r\n\n\n\nControl\nMPA\nControl\nMPA\nControl\nMPA\n\n\n\n\n2006\n52\n75\n54\n68\n125\n52\n\n\n2007\n98\n171\n33\n60\n114\n162\n\n\n2008\n78\n171\n55\n96\n178\n189\n\n\n2009\n187\n297\n51\n117\n244\n222\n\n\n2010\n148\n315\n64\n111\n198\n251\n\n\n\n\n\n\n\n\n\nWe can add plots within our table as well! While maybe not completely necessary in this instance, it can be a helpful tool to have!\n\ntable_w_plots &lt;- lobsters %&gt;%\n  # Calculate total cpue for each year\n  group_by(year) %&gt;%\n  summarize(\n      total_cpue = sum(cpue, na.rm = TRUE),\n      dist_cpue = list(cpue),\n      .groups = \"drop\") %&gt;% \n  arrange(year) %&gt;% \n  # Create gt table\n    gt() %&gt;% \n     tab_header(\n    title = \"European Lobster Catch Totals and Distribution (2006-2010)\",\n    subtitle = \"Total Catch Per Unit Effort (CPUE)\") %&gt;% \n    cols_label(\n    year = \"Year\",\n    total_cpue = \"Total CPUE\",\n    dist_cpue = \"Density CPUE\") %&gt;%\n    # Add in line density plots\n    gtExtras::gt_plt_dist( \n        dist_cpue,\n        type = \"density\", \n        line_color = \"blue\", \n        fill_color = \"red\")\n\n\ntable_w_plots \n\n\n\n\n\n\n\n\n\n\n\n\nEuropean Lobster Catch Totals and Distribution (2006-2010)\n\n\nTotal Catch Per Unit Effort (CPUE)\n\n\nYear\nTotal CPUE\nDensity CPUE\n\n\n\n\n2006\n426\n\n\n\n   \n\n\n\n2007\n638\n\n\n\n   \n\n\n\n2008\n767\n\n\n\n   \n\n\n\n2009\n1118\n\n\n\n   \n\n\n\n2010\n1087\n\n\n\n   \n\n\n\n\n\n\n\n\n\n#install.packages(\"praise\")\n#install.packages(\"cowsay\")\n#install.packages(\"beepr\")\nlibrary(praise)\nlibrary(cowsay)\nlibrary(beepr)\nsay(\"All done making some beautiful tables! :) \", \"whale\"); beep(3)  \n\n\n ___________________________________________ \n&lt; All done making some beautiful tables! :) &gt;\n ------------------------------------------- \n  \\\n   \\\n\n     .-'\n'--./ /     _.---.\n'-,  (__..-`       \\\n   \\          .     |\n    `,.__.   ,__.--/\n     '._/_.'___.-`"
  },
  {
    "objectID": "course-materials/labs/week1.html#activity-2-buntaine-policy-study-reading-comprehension-check",
    "href": "course-materials/labs/week1.html#activity-2-buntaine-policy-study-reading-comprehension-check",
    "title": "Lab 1",
    "section": "Activity 2: Buntaine Policy Study Reading Comprehension Check",
    "text": "Activity 2: Buntaine Policy Study Reading Comprehension Check\nWith your group, answer the following questions from the Buntaine article.\n\nWhat prompted this study?\nHow were the treatment and control groups formed?\nDiscuss the matched-pairs design of the study. How were neighborhoods paired? Create a diagram if helpful!\nWas the social competition strategy effective?\nWhat was the primary metric for assessing the impact of the social competition?"
  },
  {
    "objectID": "course-materials/labs/week1_solution.html",
    "href": "course-materials/labs/week1_solution.html",
    "title": "Lab 1",
    "section": "",
    "text": "There are many different ways to create tidy tables in R. You might be familiar with the kable function from {knitr} that creates tables for rectangular data. Kable tables don‚Äôt have a ton of flexibility, but are great at producing clean, simple tables. As we move into creating tables for the many different statistical models we will learn in this course, we will need to move beyond a simple kable table. That is where {gt} comes in! A {gt} table allows for the following structure, making it ideal for displaying different statistical outcomes.\n\n\n\n\n\n\n\n\n\nWe are going to use our class survey data to create some tables!\n\n# Load packages\nlibrary(tidyverse)\n\n‚îÄ‚îÄ Attaching core tidyverse packages ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ tidyverse 2.0.0 ‚îÄ‚îÄ\n‚úî dplyr     1.1.4     ‚úî readr     2.1.5\n‚úî forcats   1.0.1     ‚úî stringr   1.5.2\n‚úî ggplot2   4.0.0     ‚úî tibble    3.3.0\n‚úî lubridate 1.9.4     ‚úî tidyr     1.3.1\n‚úî purrr     1.1.0     \n‚îÄ‚îÄ Conflicts ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ tidyverse_conflicts() ‚îÄ‚îÄ\n‚úñ dplyr::filter() masks stats::filter()\n‚úñ dplyr::lag()    masks stats::lag()\n‚Ñπ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(gtsummary)\n\nWarning: package 'gtsummary' was built under R version 4.5.2\n\nlibrary(gt)\n\nWarning: package 'gt' was built under R version 4.5.2\n\nlibrary(janitor)\n\n\nAttaching package: 'janitor'\n\nThe following objects are masked from 'package:stats':\n\n    chisq.test, fisher.test\n\n# Read in class survey data and split into two random groups\nclass_data &lt;- read_csv(\"https://raw.github.com/garberadamc/W26-Policy-Eval/main/course-materials/labs/data/W26_class_survey.csv\") %&gt;%\n  mutate(random_groups = sample(rep(c(\"control\", \"treatment\"), each = n()/2)))\n\nRows: 34 Columns: 12\n‚îÄ‚îÄ Column specification ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\nDelimiter: \",\"\nchr (9): Timestamp, wakeup, fav_season, pets, dominant_hand, bday_month, cho...\ndbl (3): fav_number, siblings, height\n\n‚Ñπ Use `spec()` to retrieve the full column specification for this data.\n‚Ñπ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\nLet‚Äôs create a base table that we will use with both kable and gt.\n\n# Create the base summary object\nbalance_summary &lt;- class_data %&gt;%\n  gtsummary::tbl_summary(\n    # Select how you want to group columns\n    by = random_groups,\n    # Variables to include\n    include = c(height, pets, dominant_hand, fav_number), \n    # Display mean and standard deviation for all continuous variables\n    statistic = list(all_continuous() ~ \"{mean} ({sd})\")\n  ) %&gt;% \n  # Add p value\n  add_p()\n\nThe following warnings were returned during `add_p()`:\n! For variable `height` (`random_groups`) and \"estimate\", \"statistic\",\n  \"p.value\", \"conf.low\", and \"conf.high\" statistics: cannot compute exact\n  p-value with ties\n! For variable `height` (`random_groups`) and \"estimate\", \"statistic\",\n  \"p.value\", \"conf.low\", and \"conf.high\" statistics: cannot compute exact\n  confidence intervals with ties\n\nbalance_summary\n\n\n\n\n\n\n\n\n\n\n\n\n\nCharacteristic\ncontrol\nN = 171\ntreatment\nN = 171\np-value2\n\n\n\n\nheight\n64.2 (3.7)\n65.6 (4.1)\n0.3\n\n\npets\n13 (76%)\n9 (53%)\n0.2\n\n\ndominant_hand\n\n\n\n\n0.6\n\n\n¬†¬†¬†¬†Left\n1 (5.9%)\n3 (18%)\n\n\n\n\n¬†¬†¬†¬†Right\n16 (94%)\n14 (82%)\n\n\n\n\nfav_number\n\n\n\n\n0.6\n\n\n¬†¬†¬†¬†2\n1 (5.9%)\n4 (24%)\n\n\n\n\n¬†¬†¬†¬†3\n2 (12%)\n2 (12%)\n\n\n\n\n¬†¬†¬†¬†4\n2 (12%)\n2 (12%)\n\n\n\n\n¬†¬†¬†¬†5\n2 (12%)\n3 (18%)\n\n\n\n\n¬†¬†¬†¬†6\n1 (5.9%)\n1 (5.9%)\n\n\n\n\n¬†¬†¬†¬†7\n6 (35%)\n2 (12%)\n\n\n\n\n¬†¬†¬†¬†8\n0 (0%)\n2 (12%)\n\n\n\n\n¬†¬†¬†¬†9\n2 (12%)\n1 (5.9%)\n\n\n\n\n¬†¬†¬†¬†10\n1 (5.9%)\n0 (0%)\n\n\n\n\n\n1 Mean (SD); n (%)\n\n\n2 Wilcoxon rank sum test; Pearson‚Äôs Chi-squared test; Fisher‚Äôs exact test\n\n\n\n\n\n\n\n\nNow lets output our balance_summary with a kable table!\n\nbalance_summary %&gt;%\n  as_kable_extra(caption = \"Class Survey Balance Table\") %&gt;% \n  kableExtra::kable_styling(\n    bootstrap_options = c(\"striped\", \"condensed\", \"hover\")\n  )\n\n\nClass Survey Balance Table\n\n\n\n\n\n\n\n\nCharacteristic\ncontrol\nN = 17\ntreatment\nN = 17\np-value\n\n\n\n\nheight\n64.2 (3.7)\n65.6 (4.1)\n0.3\n\n\npets\n13 (76%)\n9 (53%)\n0.2\n\n\ndominant_hand\n\n\n0.6\n\n\nLeft\n1 (5.9%)\n3 (18%)\n\n\n\nRight\n16 (94%)\n14 (82%)\n\n\n\nfav_number\n\n\n0.6\n\n\n2\n1 (5.9%)\n4 (24%)\n\n\n\n3\n2 (12%)\n2 (12%)\n\n\n\n4\n2 (12%)\n2 (12%)\n\n\n\n5\n2 (12%)\n3 (18%)\n\n\n\n6\n1 (5.9%)\n1 (5.9%)\n\n\n\n7\n6 (35%)\n2 (12%)\n\n\n\n8\n0 (0%)\n2 (12%)\n\n\n\n9\n2 (12%)\n1 (5.9%)\n\n\n\n10\n1 (5.9%)\n0 (0%)\n\n\n\n\n1 Mean (SD); n (%)\n\n\n\n\n\n2 Wilcoxon rank sum test; Pearson's Chi-squared test; Fisher's exact test\n\n\n\n\n\n\n\n\n\nIt looks fine‚Ä¶ But we can make it a lot nicer with {gt}!\n\n# Convert our balance summary table to a gt table\nbalance_summary %&gt;%\n  as_gt() %&gt;%\n  \n  # Add a Title and Subtitle\n  tab_header(\n    title = \"Class Survey Balance Table\",\n    subtitle = \"With Randomly Assigned Groups\"\n  ) %&gt;%\n  \n  # Add a Spanner to group the data columns\n  tab_spanner(\n    label = \"Randomized Groups\",\n    columns = everything()\n  ) %&gt;%\n  \n  # Change column labels\n  cols_label(\n    label = \"Variable\",\n    p.value = \"P-Value\"\n  ) %&gt;%\n  \n  # Add a source note at the bottom\n  tab_source_note(\n    source_note = \"Note: Data from the Winter 2026 Class Survey.\"\n  )\n\n\n\n\n\n\n\n\n\n\n\n\n\nClass Survey Balance Table\n\n\nWith Randomly Assigned Groups\n\n\n\nRandomized Groups\n\n\n\nVariable\ncontrol\nN = 171\ntreatment\nN = 171\nP-Value2\n\n\n\n\nheight\n64.2 (3.7)\n65.6 (4.1)\n0.3\n\n\npets\n13 (76%)\n9 (53%)\n0.2\n\n\ndominant_hand\n\n\n\n\n0.6\n\n\n¬†¬†¬†¬†Left\n1 (5.9%)\n3 (18%)\n\n\n\n\n¬†¬†¬†¬†Right\n16 (94%)\n14 (82%)\n\n\n\n\nfav_number\n\n\n\n\n0.6\n\n\n¬†¬†¬†¬†2\n1 (5.9%)\n4 (24%)\n\n\n\n\n¬†¬†¬†¬†3\n2 (12%)\n2 (12%)\n\n\n\n\n¬†¬†¬†¬†4\n2 (12%)\n2 (12%)\n\n\n\n\n¬†¬†¬†¬†5\n2 (12%)\n3 (18%)\n\n\n\n\n¬†¬†¬†¬†6\n1 (5.9%)\n1 (5.9%)\n\n\n\n\n¬†¬†¬†¬†7\n6 (35%)\n2 (12%)\n\n\n\n\n¬†¬†¬†¬†8\n0 (0%)\n2 (12%)\n\n\n\n\n¬†¬†¬†¬†9\n2 (12%)\n1 (5.9%)\n\n\n\n\n¬†¬†¬†¬†10\n1 (5.9%)\n0 (0%)\n\n\n\n\n\n1 Mean (SD); n (%)\n\n\n2 Wilcoxon rank sum test; Pearson‚Äôs Chi-squared test; Fisher‚Äôs exact test\n\n\nNote: Data from the Winter 2026 Class Survey.\n\n\n\n\n\n\n\n\nThe {gt} table looks a lot cleaner! Let‚Äôs move on with creating some more {gt} tables!\nWe are going to use data from the Moland et al.¬†2013 study on Lobster MPAS.\nThe data we will be working with has the following variables:\n\n\n\n\n\n\n\n\nVariable\nData Type\nDescriptions\n\n\n\n\nyear\nNumeric (5-levels)\nYears measured from 2006 to 2010\n\n\nregion\nCharacter (3-levels)\nbol= Bol√¶rne , kve = Kvernskj√¶r , flo = Fl√∏devigen\n\n\ntreat\nCharacter (2-levels)\nmpa = treatment , con = control\n\n\ncpue\nNumeric\nCatch per unit effort\n\n\n\nLet‚Äôs read in our data to get started!\n\nlobsters &lt;- read_csv(\"https://raw.github.com/garberadamc/Lab2-EDS241-Moland13/main/data/moland13_lobsters.csv\")\n\nWe will start with creating a table for the total CPUE for each year and region.\nTo create a table with a column for each region, we need to untidy our data! We will do so by pivoting our data into wide format, with a column for each region, and the CPU for each year in that specific region.\nThis will be a 2 way table, since we are displaying data for two variables.\n\ntbl_2way &lt;- lobsters %&gt;%\n  \n  # Calculate total cpue for each year/region\n  group_by(year, region) %&gt;%\n  summarize(\n      total_cpue = sum(cpue, na.rm = TRUE),\n            .groups = \"drop\") %&gt;% # Same as `ungroup()`\n  \n  # Pivot to create column for each region\n  pivot_wider(\n      names_from = region, \n      values_from = total_cpue) %&gt;%\n  arrange(year) %&gt;%\n  \n  # Add a row for total cpue \n  adorn_totals(\"row\")\n\nTime to use {gt} to make this into a nice looking table!!\n\ntbl_2way %&gt;% \n  gt(rowname_col = \"year\") %&gt;%\n  tab_header(\n    title = \"European Lobster Catch by Region and Year\",\n    subtitle = \"Total Catch Per Unit Effort (CPUE) by year and region\") %&gt;% \n  cols_label(\n    bol = \"Bol√¶rne\",\n    flo = \"Kvernskj√¶r\",\n    kve = \"Fl√∏devigen\") %&gt;%\n  tab_source_note(\n      \"Source: Moland et al., 2013\")\n\n\n\n\n\n\n\nEuropean Lobster Catch by Region and Year\n\n\nTotal Catch Per Unit Effort (CPUE) by year and region\n\n\n\nBol√¶rne\nKvernskj√¶r\nFl√∏devigen\n\n\n\n\n2006\n127\n122\n177\n\n\n2007\n269\n93\n276\n\n\n2008\n249\n151\n367\n\n\n2009\n484\n168\n466\n\n\n2010\n463\n175\n449\n\n\nTotal\n1592\n709\n1735\n\n\n\nSource: Moland et al., 2013\n\n\n\n\n\n\n\n\nLet‚Äôs now add our treat variable into the table, so we can see how lobster catch varied within our control and MPA groups. This will be a 3 way table!\n\ntbl_3way &lt;- lobsters %&gt;%\n  # Calculate total cpue for each year/region/treatment group\n  group_by(year, region, treat) %&gt;%\n  summarize(total_cpue = sum(cpue, na.rm = TRUE),\n            .groups = \"drop\") %&gt;%\n  \n  # Pivot to create column for each trt/ control group within each region\n  pivot_wider(names_from = c(region, treat), values_from = total_cpue) %&gt;%\n  \n  arrange(year)\n\nTime to use {gt} to make this into a nice looking table!!\n\nfancy_table &lt;- tbl_3way %&gt;%\n  gt(rowname_col = \"year\") %&gt;%\n  tab_header(\n    title = \"European Lobster Catch by Year, Region and Treatment\",\n    subtitle = \"Total Catch Per Unit Effort (CPUE)\"\n  ) %&gt;%\n  tab_spanner(\n    label = \"Bol√¶rne\",\n    columns = c(\"bol_con\", \"bol_mpa\")\n  ) %&gt;%\n  tab_spanner(\n    label = \"Fl√∏devigen\",\n    columns = c(\"flo_con\", \"flo_mpa\")\n  ) %&gt;%\n  tab_spanner(\n    label = \"Kvernskj√¶r\",\n    columns = c(\"kve_con\", \"kve_mpa\")\n  ) %&gt;%\n  cols_label(\n    bol_con = \"Control\",\n    bol_mpa = \"MPA\",\n    flo_con = \"Control\",\n    flo_mpa = \"MPA\",\n    kve_con = \"Control\",\n    kve_mpa = \"MPA\")\n\nfancy_table\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nEuropean Lobster Catch by Year, Region and Treatment\n\n\nTotal Catch Per Unit Effort (CPUE)\n\n\n\n\nBol√¶rne\n\n\nFl√∏devigen\n\n\nKvernskj√¶r\n\n\n\nControl\nMPA\nControl\nMPA\nControl\nMPA\n\n\n\n\n2006\n52\n75\n54\n68\n125\n52\n\n\n2007\n98\n171\n33\n60\n114\n162\n\n\n2008\n78\n171\n55\n96\n178\n189\n\n\n2009\n187\n297\n51\n117\n244\n222\n\n\n2010\n148\n315\n64\n111\n198\n251\n\n\n\n\n\n\n\n\n\nWe can add plots within our table as well! While maybe not completely necessary in this instance, it can be a helpful tool to have!\n\ntable_w_plots &lt;- lobsters %&gt;%\n  # Calculate total cpue for each year\n  group_by(year) %&gt;%\n  summarize(\n      total_cpue = sum(cpue, na.rm = TRUE),\n      dist_cpue = list(cpue),\n      .groups = \"drop\") %&gt;% \n  arrange(year) %&gt;% \n  # Create gt table\n    gt() %&gt;% \n     tab_header(\n    title = \"European Lobster Catch Totals and Distribution (2006-2010)\",\n    subtitle = \"Total Catch Per Unit Effort (CPUE)\") %&gt;% \n    cols_label(\n    year = \"Year\",\n    total_cpue = \"Total CPUE\",\n    dist_cpue = \"Density CPUE\") %&gt;%\n    # Add in line density plots\n    gtExtras::gt_plt_dist( \n        dist_cpue,\n        type = \"density\", \n        line_color = \"blue\", \n        fill_color = \"red\")\n\n\ntable_w_plots \n\n\n\n\n\n\n\n\n\n\n\n\nEuropean Lobster Catch Totals and Distribution (2006-2010)\n\n\nTotal Catch Per Unit Effort (CPUE)\n\n\nYear\nTotal CPUE\nDensity CPUE\n\n\n\n\n2006\n426\n\n\n\n   \n\n\n\n2007\n638\n\n\n\n   \n\n\n\n2008\n767\n\n\n\n   \n\n\n\n2009\n1118\n\n\n\n   \n\n\n\n2010\n1087\n\n\n\n   \n\n\n\n\n\n\n\n\n\n#install.packages(\"praise\")\n#install.packages(\"cowsay\")\n#install.packages(\"beepr\")\nlibrary(praise)\nlibrary(cowsay)\nlibrary(beepr)\nsay(\"All done making some beautiful tables! :) \", \"whale\"); beep(3)  \n\n\n ___________________________________________ \n&lt; All done making some beautiful tables! :) &gt;\n ------------------------------------------- \n  \\\n   \\\n\n     .-'\n'--./ /     _.---.\n'-,  (__..-`       \\\n   \\          .     |\n    `,.__.   ,__.--/\n     '._/_.'___.-`"
  },
  {
    "objectID": "course-materials/labs/week1_solution.html#activity-1-creating-tidy-tables",
    "href": "course-materials/labs/week1_solution.html#activity-1-creating-tidy-tables",
    "title": "Lab 1",
    "section": "",
    "text": "There are many different ways to create tidy tables in R. You might be familiar with the kable function from {knitr} that creates tables for rectangular data. Kable tables don‚Äôt have a ton of flexibility, but are great at producing clean, simple tables. As we move into creating tables for the many different statistical models we will learn in this course, we will need to move beyond a simple kable table. That is where {gt} comes in! A {gt} table allows for the following structure, making it ideal for displaying different statistical outcomes.\n\n\n\n\n\n\n\n\n\nWe are going to use our class survey data to create some tables!\n\n# Load packages\nlibrary(tidyverse)\n\n‚îÄ‚îÄ Attaching core tidyverse packages ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ tidyverse 2.0.0 ‚îÄ‚îÄ\n‚úî dplyr     1.1.4     ‚úî readr     2.1.5\n‚úî forcats   1.0.1     ‚úî stringr   1.5.2\n‚úî ggplot2   4.0.0     ‚úî tibble    3.3.0\n‚úî lubridate 1.9.4     ‚úî tidyr     1.3.1\n‚úî purrr     1.1.0     \n‚îÄ‚îÄ Conflicts ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ tidyverse_conflicts() ‚îÄ‚îÄ\n‚úñ dplyr::filter() masks stats::filter()\n‚úñ dplyr::lag()    masks stats::lag()\n‚Ñπ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(gtsummary)\n\nWarning: package 'gtsummary' was built under R version 4.5.2\n\nlibrary(gt)\n\nWarning: package 'gt' was built under R version 4.5.2\n\nlibrary(janitor)\n\n\nAttaching package: 'janitor'\n\nThe following objects are masked from 'package:stats':\n\n    chisq.test, fisher.test\n\n# Read in class survey data and split into two random groups\nclass_data &lt;- read_csv(\"https://raw.github.com/garberadamc/W26-Policy-Eval/main/course-materials/labs/data/W26_class_survey.csv\") %&gt;%\n  mutate(random_groups = sample(rep(c(\"control\", \"treatment\"), each = n()/2)))\n\nRows: 34 Columns: 12\n‚îÄ‚îÄ Column specification ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\nDelimiter: \",\"\nchr (9): Timestamp, wakeup, fav_season, pets, dominant_hand, bday_month, cho...\ndbl (3): fav_number, siblings, height\n\n‚Ñπ Use `spec()` to retrieve the full column specification for this data.\n‚Ñπ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\nLet‚Äôs create a base table that we will use with both kable and gt.\n\n# Create the base summary object\nbalance_summary &lt;- class_data %&gt;%\n  gtsummary::tbl_summary(\n    # Select how you want to group columns\n    by = random_groups,\n    # Variables to include\n    include = c(height, pets, dominant_hand, fav_number), \n    # Display mean and standard deviation for all continuous variables\n    statistic = list(all_continuous() ~ \"{mean} ({sd})\")\n  ) %&gt;% \n  # Add p value\n  add_p()\n\nThe following warnings were returned during `add_p()`:\n! For variable `height` (`random_groups`) and \"estimate\", \"statistic\",\n  \"p.value\", \"conf.low\", and \"conf.high\" statistics: cannot compute exact\n  p-value with ties\n! For variable `height` (`random_groups`) and \"estimate\", \"statistic\",\n  \"p.value\", \"conf.low\", and \"conf.high\" statistics: cannot compute exact\n  confidence intervals with ties\n\nbalance_summary\n\n\n\n\n\n\n\n\n\n\n\n\n\nCharacteristic\ncontrol\nN = 171\ntreatment\nN = 171\np-value2\n\n\n\n\nheight\n64.2 (3.7)\n65.6 (4.1)\n0.3\n\n\npets\n13 (76%)\n9 (53%)\n0.2\n\n\ndominant_hand\n\n\n\n\n0.6\n\n\n¬†¬†¬†¬†Left\n1 (5.9%)\n3 (18%)\n\n\n\n\n¬†¬†¬†¬†Right\n16 (94%)\n14 (82%)\n\n\n\n\nfav_number\n\n\n\n\n0.6\n\n\n¬†¬†¬†¬†2\n1 (5.9%)\n4 (24%)\n\n\n\n\n¬†¬†¬†¬†3\n2 (12%)\n2 (12%)\n\n\n\n\n¬†¬†¬†¬†4\n2 (12%)\n2 (12%)\n\n\n\n\n¬†¬†¬†¬†5\n2 (12%)\n3 (18%)\n\n\n\n\n¬†¬†¬†¬†6\n1 (5.9%)\n1 (5.9%)\n\n\n\n\n¬†¬†¬†¬†7\n6 (35%)\n2 (12%)\n\n\n\n\n¬†¬†¬†¬†8\n0 (0%)\n2 (12%)\n\n\n\n\n¬†¬†¬†¬†9\n2 (12%)\n1 (5.9%)\n\n\n\n\n¬†¬†¬†¬†10\n1 (5.9%)\n0 (0%)\n\n\n\n\n\n1 Mean (SD); n (%)\n\n\n2 Wilcoxon rank sum test; Pearson‚Äôs Chi-squared test; Fisher‚Äôs exact test\n\n\n\n\n\n\n\n\nNow lets output our balance_summary with a kable table!\n\nbalance_summary %&gt;%\n  as_kable_extra(caption = \"Class Survey Balance Table\") %&gt;% \n  kableExtra::kable_styling(\n    bootstrap_options = c(\"striped\", \"condensed\", \"hover\")\n  )\n\n\nClass Survey Balance Table\n\n\n\n\n\n\n\n\nCharacteristic\ncontrol\nN = 17\ntreatment\nN = 17\np-value\n\n\n\n\nheight\n64.2 (3.7)\n65.6 (4.1)\n0.3\n\n\npets\n13 (76%)\n9 (53%)\n0.2\n\n\ndominant_hand\n\n\n0.6\n\n\nLeft\n1 (5.9%)\n3 (18%)\n\n\n\nRight\n16 (94%)\n14 (82%)\n\n\n\nfav_number\n\n\n0.6\n\n\n2\n1 (5.9%)\n4 (24%)\n\n\n\n3\n2 (12%)\n2 (12%)\n\n\n\n4\n2 (12%)\n2 (12%)\n\n\n\n5\n2 (12%)\n3 (18%)\n\n\n\n6\n1 (5.9%)\n1 (5.9%)\n\n\n\n7\n6 (35%)\n2 (12%)\n\n\n\n8\n0 (0%)\n2 (12%)\n\n\n\n9\n2 (12%)\n1 (5.9%)\n\n\n\n10\n1 (5.9%)\n0 (0%)\n\n\n\n\n1 Mean (SD); n (%)\n\n\n\n\n\n2 Wilcoxon rank sum test; Pearson's Chi-squared test; Fisher's exact test\n\n\n\n\n\n\n\n\n\nIt looks fine‚Ä¶ But we can make it a lot nicer with {gt}!\n\n# Convert our balance summary table to a gt table\nbalance_summary %&gt;%\n  as_gt() %&gt;%\n  \n  # Add a Title and Subtitle\n  tab_header(\n    title = \"Class Survey Balance Table\",\n    subtitle = \"With Randomly Assigned Groups\"\n  ) %&gt;%\n  \n  # Add a Spanner to group the data columns\n  tab_spanner(\n    label = \"Randomized Groups\",\n    columns = everything()\n  ) %&gt;%\n  \n  # Change column labels\n  cols_label(\n    label = \"Variable\",\n    p.value = \"P-Value\"\n  ) %&gt;%\n  \n  # Add a source note at the bottom\n  tab_source_note(\n    source_note = \"Note: Data from the Winter 2026 Class Survey.\"\n  )\n\n\n\n\n\n\n\n\n\n\n\n\n\nClass Survey Balance Table\n\n\nWith Randomly Assigned Groups\n\n\n\nRandomized Groups\n\n\n\nVariable\ncontrol\nN = 171\ntreatment\nN = 171\nP-Value2\n\n\n\n\nheight\n64.2 (3.7)\n65.6 (4.1)\n0.3\n\n\npets\n13 (76%)\n9 (53%)\n0.2\n\n\ndominant_hand\n\n\n\n\n0.6\n\n\n¬†¬†¬†¬†Left\n1 (5.9%)\n3 (18%)\n\n\n\n\n¬†¬†¬†¬†Right\n16 (94%)\n14 (82%)\n\n\n\n\nfav_number\n\n\n\n\n0.6\n\n\n¬†¬†¬†¬†2\n1 (5.9%)\n4 (24%)\n\n\n\n\n¬†¬†¬†¬†3\n2 (12%)\n2 (12%)\n\n\n\n\n¬†¬†¬†¬†4\n2 (12%)\n2 (12%)\n\n\n\n\n¬†¬†¬†¬†5\n2 (12%)\n3 (18%)\n\n\n\n\n¬†¬†¬†¬†6\n1 (5.9%)\n1 (5.9%)\n\n\n\n\n¬†¬†¬†¬†7\n6 (35%)\n2 (12%)\n\n\n\n\n¬†¬†¬†¬†8\n0 (0%)\n2 (12%)\n\n\n\n\n¬†¬†¬†¬†9\n2 (12%)\n1 (5.9%)\n\n\n\n\n¬†¬†¬†¬†10\n1 (5.9%)\n0 (0%)\n\n\n\n\n\n1 Mean (SD); n (%)\n\n\n2 Wilcoxon rank sum test; Pearson‚Äôs Chi-squared test; Fisher‚Äôs exact test\n\n\nNote: Data from the Winter 2026 Class Survey.\n\n\n\n\n\n\n\n\nThe {gt} table looks a lot cleaner! Let‚Äôs move on with creating some more {gt} tables!\nWe are going to use data from the Moland et al.¬†2013 study on Lobster MPAS.\nThe data we will be working with has the following variables:\n\n\n\n\n\n\n\n\nVariable\nData Type\nDescriptions\n\n\n\n\nyear\nNumeric (5-levels)\nYears measured from 2006 to 2010\n\n\nregion\nCharacter (3-levels)\nbol= Bol√¶rne , kve = Kvernskj√¶r , flo = Fl√∏devigen\n\n\ntreat\nCharacter (2-levels)\nmpa = treatment , con = control\n\n\ncpue\nNumeric\nCatch per unit effort\n\n\n\nLet‚Äôs read in our data to get started!\n\nlobsters &lt;- read_csv(\"https://raw.github.com/garberadamc/Lab2-EDS241-Moland13/main/data/moland13_lobsters.csv\")\n\nWe will start with creating a table for the total CPUE for each year and region.\nTo create a table with a column for each region, we need to untidy our data! We will do so by pivoting our data into wide format, with a column for each region, and the CPU for each year in that specific region.\nThis will be a 2 way table, since we are displaying data for two variables.\n\ntbl_2way &lt;- lobsters %&gt;%\n  \n  # Calculate total cpue for each year/region\n  group_by(year, region) %&gt;%\n  summarize(\n      total_cpue = sum(cpue, na.rm = TRUE),\n            .groups = \"drop\") %&gt;% # Same as `ungroup()`\n  \n  # Pivot to create column for each region\n  pivot_wider(\n      names_from = region, \n      values_from = total_cpue) %&gt;%\n  arrange(year) %&gt;%\n  \n  # Add a row for total cpue \n  adorn_totals(\"row\")\n\nTime to use {gt} to make this into a nice looking table!!\n\ntbl_2way %&gt;% \n  gt(rowname_col = \"year\") %&gt;%\n  tab_header(\n    title = \"European Lobster Catch by Region and Year\",\n    subtitle = \"Total Catch Per Unit Effort (CPUE) by year and region\") %&gt;% \n  cols_label(\n    bol = \"Bol√¶rne\",\n    flo = \"Kvernskj√¶r\",\n    kve = \"Fl√∏devigen\") %&gt;%\n  tab_source_note(\n      \"Source: Moland et al., 2013\")\n\n\n\n\n\n\n\nEuropean Lobster Catch by Region and Year\n\n\nTotal Catch Per Unit Effort (CPUE) by year and region\n\n\n\nBol√¶rne\nKvernskj√¶r\nFl√∏devigen\n\n\n\n\n2006\n127\n122\n177\n\n\n2007\n269\n93\n276\n\n\n2008\n249\n151\n367\n\n\n2009\n484\n168\n466\n\n\n2010\n463\n175\n449\n\n\nTotal\n1592\n709\n1735\n\n\n\nSource: Moland et al., 2013\n\n\n\n\n\n\n\n\nLet‚Äôs now add our treat variable into the table, so we can see how lobster catch varied within our control and MPA groups. This will be a 3 way table!\n\ntbl_3way &lt;- lobsters %&gt;%\n  # Calculate total cpue for each year/region/treatment group\n  group_by(year, region, treat) %&gt;%\n  summarize(total_cpue = sum(cpue, na.rm = TRUE),\n            .groups = \"drop\") %&gt;%\n  \n  # Pivot to create column for each trt/ control group within each region\n  pivot_wider(names_from = c(region, treat), values_from = total_cpue) %&gt;%\n  \n  arrange(year)\n\nTime to use {gt} to make this into a nice looking table!!\n\nfancy_table &lt;- tbl_3way %&gt;%\n  gt(rowname_col = \"year\") %&gt;%\n  tab_header(\n    title = \"European Lobster Catch by Year, Region and Treatment\",\n    subtitle = \"Total Catch Per Unit Effort (CPUE)\"\n  ) %&gt;%\n  tab_spanner(\n    label = \"Bol√¶rne\",\n    columns = c(\"bol_con\", \"bol_mpa\")\n  ) %&gt;%\n  tab_spanner(\n    label = \"Fl√∏devigen\",\n    columns = c(\"flo_con\", \"flo_mpa\")\n  ) %&gt;%\n  tab_spanner(\n    label = \"Kvernskj√¶r\",\n    columns = c(\"kve_con\", \"kve_mpa\")\n  ) %&gt;%\n  cols_label(\n    bol_con = \"Control\",\n    bol_mpa = \"MPA\",\n    flo_con = \"Control\",\n    flo_mpa = \"MPA\",\n    kve_con = \"Control\",\n    kve_mpa = \"MPA\")\n\nfancy_table\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nEuropean Lobster Catch by Year, Region and Treatment\n\n\nTotal Catch Per Unit Effort (CPUE)\n\n\n\n\nBol√¶rne\n\n\nFl√∏devigen\n\n\nKvernskj√¶r\n\n\n\nControl\nMPA\nControl\nMPA\nControl\nMPA\n\n\n\n\n2006\n52\n75\n54\n68\n125\n52\n\n\n2007\n98\n171\n33\n60\n114\n162\n\n\n2008\n78\n171\n55\n96\n178\n189\n\n\n2009\n187\n297\n51\n117\n244\n222\n\n\n2010\n148\n315\n64\n111\n198\n251\n\n\n\n\n\n\n\n\n\nWe can add plots within our table as well! While maybe not completely necessary in this instance, it can be a helpful tool to have!\n\ntable_w_plots &lt;- lobsters %&gt;%\n  # Calculate total cpue for each year\n  group_by(year) %&gt;%\n  summarize(\n      total_cpue = sum(cpue, na.rm = TRUE),\n      dist_cpue = list(cpue),\n      .groups = \"drop\") %&gt;% \n  arrange(year) %&gt;% \n  # Create gt table\n    gt() %&gt;% \n     tab_header(\n    title = \"European Lobster Catch Totals and Distribution (2006-2010)\",\n    subtitle = \"Total Catch Per Unit Effort (CPUE)\") %&gt;% \n    cols_label(\n    year = \"Year\",\n    total_cpue = \"Total CPUE\",\n    dist_cpue = \"Density CPUE\") %&gt;%\n    # Add in line density plots\n    gtExtras::gt_plt_dist( \n        dist_cpue,\n        type = \"density\", \n        line_color = \"blue\", \n        fill_color = \"red\")\n\n\ntable_w_plots \n\n\n\n\n\n\n\n\n\n\n\n\nEuropean Lobster Catch Totals and Distribution (2006-2010)\n\n\nTotal Catch Per Unit Effort (CPUE)\n\n\nYear\nTotal CPUE\nDensity CPUE\n\n\n\n\n2006\n426\n\n\n\n   \n\n\n\n2007\n638\n\n\n\n   \n\n\n\n2008\n767\n\n\n\n   \n\n\n\n2009\n1118\n\n\n\n   \n\n\n\n2010\n1087\n\n\n\n   \n\n\n\n\n\n\n\n\n\n#install.packages(\"praise\")\n#install.packages(\"cowsay\")\n#install.packages(\"beepr\")\nlibrary(praise)\nlibrary(cowsay)\nlibrary(beepr)\nsay(\"All done making some beautiful tables! :) \", \"whale\"); beep(3)  \n\n\n ___________________________________________ \n&lt; All done making some beautiful tables! :) &gt;\n ------------------------------------------- \n  \\\n   \\\n\n     .-'\n'--./ /     _.---.\n'-,  (__..-`       \\\n   \\          .     |\n    `,.__.   ,__.--/\n     '._/_.'___.-`"
  },
  {
    "objectID": "course-materials/labs/week1_solution.html#activity-2-buntaine-policy-study-reading-comprehension-check",
    "href": "course-materials/labs/week1_solution.html#activity-2-buntaine-policy-study-reading-comprehension-check",
    "title": "Lab 1",
    "section": "Activity 2: Buntaine Policy Study Reading Comprehension Check",
    "text": "Activity 2: Buntaine Policy Study Reading Comprehension Check\nWith your group, answer the following questions from the Buntaine article.\n\nWhat prompted this study?\nHow were the treatment and control groups formed?\nWas the social competition strategy effective?"
  },
  {
    "objectID": "course-materials/week1.html#lab-materials",
    "href": "course-materials/week1.html#lab-materials",
    "title": "Week 1:",
    "section": " Lab Materials",
    "text": "Lab Materials\n\n\n\n\n\n\n\nLab Materials to Download\nLab Solution\n\n\n\n\nWeek 1 Lab\nWeek 1 Lab Solution"
  },
  {
    "objectID": "course-materials/week2.html#lab-materials",
    "href": "course-materials/week2.html#lab-materials",
    "title": "Week 2:",
    "section": " Lab Materials",
    "text": "Lab Materials\n\n\n\n\n\n\n\nLab Materials to Download\nLab Solution\n\n\n\n\nWeek 2 Lab\nWeek 2 Lab Solution"
  },
  {
    "objectID": "course-materials/labs/week2.html",
    "href": "course-materials/labs/week2.html",
    "title": "Lab 2: Generalize Fast!",
    "section": "",
    "text": "Map of Treatment Assignment Zones in Nansana, Uganda\n\n\n\n\n\nFigure from Buntaine et al., 2024 (S5; Supplemental Materials)\n\n\n\n\n\n\nOpen source data & applied study:\nOpen access data was utilized for this class exercise from the study (Buntaine et al., 2024):\n\nBuntaine, M. T., Komakech, P., & Shen, S. V. (2024). Social competition drives collective action to reduce informal waste burning in Uganda. Proceedings of the National Academy of Sciences, 121(23). https://doi.org/10.1073/pnas.2319712121\n\n\n\nGoals of this exercise\n\nStart with a simple OLS model\nDemonstrate the rationale for estimating progressively complex models\nIllustrate a causal inference approach (i.e., identify robust estimators)\nConclusion: What is the take-home message of the Lab-1 exercise? . . .\nEstimating complex models is a relatively easy part of our work as data scientists‚Ä¶\n\nMaking informed specification decisions & communicating results is the hard part (i.e., the science side of data science)!\n\n\n\n\nLet‚Äôs get started!\nLoad packages and data:\n\nlibrary(tidyverse)   # Keeping things tidy \nlibrary(janitor)     # Housekeeping \nlibrary(here)        # Location, location, location \n#install.packages(\"jtools\")\nlibrary(jtools)      # Pretty regression output \nlibrary(gt)          # Tables \nlibrary(gtsummary)   # Table for checking balance \n#install.packages(\"performance\")\nlibrary(performance) # Check model diagnostics \n#install.packages(\"see\") # Visualization engine for performance package\n#install.packages(\"huxtable\") # Package required for export_summs\n\n\n\n# A tibble: 6 √ó 8\n  zoneid survey    waste_piles rain_0_48hrs rain_49_168hrs pre_count_avg pair_id\n   &lt;dbl&gt; &lt;chr&gt;           &lt;dbl&gt;        &lt;dbl&gt;          &lt;dbl&gt;         &lt;dbl&gt;   &lt;dbl&gt;\n1   1011 post_tre‚Ä¶          97         0              0.4           97      2021\n2   1012 post_tre‚Ä¶          12         0              0.4           12.5    2022\n3   1013 post_tre‚Ä¶          58         0              0.4           87.5    2022\n4   1014 post_tre‚Ä¶          88         0              0.4          110.     2021\n5   1015 post_tre‚Ä¶          53         0.04           0.36         110.     2017\n6   1016 post_tre‚Ä¶          18         0              0.4           23      2017\n# ‚Ñπ 1 more variable: treat &lt;dbl&gt;\n\n\n\n# Read in waste pile count data\ncounts_gpx &lt;- read_csv(here::here( \"data\", \"waste_pile_counts_gpx.csv\")) %&gt;% \n    rename(\"waste_piles\" = \"total\",\n           \"rain_0_48hrs\" = \"rf_0_to_48_hours\",\n           \"rain_49_168hrs\" = \"rf_49_to_168_hours\") %&gt;% \n    filter(survey%in%c(\n      \"post_treatment_1\",\"post_treatment_2\",\"post_treatment_3\",\n      \"post_treatment_4\",\"post_treatment_5\"))\n\n# Select subset of post-treatment periods\npost_treat_subset &lt;- counts_gpx %&gt;% \n   filter(survey == \"post_treatment_4\")\n\nhead(post_treat_subset)\n\nNow that we got our data loaded in, lets take a look at what these variables mean.\n\n\n\n\n\n\n\nLabel\nDescription\n\n\n\n\nzoneid\nZone or neighborhood ID: The observational unit (\\(N=44\\))\n\n\nwaste_piles\nThe outcome variable (\\(Y\\)): The number of waste pile burns recorded (Range: 5, 125)\n\n\ntreat\nThe treatment assignment variable (0 = Control Group; 1 = Treatment Group)\n\n\nrf_0_to_48_hours\nThe rainfall estimated between 0 and 48 hours prior to the start of the audit, derived from NASA MEERA-2 corrected precipitation estimates (mm)\n\n\nrf_49_to_168_hours\nThe rainfall estimated between 49 and 168 hours prior to the start of the audit, derived from NASA MEERA-2 corrected precipitation estimates (mm)\n\n\npre_count_avg\nThe average number of burnt piles in the pre-treatment audits, spread to zone level\n\n\npair_id\nThe unique ID assigned to each pair of zones for assignment to treatment\n\n\n\n\n\nModel 1: Simple OLS (Ordinary Least Squares) estimator\nOLS estimator (specification):\n\\[waste\\ piles =\\beta_0 + \\beta_1(treat) + \\epsilon\\]\n\nm1_ols &lt;- lm(\n    waste_piles ~ treat,\n    data = post_treat_subset)\n\nsumm(m1_ols, model.fit = FALSE)\n\n\n\n\n\nObservations\n44\n\n\nDependent variable\nwaste_piles\n\n\nType\nOLS linear regression\n\n\n\n\n  \n\n\n\n\n\nEst.\nS.E.\nt val.\np\n\n\n\n\n(Intercept)\n77.91\n4.16\n18.73\n0.00\n\n\ntreat\n-24.77\n5.88\n-4.21\n0.00\n\n\n\n Standard errors: OLS\n\n\n\n\n\n\n\n\n\n\n\n\nAre we making reasonable assumptions?\nRemember questioning (and testing) assumptions is the heart of causal inference!We want to make sure our assumptions our reasonable. To do so, let‚Äôstake a quick look at our outcome variable waste piles, and then look at the normality of our residuals.\n\npost_treat_subset %&gt;%\n  ggplot(aes(x = waste_piles)) +\n  geom_histogram(binwidth = 5, fill = \"blue\", color = \"white\", alpha = 0.7) +\n  labs(title = \"Histogram of Waste Piles\",\n    x = \"Waste Piles (counts)\",\n    y = \"Count\") +\n    theme_minimal()\n\n\n\n\n\n\n\n\nWhile our data isn‚Äôt a perfect bell curve, it also isn‚Äôt severely skewed. Let‚Äôs do another assumption check (normality of residuals) to help us decide if OLS is the right model for us.\nTo do so, we will make a QQ plot displaying residuals (y-axis) compared to the normal distribution (x-axis).\n\ncheck_model(m1_ols,  check = \"qq\" )\n\n\n\n\n\n\n\n\nHmm, it doesn‚Äôt seem like the OLS assumption, normality of residuals is a good fit for our data. Onto the next model!\n\n\n\n\n\n\nTipDiscussion\n\n\n\nSince we ruled OLS out, talk with your neighbor about another potential model to try. What kind of variable is our outcome variable? How might that inform what model we try next?\n\n\nWe can ‚Äúrelax‚Äù our assumptions by using a Generalized Linear model! Specifically, we will relax the assumption that the residuals are normally distributed.\n\n\n\n\n\n\nWarningWarning!\n\n\n\nWhen we relax an assumption and make our model more flexible, the statistics and interpretation become more complex!\n\n\nAs you discussed with your partner, our dependent variable, waste_piles is a count variable, not a continuous variable. Therefore, it‚Äôs best we pick a model that explicitly accounts for count outcomes. In walks the poisson regression model!\n\n\n\nModel 2 : Poisson Generalized Linear Regression Model\nWe are going to assume Y follows a Poisson distribution with non negative integers (counts!). To do so, we need to think about the assumptions the Poisson regression makes. This model assumes that variance (dispersion) is proportional to the mean. Does our data match the theoretical distribution proposed? Let‚Äôs check!\n\nlambda_hat &lt;- mean(post_treat_subset$waste_piles)\n\npoisson_curve &lt;- tibble(\n  x = seq(0, max(post_treat_subset$waste_piles), by = 1),  # Range of x values\n  density = dpois(seq(0, max(post_treat_subset$waste_piles), by = 1), \n                  lambda = mean(post_treat_subset$waste_piles)) \n)\n\nggplot(post_treat_subset, aes(x = waste_piles)) +\n  geom_histogram(aes(y = ..density..), binwidth = 5, color = \"white\", fill = \"blue\", alpha = 0.7) + \n  geom_line(data = poisson_curve, aes(x = x, y = density), color = \"red\", size = 1) +  # Poisson curve\n  geom_density(color = \"green\", size = 1, adjust = 1.5) +  # KDE for a smoother curve\n  labs(\n    title = \"Plot of Empirical (Green) v. Theoretical (Red) Distributions\",\n    x = \"Waste Pile Counts\",\n    y = \"Density\"\n  ) +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\nm2_poisson &lt;- glm(waste_piles ~ treat,\n                  family = poisson(link = \"log\"),\n                  data = post_treat_subset)\n\nsumm(m2_poisson, model.fit = FALSE)\n\n\n\n\n\nObservations\n44\n\n\nDependent variable\nwaste_piles\n\n\nType\nGeneralized linear model\n\n\nFamily\npoisson\n\n\nLink\nlog\n\n\n\n\n  \n\n\n\n\n\nEst.\nS.E.\nz val.\np\n\n\n\n\n(Intercept)\n4.36\n0.02\n180.32\n0.00\n\n\ntreat\n-0.38\n0.04\n-10.09\n0.00\n\n\n\n Standard errors: MLE\n\n\n\n\n\n\n\n\n\n\n\nJust like OLS, we have some assumptions we need to check!\nWe first need to check for over dispersion. This is when our variance exceeds the mean.\n\ncheck_overdispersion(m2_poisson)\n\n# Overdispersion test\n\n       dispersion ratio =   5.873\n  Pearson's Chi-Squared = 246.680\n                p-value = &lt; 0.001\n\n\nIf our dispersion ratio was equal to 1, this would mean our mean is equal to our variance. Since our dispersion ratio is 5.873, and far greater than 1, we can see that our data has significant overdispersion. Poisson isn‚Äôt the model for us!\n\n\n\n\n\n\nTipDiscussion\n\n\n\nSince we ruled OLS and Poisson Regression out, talk with your neighbor about another potential model to try.\n\n\n\n\nModel 3: Log- OLS Regression\nLog Linear Regression is an alternative way to model count outcomes using regular OLS. The only difference is that we are transforming our outcome variable to be on the log scale.\n\nm3_log &lt;- lm(\n    log(waste_piles) ~ treat,\n    data = post_treat_subset)\n\nsumm(m3_log, model.fit = FALSE)\n\n\n\n\n\nObservations\n44\n\n\nDependent variable\nlog(waste_piles)\n\n\nType\nOLS linear regression\n\n\n\n\n  \n\n\n\n\n\nEst.\nS.E.\nt val.\np\n\n\n\n\n(Intercept)\n4.31\n0.08\n51.97\n0.00\n\n\ntreat\n-0.42\n0.12\n-3.57\n0.00\n\n\n\n Standard errors: OLS\n\n\n\n\n\n\n\n\n\n\n\nWe can compare our observed data to simulated data based on the model we just created (m3_log).\n\ncheck_predictions(m3_log, verbose = FALSE)\n\nWarning: Minimum value of original data is not included in the\n  replicated data.\n  Model may not capture the variation of the data.\n\n\n\n\n\n\n\n\n\n\n\nModel Recap\nLet‚Äôs take a look at the three models we have created thus far and compare their results.\n\nexport_summs(m1_ols, m2_poisson, m3_log,\n             model.names = c(\"OLS\",\"Poisson GLM\",\"Log(Outcome) OLS\"),\n             statistics = \"none\")\n\n\n\n\nOLSPoisson GLMLog(Outcome) OLS\n\n\n\n(Intercept)77.91 ***4.36 ***4.31 ***\n\n(4.16)¬†¬†¬†(0.02)¬†¬†¬†(0.08)¬†¬†¬†\n\ntreat-24.77 ***-0.38 ***-0.42 ***\n\n(5.88)¬†¬†¬†(0.04)¬†¬†¬†(0.12)¬†¬†¬†\n\n*** p &lt; 0.001; ** p &lt; 0.01; * p &lt; 0.05.\n\n\n\n\n\nLet‚Äôs do some quick math to compare treatment effect estimates\n\\[\\% \\Delta Y = \\left( e^{\\beta_1} - 1 \\right) \\times 100\\]\n\n# Calculate percent change in waste piles in each model:\n\nm1_change = (-24.77/77.91)*100   ## % change OLS = -31.8\nm2_change = (exp(-.38) - 1)*100  ## % change POIS = -31.6\nm3_change = (exp(-0.42) - 1)*100 ## % change LOG-OLS = -34.3\n\n\n\n\n\n\n\nTipTreatment Estimate Check In\n\n\n\nInterpret the treatment estimate for the log OLS.\n\n\nThe treatment estimate for log-OLS can be interpret as follows:\n\nThe model estimated that the treatment group had a 34% reduction in waste piles relative to the control group.\n\nLet‚Äôs also take a look at the covariate balance table across treatment conditions.\n\npost_treat_subset %&gt;% \n    select(treat, waste_piles, pre_count_avg) %&gt;% \n    tbl_summary(\n        by = treat,\n        statistic = list(all_continuous() ~ \"{mean} ({sd})\")) %&gt;% \n    modify_header(label ~ \"**Variable**\") %&gt;%\n    modify_spanning_header(c(\"stat_1\", \"stat_2\") ~ \"**Treatment**\") \n\n\n\n\n\n\n\n\n\n\n\n\nVariable\n\nTreatment\n\n\n\n0\nN = 221\n1\nN = 221\n\n\n\n\nwaste_piles\n78 (21)\n53 (18)\n\n\npre_count_avg\n94 (18)\n84 (26)\n\n\n\n1 Mean (SD)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTipDiscussion\n\n\n\nWhat potential issues do you notice with the pre_count_avg variable?\n\n\nThis takes us to our next model - controlling for pre_count_avg!\n\n\n\nModel 4: Log- OLS Regression with a control variable\n\nm4_control &lt;- lm(\n    log(waste_piles) ~ \n        treat +\n        log(pre_count_avg), ## Add a control\n    data = post_treat_subset)\n\nsumm(m4_control, model.fit = FALSE)\n\n\n\n\n\nObservations\n44\n\n\nDependent variable\nlog(waste_piles)\n\n\nType\nOLS linear regression\n\n\n\n\n  \n\n\n\n\n\nEst.\nS.E.\nt val.\np\n\n\n\n\n(Intercept)\n0.47\n0.35\n1.34\n0.19\n\n\ntreat\n-0.27\n0.06\n-4.44\n0.00\n\n\nlog(pre_count_avg)\n0.85\n0.08\n11.09\n0.00\n\n\n\n Standard errors: OLS\n\n\n\n\n\n\n\n\n\n\n\nLet‚Äôs compare our two log OLS Regression Models.\n\nexport_summs(m3_log, m4_control,\n             model.names = c(\"Log OLS Regression\",\"Log OLS Regression with a control variable\"),\n             statistics = \"none\")\n\n\n\n\nLog OLS RegressionLog OLS Regression with a control variable\n\n\n\n(Intercept)4.31 ***0.47¬†¬†¬†¬†\n\n(0.08)¬†¬†¬†(0.35)¬†¬†¬†\n\ntreat-0.42 ***-0.27 ***\n\n(0.12)¬†¬†¬†(0.06)¬†¬†¬†\n\nlog(pre_count_avg)¬†¬†¬†¬†¬†¬†¬†0.85 ***\n\n¬†¬†¬†¬†¬†¬†¬†(0.08)¬†¬†¬†\n\n*** p &lt; 0.001; ** p &lt; 0.01; * p &lt; 0.05.\n\n\n\n\nWe have now controlled for pre_count_avg, but let‚Äôs think about what other variables we could control for to improve our model. Other variables in our data include:\n\n\n\n\n\n\n\nLabel\nDescription\n\n\n\n\nrf_0_to_48_hours\nThe rainfall estimated between 0 and 48 hours prior to the start of the audit, derived from NASA MEERA-2 corrected precipitation estimates (mm)\n\n\nrf_49_to_168_hours\nThe rainfall estimated between 49 and 168 hours prior to the start of the audit, derived from NASA MEERA-2 corrected precipitation estimates (mm)\n\n\n\nThe environment doesn‚Äôt sit still during our experiment, so let‚Äôs control for it! We should account for events that might also affect trash burning during the treatment period (like rainfall)\nLet‚Äôs start by looking at a covariate balance table for across treatment conditions for average rain events.\n\npost_treat_subset %&gt;% \n    select(treat, waste_piles, rain_0_48hrs, rain_49_168hrs) %&gt;% \n    tbl_summary(\n        by = treat,\n        statistic = list(all_continuous() ~ \"{mean} ({sd})\")) %&gt;% \n    modify_header(label ~ \"**Variable**\") %&gt;% \n    modify_spanning_header(c(\"stat_1\", \"stat_2\") ~ \"**Treatment**\") \n\n\n\n\n\n\n\n\n\n\n\n\nVariable\n\nTreatment\n\n\n\n0\nN = 221\n1\nN = 221\n\n\n\n\nwaste_piles\n78 (21)\n53 (18)\n\n\nrain_0_48hrs\n0.18 (0.15)\n0.20 (0.16)\n\n\nrain_49_168hrs\n0.17 (0.17)\n0.17 (0.17)\n\n\n\n1 Mean (SD)\n\n\n\n\n\n\n\n\n\nm5_control2 &lt;- lm(\n    log(waste_piles) ~ \n        treat +\n        log(pre_count_avg) +\n        rain_0_48hrs + rain_49_168hrs,\n    data = post_treat_subset)\n\nsumm(m5_control2,\n     model.fit = FALSE,\n     robust = \"HC2\")  # Heteroskedasticity robust standard error adj.\n\n\n\n\n\nObservations\n44\n\n\nDependent variable\nlog(waste_piles)\n\n\nType\nOLS linear regression\n\n\n\n\n  \n\n\n\n\n\nEst.\nS.E.\nt val.\np\n\n\n\n\n(Intercept)\n0.43\n0.45\n0.95\n0.35\n\n\ntreat\n-0.27\n0.06\n-4.29\n0.00\n\n\nlog(pre_count_avg)\n0.86\n0.07\n12.51\n0.00\n\n\nrain_0_48hrs\n-0.02\n0.85\n-0.03\n0.98\n\n\nrain_49_168hrs\n0.07\n0.79\n0.09\n0.93\n\n\n\n Standard errors: Robust, type = HC2\n\n\n\n\n\n\n\n\n\n\n\nNow that we have two fixed effects models, lets look at our results side by side.\n\nexport_summs(m4_control, m5_control2, robust = \"HC2\",\n             model.names = c(\"M4 - Pre-treat\",\"M5 - Add Rain\"),\n             statistics = \"none\")\n\n\n\n\nM4 - Pre-treatM5 - Add Rain\n\n\n\n(Intercept)0.47¬†¬†¬†¬†0.43¬†¬†¬†¬†\n\n(0.33)¬†¬†¬†(0.45)¬†¬†¬†\n\ntreat-0.27 ***-0.27 ***\n\n(0.06)¬†¬†¬†(0.06)¬†¬†¬†\n\nlog(pre_count_avg)0.85 ***0.86 ***\n\n(0.07)¬†¬†¬†(0.07)¬†¬†¬†\n\nrain_0_48hrs¬†¬†¬†¬†¬†¬†¬†-0.02¬†¬†¬†¬†\n\n¬†¬†¬†¬†¬†¬†¬†(0.85)¬†¬†¬†\n\nrain_49_168hrs¬†¬†¬†¬†¬†¬†¬†0.07¬†¬†¬†¬†\n\n¬†¬†¬†¬†¬†¬†¬†(0.79)¬†¬†¬†\n\nStandard errors are heteroskedasticity robust. *** p &lt; 0.001; ** p &lt; 0.01; * p &lt; 0.05.\n\n\n\n\n\n\n\n\n\n\nTipAre neighborhoods independent?\n\n\n\nWe have another issue which may bias our treatment effect - the neighborhoods were paired together. We have to account for this paired data feature which is called clustered data. Clustered data violates the regression assumption that observations are independent and identically distributed (i.i.d). In-other-words, the data has within group dependencies, paired neighborhoods will likely be more similar than un-paired neighborhoods.\n\n\nLets account for these paired neighborhoods by adding a standard error adjustment based on our clusters. We can make this adjustment with the fixed effects model we just created (m5_control2) .\n\nsumm(m5_control2,\n     robust = \"HC2\",\n     cluster = \"pair_id\", # Add SE adj. based on pair-clusters\n     model.fit = FALSE)\n\n\n\n\n\nObservations\n44\n\n\nDependent variable\nlog(waste_piles)\n\n\nType\nOLS linear regression\n\n\n\n\n  \n\n\n\n\n\nEst.\nS.E.\nt val.\np\n\n\n\n\n(Intercept)\n0.43\n0.49\n0.88\n0.39\n\n\ntreat\n-0.27\n0.07\n-3.63\n0.00\n\n\nlog(pre_count_avg)\n0.86\n0.07\n12.44\n0.00\n\n\nrain_0_48hrs\n-0.02\n0.97\n-0.03\n0.98\n\n\nrain_49_168hrs\n0.07\n0.94\n0.07\n0.94\n\n\n\n Standard errors: Cluster-robust, type = HC2\n\n\n\n\n\n\n\n\n\n\n\nWhat are the key takeaways from this summary? Discuss with your neighbor!"
  },
  {
    "objectID": "course-materials/labs/week2_test.html",
    "href": "course-materials/labs/week2_test.html",
    "title": "week3.qmd",
    "section": "",
    "text": "Figure: Map of Treatment Assignment Zones in Nansana, Uganda\n* SOURCE: Figure from Buntaine et al., 2024 (S5; Supplemental Materials)\n\n\n\nOpen source data & applied study:\nOpen access data was utilized for this class exercise from the study (Buntaine et al., 2024):\n\nBuntaine, M. T., Komakech, P., & Shen, S. V. (2024). Social competition drives collective action to reduce informal waste burning in Uganda. Proceedings of the National Academy of Sciences, 121(23). https://doi.org/10.1073/pnas.2319712121\n\n\n\nGoals of this exercise\n\nStart with a simple OLS model\nDemonstrate the rationale for estimating progressively complex models\nIllustrate a causal inference approach (i.e., identify robust estimators)\nConclusion: What is the take-home message of the Lab-1 exercise? . . .\nEstimating complex models is a relatively easy part of our work as data scientists‚Ä¶\n\nMaking informed specification decisions & communicating results is the hard part (i.e., the science side of data science)!\n\n\n\n\nLet‚Äôs get started!\nLoad packages and data:\n\nlibrary(tidyverse)   # Keeping things tidy \n\n‚îÄ‚îÄ Attaching core tidyverse packages ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ tidyverse 2.0.0 ‚îÄ‚îÄ\n‚úî dplyr     1.1.4     ‚úî readr     2.1.5\n‚úî forcats   1.0.1     ‚úî stringr   1.5.2\n‚úî ggplot2   4.0.0     ‚úî tibble    3.3.0\n‚úî lubridate 1.9.4     ‚úî tidyr     1.3.1\n‚úî purrr     1.1.0     \n‚îÄ‚îÄ Conflicts ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ tidyverse_conflicts() ‚îÄ‚îÄ\n‚úñ dplyr::filter() masks stats::filter()\n‚úñ dplyr::lag()    masks stats::lag()\n‚Ñπ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(janitor)     # Housekeeping \n\n\nAttaching package: 'janitor'\n\nThe following objects are masked from 'package:stats':\n\n    chisq.test, fisher.test\n\nlibrary(here)        # Location, location, location \n\nhere() starts at /Users/annalieseadams/Git/W26-Policy-Eval\n\n#install.packages(\"jtools\")\nlibrary(jtools)      # Pretty regression output \nlibrary(gt)          # Tables \n\nWarning: package 'gt' was built under R version 4.5.2\n\nlibrary(gtsummary)   # Table for checking balance \n\nWarning: package 'gtsummary' was built under R version 4.5.2\n\n#install.packages(\"performance\")\nlibrary(performance) # Check model diagnostics \n\n\n# Read in waste pile count data\n\ncounts_gpx &lt;- read_csv(\"https://raw.github.com/garberadamc/Lab1-EDS241-Buntaine24/main/data/waste_pile_counts_gpx.csv\") %&gt;% \n    rename(\"waste_piles\" = \"total\",\n           \"rain_0_48hrs\" = \"rf_0_to_48_hours\",\n           \"rain_49_168hrs\" = \"rf_49_to_168_hours\") %&gt;% \n    filter(survey%in%c(\n      \"post_treatment_1\",\"post_treatment_2\",\"post_treatment_3\",\n      \"post_treatment_4\",\"post_treatment_5\"))\n\nRows: 308 Columns: 8\n‚îÄ‚îÄ Column specification ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\nDelimiter: \",\"\nchr (1): survey\ndbl (7): zoneid, total, rf_0_to_48_hours, rf_49_to_168_hours, pre_count_avg,...\n\n‚Ñπ Use `spec()` to retrieve the full column specification for this data.\n‚Ñπ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n# Select subset of post-treatment periods (remove time point 5)\npost_treat_subset &lt;- counts_gpx %&gt;% \n   filter(survey == \"post_treatment_4\")\n\nNow that we got our data loaded in, lets take a look at what these variables mean."
  },
  {
    "objectID": "course-materials/labs/week2_notes.html",
    "href": "course-materials/labs/week2_notes.html",
    "title": "Lab 2: Generalize Fast!",
    "section": "",
    "text": "Map of Treatment Assignment Zones in Nansana, Uganda\n\n* SOURCE: Figure from Buntaine et al., 2024 (S5; Supplemental Materials)\n\n\nOpen source data & applied study:\nOpen access data was utilized for this class exercise from the study (Buntaine et al., 2024):\n\nBuntaine, M. T., Komakech, P., & Shen, S. V. (2024). Social competition drives collective action to reduce informal waste burning in Uganda. Proceedings of the National Academy of Sciences, 121(23). https://doi.org/10.1073/pnas.2319712121\n\n\n\nGoals of this exercise\n\nStart with a simple OLS model\nDemonstrate the rationale for estimating progressively complex models\nIllustrate a causal inference approach (i.e., identify robust estimators)\nConclusion: What is the take-home message of the Lab-1 exercise? . . .\nEstimating complex models is a relatively easy part of our work as data scientists‚Ä¶\n\nMaking informed specification decisions & communicating results is the hard part (i.e., the science side of data science)!\n\n\n\n\nLet‚Äôs get started!\nLoad packages and data:\n\nlibrary(tidyverse)   # Keeping things tidy \nlibrary(janitor)     # Housekeeping \nlibrary(here)        # Location, location, location \n#install.packages(\"jtools\")\nlibrary(jtools)      # Pretty regression output \nlibrary(gt)          # Tables \nlibrary(gtsummary)   # Table for checking balance \n#install.packages(\"performance\")\nlibrary(performance) # Check model diagnostics \n#install.packages(\"see\") # Visualization engine for performance package\n#install.packages(\"huxtable\") # Package required for export_summs\n\n\n# Read in waste pile count data\ncounts_gpx &lt;- read_csv(here::here(\"course-materials\", \"labs\", \"data\", \"waste_pile_counts_gpx.csv\")) %&gt;% \n    rename(\"waste_piles\" = \"total\",\n           \"rain_0_48hrs\" = \"rf_0_to_48_hours\",\n           \"rain_49_168hrs\" = \"rf_49_to_168_hours\") %&gt;% \n    filter(survey%in%c(\n      \"post_treatment_1\",\"post_treatment_2\",\"post_treatment_3\",\n      \"post_treatment_4\",\"post_treatment_5\"))\n\n# Select subset of post-treatment periods \npost_treat_subset &lt;- counts_gpx %&gt;% \n   filter(survey == \"post_treatment_4\")\n\nhead(post_treat_subset)\n\n# A tibble: 6 √ó 8\n  zoneid survey    waste_piles rain_0_48hrs rain_49_168hrs pre_count_avg pair_id\n   &lt;dbl&gt; &lt;chr&gt;           &lt;dbl&gt;        &lt;dbl&gt;          &lt;dbl&gt;         &lt;dbl&gt;   &lt;dbl&gt;\n1   1011 post_tre‚Ä¶          97         0              0.4           97      2021\n2   1012 post_tre‚Ä¶          12         0              0.4           12.5    2022\n3   1013 post_tre‚Ä¶          58         0              0.4           87.5    2022\n4   1014 post_tre‚Ä¶          88         0              0.4          110.     2021\n5   1015 post_tre‚Ä¶          53         0.04           0.36         110.     2017\n6   1016 post_tre‚Ä¶          18         0              0.4           23      2017\n# ‚Ñπ 1 more variable: treat &lt;dbl&gt;\n\n\nNow that we got our data loaded in, lets take a look at what these variables mean.\n\n\n\n\n\n\n\nLabel\nDescription\n\n\n\n\nzoneid\nZone or neighborhood ID: The observational unit (\\(N=44\\))\n\n\nwaste_piles\nThe outcome variable (\\(Y\\)): The number of waste pile burns recorded (Range: 5, 125)\n\n\ntreat\nThe treatment assignment variable (0 = Control Group; 1 = Treatment Group)\n\n\nrf_0_to_48_hours\nThe rainfall estimated between 0 and 48 hours prior to the start of the audit, derived from NASA MEERA-2 corrected precipitation estimates (mm)\n\n\nrf_49_to_168_hours\nThe rainfall estimated between 49 and 168 hours prior to the start of the audit, derived from NASA MEERA-2 corrected precipitation estimates (mm)\n\n\npre_count_avg\nThe average number of burnt piles in the pre-treatment audits, spread to zone level\n\n\npair_id\nThe unique ID assigned to each pair of zones for assignment to treatment\n\n\n\n\n\nModel 1: Simple OLS (Ordinary Least Squares) estimator\nOLS estimator (specification):\n\\[waste\\ piles =\\beta_0 + \\beta_1(treat) + \\epsilon\\]\n\nm1_ols &lt;- lm(\n    waste_piles ~ treat,\n    data = post_treat_subset)\n\nsumm(m1_ols, model.fit = FALSE) # model.fit = TRUE explains accuracy reports like R2\n\n\n\n\n\nObservations\n44\n\n\nDependent variable\nwaste_piles\n\n\nType\nOLS linear regression\n\n\n\n\n  \n\n\n\n\n\nEst.\nS.E.\nt val.\np\n\n\n\n\n(Intercept)\n77.91\n4.16\n18.73\n0.00\n\n\ntreat\n-24.77\n5.88\n-4.21\n0.00\n\n\n\n Standard errors: OLS\n\n\n\n\n\n\n\n\n\n\n\nInterpretatation: - interecept coeff: On average, the control group had 78 waste piles.\n\ntreat coeff: On average, the treatment groups had 25 less waste piles than the control group. or : The treatment reduced the average number of trash piles by approximately 25 piles\ninterecept SE: The true control average likely falls within 4.16 trash piles of 78.\ntreat SE: the true treatment effect likely falls within about +- 5.88 trash piles of -24.77\n\nt value (est/SE): - larger t value = stronger evidence the effect is real.\n\nAre we making reasonable assumptions?\nRemember questioning (and testing) assumptions is the heart of causal inference!We want to make sure our assumptions our reasonable. To do so, let‚Äôstake a quick look at our outcome variable waste piles, and then look at the normality of our residuals.\n\npost_treat_subset %&gt;%\n  ggplot(aes(x = waste_piles)) +\n  geom_histogram(binwidth = 5, fill = \"blue\", color = \"white\", alpha = 0.7) +\n  labs(title = \"Histogram of Waste Piles\",\n    x = \"Waste Piles (counts)\",\n    y = \"Count\") +\n    theme_minimal()\n\n\n\n\n\n\n\n\nWhile our data isn‚Äôt a perfect bell curve, it also isn‚Äôt severely skewed. Let‚Äôs do another assumption check (normality of residuals) to help us decide if OLS is the right model for us.\nTo do so, we will make a QQ plot displaying residuals (y-axis) compared to the normal distribution (x-axis).\n\ncheck_model(m1_ols,  check = \"qq\" )\n\n\n\n\n\n\n\n# de trended qq plot, where rather than y = x ( goal is a diag line), we want y - x = 0 (goal is a horizontal line). Makes errors easier to see\n\nHmm, it doesn‚Äôt seem like the OLS assumption, normality of residuals is a good fit for our data. Onto the next model!\n\n\n\n\n\n\nTipDiscussion\n\n\n\nSince we ruled OLS out, talk with your neighbor about another potential model to try. What kind of variable is our outcome variable? How might that inform what model we try next?\n\n\nWe can ‚Äúrelax‚Äù our assumptions by using a Generalized Linear model! Specifically, we will relax the assumption that the residuals are normally distributed.\n\n\n\n\n\n\nWarningWarning!\n\n\n\nWhen we relax an assumption and make our model more flexible, the statistics and interpretation become more complex!\n\n\nAs you discussed with your partner, our dependent variable, waste_piles is a count variable, not a continuous variable. Therefore, it‚Äôs best we pick a model that explicitly accounts for count outcomes. In walks the poisson regression model!\n\n\n\nModel 2 : Poisson Generalized Linear Regression Model\nWe are going to assume Y follows a Poisson distribution with non negative integers (counts!). To do so, we need to think about the assumptions the Poisson regression makes. This model assumes that variance (dispersion) is proportional to the mean. Does our data match the theoretical distribution proposed? Let‚Äôs check!\n\nlambda_hat &lt;- mean(post_treat_subset$waste_piles)\n\n# Generate theoretical poisson data\npoisson_curve &lt;- tibble(\n  # Create a sequence of integers from 0 to max waste pile count\n  x = seq(0, max(post_treat_subset$waste_piles), by = 1),  \n  \n  # Calculate the probability (density) for each x value using the poisson formula\n  density = dpois(seq(0, max(post_treat_subset$waste_piles), by = 1), \n                  lambda = mean(post_treat_subset$waste_piles)) \n)\n\nggplot(post_treat_subset, aes(x = waste_piles)) +\n  # ..desnity .. resizes histogram bars so they can be compared to probability curve\n  geom_histogram(aes(y = ..density..), binwidth = 5, color = \"white\", fill = \"blue\", alpha = 0.7) + \n  # red line is showing us what poisson dist w averge of our data should look like\n  geom_line(data = poisson_curve, aes(x = x, y = density), color = \"red\", size = 1) +  # Poisson curve\n  # smoothed density line of our histogram \n  geom_density(color = \"green\", size = 1, adjust = 1.5) +  # KDE for a smoother curve\n  labs(\n    title = \"Plot of Empirical (Green) v. Theoretical (Red) Distributions\",\n    x = \"Waste Pile Counts\",\n    y = \"Density\"\n  ) +\n  theme_minimal()\n\n\n\n\n\n\n\n\nRed is what our dist should look like for poisson. Green is our actual data. Not a great fit! Our variance is much larger than our mean (overdispersion).\n\nm2_poisson &lt;- glm(waste_piles ~ treat,\n                  family = poisson(link = \"log\"), # log bc it resticts outcome to positive values and changes model to be multiplicative ( percent change formula when we interpret)\n                  data = post_treat_subset)\n\nsumm(m2_poisson, model.fit = FALSE)\n\n\n\n\n\nObservations\n44\n\n\nDependent variable\nwaste_piles\n\n\nType\nGeneralized linear model\n\n\nFamily\npoisson\n\n\nLink\nlog\n\n\n\n\n  \n\n\n\n\n\nEst.\nS.E.\nz val.\np\n\n\n\n\n(Intercept)\n4.36\n0.02\n180.32\n0.00\n\n\ntreat\n-0.38\n0.04\n-10.09\n0.00\n\n\n\n Standard errors: MLE\n\n\n\n\n\n\n\n\n\n\n\nTo get percent change: (e^beta_1 -1) * 100 - z test here bc variance is assumed/known from dist. IN OLS it is estimated from data but not known. - p value for int: H0: is log(control mean) = 0, alt: not euqal to zero. p = 0 means we reject null hypothesis that control has 1 waste pile\nInterpret:\nIntercept: On average, our control group has 78 (exp(4.36)) waste piles. Treat coeff: exp(-.038) = .68. .68 - 1 = -.32. This model estimates that treatment reduced waste burning by 32% in treated neighborhoods.\nJust like OLS, we have some assumptions we need to check!\nWe first need to check for over dispersion. This is when our variance exceeds the mean.\n\ncheck_overdispersion(m2_poisson)\n\n# Overdispersion test\n\n       dispersion ratio =   5.873\n  Pearson's Chi-Squared = 246.680\n                p-value = &lt; 0.001\n\n\nIf our dispersion ratio was equal to 1, this would mean our mean is equal to our variance. Since our dispersion ratio is 5.873, and far greater than 1, we can see that our data has significant overdispersion. Poisson isn‚Äôt the model for us!\n\n\n\n\n\n\nTipDiscussion\n\n\n\nSince we ruled OLS and Poisson Regression out, talk with your neighbor about another potential model to try.\n\n\n** Note, mabe negative binomial would be good here. Negative binomial assumes variance &gt;mean\n\n\nModel 3: Log- OLS Regression\nLog Linear Regression is an alternative way to model count outcomes using regular OLS. The only difference is that we are transforming our outcome variable to be on the log scale.\nWhy ? Helps to fix non normality issue! Prevents negative outcomes! percent change interpretation which is often better than direct counts ( more interpretable)\n\nm3_log &lt;- lm(\n    log(waste_piles) ~ treat,\n    data = post_treat_subset)\n\nsumm(m3_log, model.fit = FALSE)\n\n\n\n\n\nObservations\n44\n\n\nDependent variable\nlog(waste_piles)\n\n\nType\nOLS linear regression\n\n\n\n\n  \n\n\n\n\n\nEst.\nS.E.\nt val.\np\n\n\n\n\n(Intercept)\n4.31\n0.08\n51.97\n0.00\n\n\ntreat\n-0.42\n0.12\n-3.57\n0.00\n\n\n\n Standard errors: OLS\n\n\n\n\n\n\n\n\n\n\n\ninterpretation:\nintercept: exp(4.31) = 74.44. There are 74 waste piles on average in the control group.\ntreat coeff: exp(-0.42) = .656. .657 - 1 = .35. This model estimates that treatment reduced waste burning by an average of 35% in treated neighborhoods.\nWe can compare our observed data to simulated data based on the model we just created (m3_log).\n\ncheck_predictions(m3_log, verbose = FALSE)\n\nWarning: Minimum value of original data is not included in the\n  replicated data.\n  Model may not capture the variation of the data.\n\n\n\n\n\n\n\n\n\ngenerating fake waste pile count data and using our model.\n\n\nModel Recap\nLet‚Äôs take a look at the three models we have created thus far and compare their results.\n\nexport_summs(m1_ols, m2_poisson, m3_log,\n             model.names = c(\"OLS\",\"Poisson GLM\",\"Log(Outcome) OLS\"),\n             statistics = \"none\")\n\n\n\n\nOLSPoisson GLMLog(Outcome) OLS\n\n\n\n(Intercept)77.91 ***4.36 ***4.31 ***\n\n(4.16)¬†¬†¬†(0.02)¬†¬†¬†(0.08)¬†¬†¬†\n\ntreat-24.77 ***-0.38 ***-0.42 ***\n\n(5.88)¬†¬†¬†(0.04)¬†¬†¬†(0.12)¬†¬†¬†\n\n*** p &lt; 0.001; ** p &lt; 0.01; * p &lt; 0.05.\n\n\n\n\n\nLet‚Äôs do some quick math to compare treatment effect estimates\n\\[\\% \\Delta Y = \\left( e^{\\beta_1} - 1 \\right) \\times 100\\]\n\n# Calculate percent change in waste piles in each model:\n\nm1_change = (-24.77/77.91)*100   ## % change OLS = -31.8\nm2_change = (exp(-.38) - 1)*100  ## % change POIS = -31.6\nm3_change = (exp(-0.42) - 1)*100 ## % change LOG-OLS = -34.3\n\nCould convert everything to trash pile counts, or to percentages.\nTo percentages: OLS -&gt; percentage ( -24.77/77.91)\nTo trash pile counts:\n\npoisson -&gt; counts: exp(4.36), exp(-0.38)\nlog OLS -&gt; counts: exp(4.31), exp(-0.42)\n\n\n\n\n\n\n\nTipTreatment Estimate Check In\n\n\n\nInterpret the treatment estimate for the log OLS.\n\n\nThe treatment estimate for log-OLS can be interpret as follows:\n\nThe model estimated that the treatment group had, on average, a 34% reduction in waste piles relative to the control group.\n\nLet‚Äôs also take a look at the covariate balance table across treatment conditions.\n\npost_treat_subset %&gt;% \n    select(treat, waste_piles, pre_count_avg) %&gt;% \n    tbl_summary(\n        by = treat,\n        statistic = list(all_continuous() ~ \"{mean} ({sd})\")) %&gt;% \n    modify_header(label ~ \"**Variable**\") %&gt;%\n    modify_spanning_header(c(\"stat_1\", \"stat_2\") ~ \"**Treatment**\") \n\n\n\n\n\n\n\n\n\n\n\n\nVariable\n\nTreatment\n\n\n\n0\nN = 221\n1\nN = 221\n\n\n\n\nwaste_piles\n78 (21)\n53 (18)\n\n\npre_count_avg\n94 (18)\n84 (26)\n\n\n\n1 Mean (SD)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTipDiscussion\n\n\n\nWhat potential issues do you notice with the pre_count_avg variable?\n\n\nThis takes us to our next model - controlling for pre_count_avg!\n\n\n\nModel 4: Log- OLS Regression with a control variable\n\nm4_control &lt;- lm(\n    log(waste_piles) ~ \n        treat +\n        log(pre_count_avg), ## Add a control\n    data = post_treat_subset)\n\nsumm(m4_control, model.fit = FALSE)\n\n\n\n\n\nObservations\n44\n\n\nDependent variable\nlog(waste_piles)\n\n\nType\nOLS linear regression\n\n\n\n\n  \n\n\n\n\n\nEst.\nS.E.\nt val.\np\n\n\n\n\n(Intercept)\n0.47\n0.35\n1.34\n0.19\n\n\ntreat\n-0.27\n0.06\n-4.44\n0.00\n\n\nlog(pre_count_avg)\n0.85\n0.08\n11.09\n0.00\n\n\n\n Standard errors: OLS\n\n\n\n\n\n\n\n\n\n\n\nInterpretation:\n\nInterecept: Our model estimates the expected waste piles for control areas at a pre-treatment average of 1 pile to be exp(0.47) ‚âà 1.6 waste piles, though this is not statistically significant (p = 0.19).\ntreat: The treatment coefficient of -0.27 indicates that, after controlling for baseline waste levels, the treatment reduces waste piles by an average of 24%.\nlog(pre_count_aveage): The coefficient of 0.85 on log(pre_count_avg) means that a 1% increase in baseline waste ( pre count avg) is associated with a 0.85% increase in post-treatment waste.( in log - log models, the coefficient tells you the percent directly! )\n\nLet‚Äôs compare our two log OLS Regression Models.\n\nexport_summs(m3_log, m4_control,\n             model.names = c(\"Log OLS Regression\",\"Log OLS Regression with a control variable\"),\n             statistics = \"none\")\n\n\n\n\nLog OLS RegressionLog OLS Regression with a control variable\n\n\n\n(Intercept)4.31 ***0.47¬†¬†¬†¬†\n\n(0.08)¬†¬†¬†(0.35)¬†¬†¬†\n\ntreat-0.42 ***-0.27 ***\n\n(0.12)¬†¬†¬†(0.06)¬†¬†¬†\n\nlog(pre_count_avg)¬†¬†¬†¬†¬†¬†¬†0.85 ***\n\n¬†¬†¬†¬†¬†¬†¬†(0.08)¬†¬†¬†\n\n*** p &lt; 0.001; ** p &lt; 0.01; * p &lt; 0.05.\n\n\n\n\nWe have now controlled for pre_count_avg, but let‚Äôs think about what other variables we could control for to improve our model. Other variables in our data include:\n\n\n\n\n\n\n\nLabel\nDescription\n\n\n\n\nrf_0_to_48_hours\nThe rainfall estimated between 0 and 48 hours prior to the start of the audit, derived from NASA MEERA-2 corrected precipitation estimates (mm)\n\n\nrf_49_to_168_hours\nThe rainfall estimated between 49 and 168 hours prior to the start of the audit, derived from NASA MEERA-2 corrected precipitation estimates (mm)\n\n\n\nThe environment doesn‚Äôt sit still during our experiment, so let‚Äôs control for it! We should account for events that might also affect trash burning during the treatment period (like rainfall)\nLet‚Äôs start by looking at a covariate balance table for across treatment conditions for average rain events.\n\npost_treat_subset %&gt;% \n    select(treat, waste_piles, rain_0_48hrs, rain_49_168hrs) %&gt;% \n    tbl_summary(\n        by = treat,\n        statistic = list(all_continuous() ~ \"{mean} ({sd})\")) %&gt;% \n    modify_header(label ~ \"**Variable**\") %&gt;% \n    modify_spanning_header(c(\"stat_1\", \"stat_2\") ~ \"**Treatment**\") \n\n\n\n\n\n\n\n\n\n\n\n\nVariable\n\nTreatment\n\n\n\n0\nN = 221\n1\nN = 221\n\n\n\n\nwaste_piles\n78 (21)\n53 (18)\n\n\nrain_0_48hrs\n0.18 (0.15)\n0.20 (0.16)\n\n\nrain_49_168hrs\n0.17 (0.17)\n0.17 (0.17)\n\n\n\n1 Mean (SD)\n\n\n\n\n\n\n\n\n\nm5_control2 &lt;- lm(\n    log(waste_piles) ~ \n        treat +\n        log(pre_count_avg) +\n        rain_0_48hrs + rain_49_168hrs, # dont take log of rain bc rain can be 0, diff relationship -&gt; additive makes more sense for rainfall( each mm of rain adds certain amount of waste)\n    data = post_treat_subset)\n\nsumm(m5_control2,\n     model.fit = FALSE,\n     robust = \"HC2\")  # Heteroskedasticity robust standard error adj.\n\n\n\n\n\nObservations\n44\n\n\nDependent variable\nlog(waste_piles)\n\n\nType\nOLS linear regression\n\n\n\n\n  \n\n\n\n\n\nEst.\nS.E.\nt val.\np\n\n\n\n\n(Intercept)\n0.43\n0.45\n0.95\n0.35\n\n\ntreat\n-0.27\n0.06\n-4.29\n0.00\n\n\nlog(pre_count_avg)\n0.86\n0.07\n12.51\n0.00\n\n\nrain_0_48hrs\n-0.02\n0.85\n-0.03\n0.98\n\n\nrain_49_168hrs\n0.07\n0.79\n0.09\n0.93\n\n\n\n Standard errors: Robust, type = HC2\n\n\n\n\n\n\n\n\n\n\n\nIntercept (0.43): Our model estimates the expected waste piles for control areas with a pre-treatment average of 1 pile and no rainfall to be exp(0.43) ‚âà 1.5 waste piles, though this is not statistically significant (p = 0.35).\nTreatment coefficient (-0.27): The treatment coefficient of -0.27 indicates that, after controlling for baseline waste levels and rainfall, the treatment reduces waste piles by an average of 24% (p &lt; 0.001).\nlog(pre_count_avg) coefficient (0.86): The coefficient of 0.86 on log(pre_count_avg) means that a 1% increase in baseline waste is associated with a 0.86% increase in post-treatment waste.\nrain_0_48hrs coefficient (-0.02): The coefficient of -0.02 suggests that each additional millimeter of rain in the 0-48 hours before measurement is associated with a 2% decrease in waste piles, though this effect is not statistically significant (p = 0.98).\nrain_49_168hrs coefficient (0.07): The coefficient of 0.07 suggests that each additional millimeter of rain in the 49-168 hours before measurement is associated with a 7% increase in waste piles, though this effect is not statistically significant (p = 0.93).\nNow that we have two fixed effects models, lets look at our results side by side.\n\nexport_summs(m4_control, m5_control2, robust = \"HC2\",\n             model.names = c(\"M4 - Pre-treat\",\"M5 - Add Rain\"),\n             statistics = \"none\")\n\n\n\n\nM4 - Pre-treatM5 - Add Rain\n\n\n\n(Intercept)0.47¬†¬†¬†¬†0.43¬†¬†¬†¬†\n\n(0.33)¬†¬†¬†(0.45)¬†¬†¬†\n\ntreat-0.27 ***-0.27 ***\n\n(0.06)¬†¬†¬†(0.06)¬†¬†¬†\n\nlog(pre_count_avg)0.85 ***0.86 ***\n\n(0.07)¬†¬†¬†(0.07)¬†¬†¬†\n\nrain_0_48hrs¬†¬†¬†¬†¬†¬†¬†-0.02¬†¬†¬†¬†\n\n¬†¬†¬†¬†¬†¬†¬†(0.85)¬†¬†¬†\n\nrain_49_168hrs¬†¬†¬†¬†¬†¬†¬†0.07¬†¬†¬†¬†\n\n¬†¬†¬†¬†¬†¬†¬†(0.79)¬†¬†¬†\n\nStandard errors are heteroskedasticity robust. *** p &lt; 0.001; ** p &lt; 0.01; * p &lt; 0.05.\n\n\n\n\n\n\n\n\n\n\nTipAre neighborhoods independent?\n\n\n\nWe have another issue which may bias our treatment effect - the neighborhoods were paired together. We have to account for this paired data feature which is called clustered data. Clustered data violates the regression assumption that observations are independent and identically distributed (i.i.d). In-other-words, the data has within group dependencies, paired neighborhoods will likely be more similar than un-paired neighborhoods.\n\n\nLets account for these paired neighborhoods by adding a standard error adjustment based on our clusters. We can make this adjustment with the fixed effects model we just created (m5_control2) .\n\nsumm(m5_control2,\n     robust = \"HC2\", # HC2 is a specific type of robust standard errors (bias-corrected)\n     cluster = \"pair_id\", # Add SE adj. based on pair-clusters\n     model.fit = FALSE)\n\n\n\n\n\nObservations\n44\n\n\nDependent variable\nlog(waste_piles)\n\n\nType\nOLS linear regression\n\n\n\n\n  \n\n\n\n\n\nEst.\nS.E.\nt val.\np\n\n\n\n\n(Intercept)\n0.43\n0.49\n0.88\n0.39\n\n\ntreat\n-0.27\n0.07\n-3.63\n0.00\n\n\nlog(pre_count_avg)\n0.86\n0.07\n12.44\n0.00\n\n\nrain_0_48hrs\n-0.02\n0.97\n-0.03\n0.98\n\n\nrain_49_168hrs\n0.07\n0.94\n0.07\n0.94\n\n\n\n Standard errors: Cluster-robust, type = HC2\n\n\n\n\n\n\n\n\n\n\n\nWhat are the key takeaways from this summary? Discuss with your neighbor!"
  },
  {
    "objectID": "course-materials/labs/week3.html",
    "href": "course-materials/labs/week3.html",
    "title": "Lab 3: Difference-in-Difference",
    "section": "",
    "text": "Use the Difference-in-Differences (DiD) framework to estimate the impact of COVID-19 lockdowns on air pollution (\\(NO_2\\)).\n\n\n\n\n\n\n\n\nFigure¬†1: Words on the Orpheum theater marquee try to bring levity to the current situation along the normally bustling commercial area on Broadway in downtown Los Angeles. Source: LA Times\n\n\n\n\n\nIn March 2020, counties across the United States responded to the start of what would be the COVID 19 Pandemic. While every state faced the same biological threat, governors responded with vastly different mandates. Some states, like California and New Mexico, implemented strict ‚ÄúStay at Home‚Äù orders early on. Others, like Arizona and Texas, opted for more relaxed restrictions, allowing businesses like golf courses and salons to remain open longer.In this lab, we will use a Difference-in-Difference (DiD) framework to isolate the impact of these varying lockdown levels. By comparing ‚Äústrict‚Äù counties to their more ‚Äúrelaxed‚Äù neighbors, we can determine if the severity of the lockdown had a measurable impact on \\(NO_2\\) levels‚Äî a key indicator of vehicle traffic and urban activity."
  },
  {
    "objectID": "course-materials/labs/week3.html#objective",
    "href": "course-materials/labs/week3.html#objective",
    "title": "Lab 3: Difference-in-Difference",
    "section": "",
    "text": "Use the Difference-in-Differences (DiD) framework to estimate the impact of COVID-19 lockdowns on air pollution (\\(NO_2\\)).\n\n\n\n\n\n\n\n\nFigure¬†1: Words on the Orpheum theater marquee try to bring levity to the current situation along the normally bustling commercial area on Broadway in downtown Los Angeles. Source: LA Times\n\n\n\n\n\nIn March 2020, counties across the United States responded to the start of what would be the COVID 19 Pandemic. While every state faced the same biological threat, governors responded with vastly different mandates. Some states, like California and New Mexico, implemented strict ‚ÄúStay at Home‚Äù orders early on. Others, like Arizona and Texas, opted for more relaxed restrictions, allowing businesses like golf courses and salons to remain open longer.In this lab, we will use a Difference-in-Difference (DiD) framework to isolate the impact of these varying lockdown levels. By comparing ‚Äústrict‚Äù counties to their more ‚Äúrelaxed‚Äù neighbors, we can determine if the severity of the lockdown had a measurable impact on \\(NO_2\\) levels‚Äî a key indicator of vehicle traffic and urban activity."
  },
  {
    "objectID": "course-materials/labs/week3.html#did-1-los-angeles-ca-vs-phoenix-az",
    "href": "course-materials/labs/week3.html#did-1-los-angeles-ca-vs-phoenix-az",
    "title": "Lab 3: Difference-in-Difference",
    "section": "DiD # 1: Los Angeles, CA vs Phoenix, AZ",
    "text": "DiD # 1: Los Angeles, CA vs Phoenix, AZ\nIn this scenario, we compare two major Southwestern hubs that faced very different lockdown mandates in March 2019: Los Angeles, CA and Phoenix, AZ.\nOn March 19th, 2020, California Governor Gavin Newsom issued a Statewide shelter in place order. Phoenix, AZ did not start their lockdown until March 31, 2020. The strictness of the lockdown varied from city to city. In Arizona, hair salons and golf courses were allowed to stay open, while Los Angeles had a stricter lockdown order that did not permit such businesses to operate.\nWe will use these varying lockdown levels to see if the severity/ strictness of the lockdown had an impact on NO2 levels.\n\nOutcome (\\(Y\\)): Nitrogen Dioxide (\\(NO_2\\)), which is primarily produced by vehicle traffic\nTreatment Group: Los Angeles (Early, strict lockdown)\nControl Group: Phoenix (Later, shorter lockdown)\n\n\nThe Question: Can we use Phoenix as a ‚Äúmirror‚Äù for what would have happened in LA without a lockdown?\n\n\nlibrary(tidyverse)\nlibrary(janitor)\nlibrary(lubridate)"
  },
  {
    "objectID": "course-materials/labs/week3.html#did-2-dona-ana-county-nm-and-el-paso-county-tx",
    "href": "course-materials/labs/week3.html#did-2-dona-ana-county-nm-and-el-paso-county-tx",
    "title": "Lab 3: Difference-in-Difference",
    "section": "DiD # 2: Dona Ana County, NM and El Paso County, TX",
    "text": "DiD # 2: Dona Ana County, NM and El Paso County, TX\nTo improve our study, we will look at two counties that share a border: Dona Ana County, NM (Treated) and El Paso County, TX (Control). Because they share the same desert air and weather patterns, El Paso is a much ‚Äúcleaner‚Äù control group for Dona Ana.\n\nOutcome (\\(Y\\)): Nitrogen Dioxide (\\(NO_2\\)), which is primarily produced by vehicle traffic\nTreatment Group: Dona Ana County, NM (Early, strict lockdown)\nControl Group: El Paso County, TX (Later, shorter lockdown)\n\nWe will start with reading in our New Mexico/ Texas data, and checking the parallel trends assumption for these two counties.\n\n\nNM_2019 &lt;- read_csv(here::here(\"week3\", \"data\", \"nm_2019.csv\")) %&gt;% mutate(state = \"NM\", year = 2019, \n      `County FIPS Code`  = as.numeric(`County FIPS Code`))                                                                                   \n\nNM_2020 &lt;- read_csv(here::here(\"week3\", \"data\", \"nm_2020.csv\")) %&gt;%  mutate(state = \"NM\", year = 2020,                       `County FIPS Code`  = as.numeric(`County FIPS Code`))\n\nTX_2019 &lt;- read_csv(here::here(\"week3\", \"data\", \"tx_2019.csv\")) %&gt;% \n  mutate(state = \"TX\", year = 2019)\n\nTX_2020 &lt;- read_csv(here::here(\"week3\", \"data\", \"tx_2020.csv\")) %&gt;% \n  mutate(state = \"TX\", year = 2020)\n\n\n# Combine all into one master data frame\ndid_data_nm_tx &lt;- bind_rows(NM_2019, NM_2020, TX_2019, TX_2020) %&gt;% clean_names()\n\n\nDiD # 2: Parallel Trends Assumption\n\ndid_weekly &lt;- did_data_nm_tx %&gt;%\n  mutate(\n    date_obj = mdy(date),\n    week = floor_date(date_obj, unit = \"week\")\n  ) %&gt;%\n\n  filter(year == 2020, month(date_obj) %in% c(2, 3, 4)) %&gt;%\n  group_by(week, state) %&gt;%\n  summarize(\n    mean_no2 = mean(daily_max_1_hour_no2_concentration, na.rm = TRUE), \n    .groups = \"drop\"\n  )\n\nggplot(did_weekly, aes(x = week, y = mean_no2, color = state)) +\n\n  geom_line(size = 1.2) +\n\n  geom_point(size = 3) +\n  \n  # Vertical line for the NM Lockdown (March 23, 2020)\n  geom_vline(xintercept = as.Date(\"2020-03-23\"), linetype = \"dashed\", color = \"gray30\", size = 1) +\n  annotate(\"label\", x = as.Date(\"2020-03-24\"), y = 35, label = \"NM Lockdown Starts\", hjust = 0, size = 2.5) +\n  labs(\n    title = \"Weekly 2020 NO2 Trends: NM vs TX\",\n    x = \"Date\",\n    y = \"Weekly Avg NO2 (ppb)\",\n    color = \"State\"\n  ) +\n  theme_minimal() +\n  scale_color_brewer(palette = \"Set1\")\n\n\n\n\n\n\n\n\n\nNotice how much more ‚Äúin sync‚Äù these two lines are compared to LA and Phoenix. Why is a border comparison (same weather, same air) usually considered ‚Äúcleaner‚Äù than comparing two distant cities?\n\n\n\nGetting ready for DiD\nIn order to include treatment and time as part of our DiD, we need to create dummy variables for them. We will create a variable treated that will be = 1 for New Mexico (the ‚Äútreated‚Äù group in our scenario will be a stricter lockdown), and will be = 0 for Texas (the ‚Äúcontrol‚Äù group in our scenario will be the more relaxed lockdown.)\nOur is_post variable will denote weather or not the observation is after the intervention (in this case, the start of the lockdown in New Mexico.). is_post will = 0 for observations before March 23,2020, and will = 1 for observations after March 23, 2020.\n\ndid_data_nm_tx &lt;- did_data_nm_tx %&gt;%\n  mutate(\n    date_obj = mdy(date),\n    #treatment = NM, control = TX\n    treated = ifelse(state == \"New Mexico\", 1, 0),\n\n    # Post = after Mar 23\n    is_post = ifelse(date_obj &gt;= as.Date(\"2020-03-23\"), 1, 0)\n  )\n\n\n\nEstimating the treatment effect\nThe simplest form of estimating the treatment effect can be denoted by :\n\\((Y_{\\text{treat, post}} - Y_{\\text{treat, pre}}) - (Y_{\\text{control, post}} - Y_{\\text{control, pre}})\\)\nAnother way to estimate DiD (and the way we will often use in this course), is to use regression.\n\\[y = \\beta_0 + \\beta_1 \\cdot \\text{treat} + \\beta_2 \\cdot \\text{year} + \\beta_3 \\cdot (\\text{treat} \\times \\text{year})\\]\nLet‚Äôs start by manually calculating the treatment effect using the first formula.\n\ny_trt_post &lt;- did_data_nm_tx %&gt;% filter(treated == 1, is_post  == 1, year == 2020,  month(date_obj) %in% c(2, 3, 4)) %&gt;%\n  summarise(mean(daily_max_1_hour_no2_concentration)) %&gt;% pull()\n\ny_trt_pre &lt;- did_data_nm_tx %&gt;% filter(treated == 1, is_post  == 0, year == 2020,  month(date_obj) %in% c(2, 3, 4)) %&gt;%\n  summarise(mean(daily_max_1_hour_no2_concentration)) %&gt;% pull()\n\ny_con_post &lt;- did_data_nm_tx %&gt;% filter(treated == 0, is_post  == 1, year == 2020,  month(date_obj) %in% c(2, 3, 4)) %&gt;%\n  summarise(mean(daily_max_1_hour_no2_concentration)) %&gt;% pull()\n\ny_con_pre &lt;- did_data_nm_tx %&gt;% filter(treated == 0, is_post  == 0, year == 2020,  month(date_obj) %in% c(2, 3, 4)) %&gt;%\n  summarise(mean(daily_max_1_hour_no2_concentration)) %&gt;% pull()\n\ntrt_effect &lt;- (y_trt_post- y_trt_pre) - (y_con_post - y_con_pre)\n\nprint(paste(\"The treatment effect is\", round(trt_effect,4)))\n\n[1] \"The treatment effect is -3.5162\"\n\n\nNow, lets use a regression to estimate our treatment effect and see if we get a similar result!\n\\[y = \\beta_0 + \\beta_1 \\cdot \\text{treat} + \\beta_2 \\cdot \\text{year} + \\beta_3 \\cdot (\\text{treat} \\times \\text{year})\\]\n\ntx_nm_did &lt;- lm(daily_max_1_hour_no2_concentration ~ treated * is_post, \n                 data = filter(did_data_nm_tx, year == 2020, month(date_obj) %in% c(2, 3, 4) ))\n\nsummary(tx_nm_did)\n\n\nCall:\nlm(formula = daily_max_1_hour_no2_concentration ~ treated * is_post, \n    data = filter(did_data_nm_tx, year == 2020, month(date_obj) %in% \n        c(2, 3, 4)))\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-26.019  -5.821  -0.280   6.512  20.681 \n\nCoefficients:\n                Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)       32.863      1.314  25.003  &lt; 2e-16 ***\ntreated          -10.583      1.868  -5.665 6.06e-08 ***\nis_post           -3.943      2.043  -1.930   0.0553 .  \ntreated:is_post   -3.516      2.863  -1.228   0.2210    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 9.386 on 172 degrees of freedom\nMultiple R-squared:  0.3488,    Adjusted R-squared:  0.3374 \nF-statistic: 30.71 on 3 and 172 DF,  p-value: 5.978e-16\n\n\n\n\n\n\n\n\nTipComprehension Check\n\n\n\nWhat does this treatment effect of -3.5 mean? Talk with a partner!\n\n\n\n\n\n\n\n\nWarning\n\n\n\nDoes our 2020 data tell the whole story? What if the drop we were experiencing in NO2 were due to seasonal trends, and were bound to happen with or without a lockdown?"
  },
  {
    "objectID": "course-materials/labs/week3_v2.html",
    "href": "course-materials/labs/week3_v2.html",
    "title": "Lab 3: Difference-in-Difference",
    "section": "",
    "text": "Use the Difference-in-Differences (DiD) framework to estimate the impact of COVID-19 lockdowns on air pollution (\\(NO_2\\))."
  },
  {
    "objectID": "course-materials/labs/week3_v2.html#objective",
    "href": "course-materials/labs/week3_v2.html#objective",
    "title": "Lab 3: Difference-in-Difference",
    "section": "",
    "text": "Use the Difference-in-Differences (DiD) framework to estimate the impact of COVID-19 lockdowns on air pollution (\\(NO_2\\))."
  },
  {
    "objectID": "course-materials/labs/week3_v2.html#did-1-los-angeles-ca-vs-phoenix-az",
    "href": "course-materials/labs/week3_v2.html#did-1-los-angeles-ca-vs-phoenix-az",
    "title": "Lab 3: Difference-in-Difference",
    "section": "DiD # 1: Los Angeles, CA vs Phoenix, AZ",
    "text": "DiD # 1: Los Angeles, CA vs Phoenix, AZ\nIn this scenario, we compare two major Southwestern hubs that faced very different lockdown mandates in March 2019: Los Angeles, CA and Phoenix, AZ.\nOn March 19th, 2020, California Governor Gavin Newsom issued a Statewide shelter in place order. Phoenix, AZ did not start their lockdown until March 31, 2020. The strictness of the lockdown varied from city to city. In Arizona, hair salons and golf courses were allowed to stay open ( https://www.nbcnews.com/news/us-news/arizona-mayors-slam-covid-19-stay-home-order-allows-hair-n1174186), while Los Angeles had a stricter lockdown order that did not permit such businesses to operate.\n\nOutcome (\\(Y\\)): Nitrogen Dioxide (\\(NO_2\\)), which is primarily produced by vehicle traffic\nTreatment Group: Los Angeles (Early, strict lockdown)\nControl Group: Phoenix (Later, shorter lockdown)\n\n\nlibrary(tidyverse)\nlibrary(janitor)\n\n\nRead in LA and Phoenix EPA Data\n\nLA_2019 &lt;- read_csv(here::here(\"course-materials\", \"labs\", \"data\", \"DiD\", \"LA_2019.csv\")) %&gt;%\n  filter(`Local Site Name` == \"West Los Angeles\") %&gt;% mutate(city = \"LA\", year = 2019)\n\n\nLA_2020 &lt;- read_csv(here::here(\"course-materials\", \"labs\", \"data\", \"DiD\", \"LA_2020.csv\")) %&gt;% \n  filter(`Local Site Name` == \"West Los Angeles\") %&gt;% mutate(city = \"LA\", year = 2020)\n\nPhoenix_2019 &lt;- read_csv(here::here(\"course-materials\", \"labs\", \"data\", \"DiD\", \"phoenix_2019.csv\")) %&gt;% \n  filter(`Local Site Name` == \"WEST PHOENIX\") %&gt;% \n  mutate(city = \"Phoenix\", year = 2019)\n\nPhoenix_2020 &lt;- read_csv(here::here(\"course-materials\", \"labs\", \"data\", \"DiD\", \"phoenix_2020.csv\")) %&gt;% \n  filter(`Local Site Name` == \"WEST PHOENIX\") %&gt;% \nmutate(city = \"Phoenix\", year = 2020)\n\n# Combine all into one master data frame\ndid_data_az_ca &lt;- bind_rows(LA_2019, LA_2020, Phoenix_2019, Phoenix_2020) %&gt;% clean_names()\n\n\n\nParallel Trends\nA DiD is only valid if the Treatment and Control groups were moving in the same direction before the policy hit. Let‚Äôs see if we pass the parallel trends assumption.\n\nweekly_la_phx &lt;- did_data_az_ca %&gt;%\n  mutate(\n    date_obj = mdy(date),\n    week = floor_date(date_obj, unit = \"week\")\n  ) %&gt;%\n  filter(year == 2020, month(date_obj) %in% c(2, 3, 4)) %&gt;%\n  group_by(week, city) %&gt;%\n  summarize(\n    mean_no2 = mean(daily_max_1_hour_no2_concentration, na.rm = TRUE), \n    .groups = \"drop\"\n  )\n\n# Create line plot of phx and LA\nggplot(weekly_la_phx, aes(x = week, y = mean_no2, color = city)) +\n  geom_line(size = 1.2) +\n  geom_point(size = 2.5) +\n  \n  # Add CA lockdown start date dashed line\n  geom_vline(xintercept = as.Date(\"2020-03-19\"), linetype = \"dashed\", color = \"gray30\", size = 1) +\n\n  # Add CA lockdown start date label\n  annotate(\"label\", x = as.Date(\"2020-03-20\"), y = 30, \n           label = \"CA Lockdown \\n Starts\", hjust = 0, size = 3) +\n           \n  labs(\n    title = \"Weekly NO2 Trends in 2020: Los Angeles vs. Phoenix\",\n    x = \"Date\",\n    y = \"Weekly Avg NO2 (ppb)\",\n    color = \"City\"\n  ) +\n  theme_minimal() +\n  scale_color_brewer(palette = \"Set1\")\n\nWarning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\n‚Ñπ Please use `linewidth` instead.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTip\n\n\n\nLook at the lines before the dashed vertical line. Are they moving together? If Phoenix is rising while LA is falling (or vice versa) before the lockdown, can we truly blame the lockdown for the difference we see afterward?\n\n\n\nLet‚Äôs move on to two other counties that may display more of a parallel trend."
  },
  {
    "objectID": "course-materials/labs/week3_v2.html#did-2-dona-ana-county-nm-and-el-paso-county-tx",
    "href": "course-materials/labs/week3_v2.html#did-2-dona-ana-county-nm-and-el-paso-county-tx",
    "title": "Lab 3: Difference-in-Difference",
    "section": "DiD # 2: Dona Ana County, NM and El Paso County, TX",
    "text": "DiD # 2: Dona Ana County, NM and El Paso County, TX\nThis DiD is for Dona Ana County in New Mexico and El Paso County in Texas. These are bordering counties that had two different lockdown scenarios.\n\nOutcome (\\(Y\\)): Nitrogen Dioxide (\\(NO_2\\)), which is primarily produced by vehicle traffic\nTreatment Group: Dona Ana County, NM (Early, strict lockdown)\nControl Group: El Paso County, TX (Later, shorter lockdown)\n\nWe will start with reading in our New Mexico/ Texas data, and checking the parallel trends assumption for these two counties.\n\n\nNM_2019 &lt;- read_csv(here::here(\"week3\", \"data\", \"nm_2019.csv\")) %&gt;% mutate(state = \"NM\", year = 2019, \n      `County FIPS Code`  = as.numeric(`County FIPS Code`))                                                                                   \n\nNM_2020 &lt;- read_csv(here::here(\"week3\", \"data\", \"nm_2020.csv\")) %&gt;%  mutate(state = \"NM\", year = 2020,                       `County FIPS Code`  = as.numeric(`County FIPS Code`))\n\nTX_2019 &lt;- read_csv(here::here(\"week3\", \"data\", \"tx_2019.csv\")) %&gt;% \n  mutate(state = \"TX\", year = 2019)\n\nTX_2020 &lt;- read_csv(here::here(\"week3\", \"data\", \"tx_2020.csv\")) %&gt;% \n  mutate(state = \"TX\", year = 2020)\n\n\n# Combine all into one master data frame\ndid_data_nm_tx &lt;- bind_rows(NM_2019, NM_2020, TX_2019, TX_2020) %&gt;% clean_names()\n\n\ndid_weekly &lt;- did_data_nm_tx %&gt;%\n  mutate(\n    date_obj = mdy(date),\n    week = floor_date(date_obj, unit = \"week\")\n  ) %&gt;%\n\n  filter(year == 2020, month(date_obj) %in% c(2, 3, 4)) %&gt;%\n  group_by(week, state) %&gt;%\n  summarize(\n    mean_no2 = mean(daily_max_1_hour_no2_concentration, na.rm = TRUE), \n    .groups = \"drop\"\n  )\n\nggplot(did_weekly, aes(x = week, y = mean_no2, color = state)) +\n\n  geom_line(size = 1.2) +\n\n  geom_point(size = 3) +\n  \n  # Vertical line for the NM Lockdown (March 23, 2020)\n  geom_vline(xintercept = as.Date(\"2020-03-23\"), linetype = \"dashed\", color = \"gray30\", size = 1) +\n  annotate(\"label\", x = as.Date(\"2020-03-24\"), y = 35, label = \"NM Lockdown Starts\", hjust = 0, size = 2.5) +\n  labs(\n    title = \"Weekly 2020 NO2 Trends: NM vs TX\",\n    x = \"Date\",\n    y = \"Weekly Avg NO2 (ppb)\",\n    color = \"State\"\n  ) +\n  theme_minimal() +\n  scale_color_brewer(palette = \"Set1\")\n\n\n\n\n\n\n\n\n\nNotice how much more ‚Äúin sync‚Äù these two lines are compared to LA and Phoenix. Why is a border comparison (same weather, same air) usually considered ‚Äúcleaner‚Äù than comparing two distant cities?\n\n\nGet ready for DiD\nIn order to include treatment and time as part of our DiD, we need to create dummy variables for them. We will create a variable treated that will be = 1 for New Mexico (the ‚Äútreated‚Äù group in our scenario will be a stricter lockdown), and will be = 0 for Texas (the ‚Äúcontrol‚Äù group in our scenario will be the more relaxed lockdown.)\nOur is_post variable will denote weather or not the observation is after the intervention (in this case, the start of the lockdown in New Mexico.). is_post will = 0 for observations before March 23,2020, and will = 1 for observations after March 23, 2020.\n\ndid_data_nm_tx &lt;- did_data_nm_tx %&gt;%\n  mutate(\n    date_obj = mdy(date),\n    #treatment = NM, control = TX\n    treated = ifelse(state == \"New Mexico\", 1, 0),\n\n    # Post = after Mar 23\n    is_post = ifelse(date_obj &gt;= as.Date(\"2020-03-23\"), 1, 0)\n  )\n\n\n\nEstimating the treatment effect\nThe simplest form of estimating the treatment effect can be denoted by :\n\\((Y_{\\text{treat, post}} - Y_{\\text{treat, pre}}) - (Y_{\\text{control, post}} - Y_{\\text{control, pre}})\\)\nAnother way to estimate DiD (and the way we will often use in this course), is to use regression.\n\\[y = \\beta_0 + \\beta_1 \\cdot \\text{treat} + \\beta_2 \\cdot \\text{year} + \\beta_3 \\cdot (\\text{treat} \\times \\text{year})\\]\nLet‚Äôs start by manually calculating the treatment effect using the first formula.\n\ny_trt_post &lt;- did_data_nm_tx %&gt;% filter(treated == 1, is_post  == 1, year == 2020) %&gt;%\n  summarise(mean(daily_max_1_hour_no2_concentration)) %&gt;% pull()\n\ny_trt_pre &lt;- did_data_nm_tx %&gt;% filter(treated == 1, is_post  == 0, year == 2020) %&gt;%\n  summarise(mean(daily_max_1_hour_no2_concentration)) %&gt;% pull()\n\ny_con_post &lt;- did_data_nm_tx %&gt;% filter(treated == 0, is_post  == 1, year == 2020) %&gt;%\n  summarise(mean(daily_max_1_hour_no2_concentration)) %&gt;% pull()\n\ny_con_pre &lt;- did_data_nm_tx %&gt;% filter(treated == 0, is_post  == 0, year == 2020) %&gt;%\n  summarise(mean(daily_max_1_hour_no2_concentration)) %&gt;% pull()\n\ntrt_effect &lt;- (y_trt_post- y_trt_pre) - (y_con_post - y_con_pre)\n\nprint(paste(\"The treatment effect is\", round(trt_effect,4)))\n\n[1] \"The treatment effect is 0.6052\"\n\n\nNow, lets use a regression to estimate our treatment effect and see if we get a similar result!\n\\[y = \\beta_0 + \\beta_1 \\cdot \\text{treat} + \\beta_2 \\cdot \\text{year} + \\beta_3 \\cdot (\\text{treat} \\times \\text{year})\\]\n\ntx_nm_did &lt;- lm(daily_max_1_hour_no2_concentration ~ treated * is_post, \n                 data = filter(did_data_nm_tx, year == 2020))\n\nsummary(tx_nm_did)\n\n\nCall:\nlm(formula = daily_max_1_hour_no2_concentration ~ treated * is_post, \n    data = filter(did_data_nm_tx, year == 2020))\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-27.165 -11.336  -1.815   8.484  48.735 \n\nCoefficients:\n                Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)      34.6480     1.5967  21.700  &lt; 2e-16 ***\ntreated         -10.1542     2.2158  -4.583 5.48e-06 ***\nis_post          -4.5830     1.8320  -2.502   0.0126 *  \ntreated:is_post   0.6052     2.5292   0.239   0.8110    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 13.83 on 670 degrees of freedom\nMultiple R-squared:  0.1241,    Adjusted R-squared:  0.1202 \nF-statistic: 31.65 on 3 and 670 DF,  p-value: &lt; 2.2e-16\n\n\n\nWhat does this treatment effect of .6052 mean? Talk with a partner!"
  },
  {
    "objectID": "course-materials/week3.html#lab-materials",
    "href": "course-materials/week3.html#lab-materials",
    "title": "Week 3: Difference-in-Difference",
    "section": " Lab Materials",
    "text": "Lab Materials\n\n\n\n\n\n\n\nLab Materials to Download\nLab Solution\n\n\n\n\nWeek 3 Lab\nWeek 3 Lab Solution"
  },
  {
    "objectID": "course-materials/labs/week3.html#discussion-questions",
    "href": "course-materials/labs/week3.html#discussion-questions",
    "title": "Lab 3: Difference-in-Difference",
    "section": "Discussion Questions:",
    "text": "Discussion Questions:\n\nBefore the lockdown, we see week-to-week fluctuations in NO2 in New Mexico and Texas. Do you think that Texas make a compelling counterfactual which satisfies the main causal identification assumption for DiD?\nLook at your summary(tx_nm_did) output. Which specific coefficient represents the ‚ÄúTreatment Effect‚Äù (the actual impact of the lockdown)? Is this coefficient statistically significant at the 5% level (\\(p &lt; 0.05\\))? Explain what that \\(p\\)-value tells us about the relationship between the lockdown and \\(NO_2\\).\nThe DiD framework relies on a ‚ÄúCounterfactual‚Äù‚Äîthe idea of what would have happened to New Mexico if it hadn‚Äôt locked down. In this lab, what serves as our counterfactual? If a massive dust storm hit both El Paso and Dona Ana on March 25th, would that ruin our DiD estimate? Why or why not? (Hint: Think about whether the storm affects both groups equally).\nGovernment mandates (lockdowns) are the ‚ÄúTreatment,‚Äù but the actual ‚ÄúCause‚Äù of lower \\(NO_2\\) is fewer cars on the road. If people in El Paso (the ‚ÄúControl‚Äù group) got scared and stayed home just as much as people in New Mexico (the ‚ÄúTreated‚Äù group) despite the lack of a legal mandate, what would happen to our DiD estimate? Would it get larger or smaller?"
  },
  {
    "objectID": "course-materials/labs/week3.html#about-the-data",
    "href": "course-materials/labs/week3.html#about-the-data",
    "title": "Lab 3: Difference-in-Difference",
    "section": "About the Data",
    "text": "About the Data\n\nData Source\nAir quality data for this analysis was obtained from the U.S. Environmental Protection Agency‚Äôs (EPA) Outdoor Air Quality Data portal. The dataset includes daily measurements of nitrogen dioxide (NO2) concentrations from monitoring stations in Los Angeles, California and Phoenix, Arizona.\n\nRead in LA and Phoenix EPA Data\n\nLA_2019 &lt;- read_csv(here::here(\"week3\" ,\"data\",  \"LA_2019.csv\")) %&gt;%\n  filter(`Local Site Name` == \"West Los Angeles\") %&gt;% mutate(city = \"LA\", year = 2019)\n\n\nLA_2020 &lt;- read_csv(here::here(\"week3\" ,\"data\", \"LA_2020.csv\")) %&gt;% \n  filter(`Local Site Name` == \"West Los Angeles\") %&gt;% mutate(city = \"LA\", year = 2020)\n\nPhoenix_2019 &lt;- read_csv(here::here(\"week3\" ,\"data\", \"phoenix_2019.csv\")) %&gt;% \n  filter(`Local Site Name` == \"WEST PHOENIX\") %&gt;% \n  mutate(city = \"Phoenix\", year = 2019)\n\nPhoenix_2020 &lt;- read_csv(here::here(\"week3\" ,\"data\", \"phoenix_2020.csv\")) %&gt;% \n  filter(`Local Site Name` == \"WEST PHOENIX\") %&gt;% \nmutate(city = \"Phoenix\", year = 2020)\n\n# Combine all into one master data frame\ndid_data_az_ca &lt;- bind_rows(LA_2019, LA_2020, Phoenix_2019, Phoenix_2020) %&gt;% clean_names()\n\n\n\n\nParallel Trends\nA DiD is only valid if the Treatment and Control groups were moving in the same direction before the intervention hit. Let‚Äôs see if we pass the parallel trends assumption.\n\nweekly_la_phx &lt;- did_data_az_ca %&gt;%\n  mutate(\n    date_obj = mdy(date),\n    week = floor_date(date_obj, unit = \"week\")\n  ) %&gt;%\n  filter(year == 2020, month(date_obj) %in% c(2, 3, 4)) %&gt;%\n  group_by(week, city) %&gt;%\n  summarize(\n    mean_no2 = mean(daily_max_1_hour_no2_concentration, na.rm = TRUE), \n    .groups = \"drop\"\n  )\n\n# Create line plot of phx and LA\nggplot(weekly_la_phx, aes(x = week, y = mean_no2, color = city)) +\n  geom_line(size = 1.2) +\n  geom_point(size = 2.5) +\n  \n  # Add CA lockdown start date dashed line\n  geom_vline(xintercept = as.Date(\"2020-03-19\"), linetype = \"dashed\", color = \"gray30\", size = 1) +\n\n  # Add CA lockdown start date label\n  annotate(\"label\", x = as.Date(\"2020-03-20\"), y = 30, \n           label = \"CA Lockdown \\n Starts\", hjust = 0, size = 3) +\n           \n  labs(\n    title = \"Weekly NO2 Trends in 2020: Los Angeles vs. Phoenix\",\n    x = \"Date\",\n    y = \"Weekly Avg NO2 (ppb)\",\n    color = \"City\"\n  ) +\n  theme_minimal() +\n  scale_color_brewer(palette = \"Set1\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTip\n\n\n\nLook at the lines before the dashed vertical line. Are they moving together? If Phoenix is rising while LA is falling (or vice versa) before the lockdown, can we truly blame the lockdown for the difference we see afterward?"
  },
  {
    "objectID": "course-materials/labs/week3_notes.html",
    "href": "course-materials/labs/week3_notes.html",
    "title": "Lab 3: Difference-in-Difference",
    "section": "",
    "text": "Use the Difference-in-Differences (DiD) framework to estimate the impact of COVID-19 lockdowns on air pollution (\\(NO_2\\)).\n\n\n\n\n\n\n\n\nFigure¬†1: Words on the Orpheum theater marquee try to bring levity to the current situation along the normally bustling commercial area on Broadway in downtown Los Angeles. Source: LA Times\n\n\n\n\n\nIn March 2020, counties across the United States responded to the start of what would be the COVID 19 Pandemic. While every state faced the same biological threat, governors responded with vastly different mandates. Some states, like California and New Mexico, implemented strict ‚ÄúStay at Home‚Äù orders early on. Others, like Arizona and Texas, opted for more relaxed restrictions, allowing businesses like golf courses and salons to remain open longer.In this lab, we will use a Difference-in-Difference (DiD) framework to isolate the impact of these varying lockdown levels. By comparing ‚Äústrict‚Äù counties to their more ‚Äúrelaxed‚Äù neighbors, we can determine if the severity of the lockdown had a measurable impact on \\(NO_2\\) levels‚Äî a key indicator of vehicle traffic and urban activity."
  },
  {
    "objectID": "course-materials/labs/week3_notes.html#objective",
    "href": "course-materials/labs/week3_notes.html#objective",
    "title": "Lab 3: Difference-in-Difference",
    "section": "",
    "text": "Use the Difference-in-Differences (DiD) framework to estimate the impact of COVID-19 lockdowns on air pollution (\\(NO_2\\)).\n\n\n\n\n\n\n\n\nFigure¬†1: Words on the Orpheum theater marquee try to bring levity to the current situation along the normally bustling commercial area on Broadway in downtown Los Angeles. Source: LA Times\n\n\n\n\n\nIn March 2020, counties across the United States responded to the start of what would be the COVID 19 Pandemic. While every state faced the same biological threat, governors responded with vastly different mandates. Some states, like California and New Mexico, implemented strict ‚ÄúStay at Home‚Äù orders early on. Others, like Arizona and Texas, opted for more relaxed restrictions, allowing businesses like golf courses and salons to remain open longer.In this lab, we will use a Difference-in-Difference (DiD) framework to isolate the impact of these varying lockdown levels. By comparing ‚Äústrict‚Äù counties to their more ‚Äúrelaxed‚Äù neighbors, we can determine if the severity of the lockdown had a measurable impact on \\(NO_2\\) levels‚Äî a key indicator of vehicle traffic and urban activity."
  },
  {
    "objectID": "course-materials/labs/week3_notes.html#did-1-los-angeles-ca-vs-phoenix-az",
    "href": "course-materials/labs/week3_notes.html#did-1-los-angeles-ca-vs-phoenix-az",
    "title": "Lab 3: Difference-in-Difference",
    "section": "DiD # 1: Los Angeles, CA vs Phoenix, AZ",
    "text": "DiD # 1: Los Angeles, CA vs Phoenix, AZ\nIn this scenario, we compare two major Southwestern hubs that faced very different lockdown mandates in March 2019: Los Angeles, CA and Phoenix, AZ.\nOn March 19th, 2020, California Governor Gavin Newsom issued a Statewide shelter in place order. Phoenix, AZ did not start their lockdown until March 31, 2020. The strictness of the lockdown varied from city to city. In Arizona, hair salons and golf courses were allowed to stay open, while Los Angeles had a stricter lockdown order that did not permit such businesses to operate.\nWe will use these varying lockdown levels to see if the severity/ strictness of the lockdown had an impact on NO2 levels.\n\nOutcome (\\(Y\\)): Nitrogen Dioxide (\\(NO_2\\)), which is primarily produced by vehicle traffic\nTreatment Group: Los Angeles (Early, strict lockdown)\nControl Group: Phoenix (Later, shorter lockdown)\n\n\nThe Question: Can we use Phoenix as a ‚Äúmirror‚Äù for what would have happened in LA without a lockdown?\n\n\nlibrary(tidyverse)\nlibrary(janitor)\nlibrary(lubridate)\n\n\nRead in LA and Phoenix EPA Data\n\nLA_2019 &lt;- read_csv(here::here(\"week3\" ,\"data\",  \"LA_2019.csv\")) %&gt;%\n  filter(`Local Site Name` == \"West Los Angeles\") %&gt;% mutate(city = \"LA\", year = 2019)\n\n\nLA_2020 &lt;- read_csv(here::here(\"week3\" ,\"data\", \"LA_2020.csv\")) %&gt;% \n  filter(`Local Site Name` == \"West Los Angeles\") %&gt;% mutate(city = \"LA\", year = 2020)\n\nPhoenix_2019 &lt;- read_csv(here::here(\"week3\" ,\"data\", \"phoenix_2019.csv\")) %&gt;% \n  filter(`Local Site Name` == \"WEST PHOENIX\") %&gt;% \n  mutate(city = \"Phoenix\", year = 2019)\n\nPhoenix_2020 &lt;- read_csv(here::here(\"week3\" ,\"data\", \"phoenix_2020.csv\")) %&gt;% \n  filter(`Local Site Name` == \"WEST PHOENIX\") %&gt;% \nmutate(city = \"Phoenix\", year = 2020)\n\n# Combine all into one master data frame\ndid_data_az_ca &lt;- bind_rows(LA_2019, LA_2020, Phoenix_2019, Phoenix_2020) %&gt;% clean_names()\n\n\n\nParallel Trends\nA DiD is only valid if the Treatment and Control groups were moving in the same direction before the intervention hit. Let‚Äôs see if we pass the parallel trends assumption.\n\nweekly_la_phx &lt;- did_data_az_ca %&gt;%\n  mutate(\n    date_obj = mdy(date), # convert date to date object\n    # Rounds each date down to the start of its week\n    week = floor_date(date_obj, unit = \"week\") \n    #(e.g., all dates in a week become the Monday of that week)\n  ) %&gt;%\n  filter(year == 2020, month(date_obj) %in% c(2, 3, 4)) %&gt;% # filter for feb march april obs in 2020\n  group_by(week, city) %&gt;%\n  summarize(\n    mean_no2 = mean(daily_max_1_hour_no2_concentration, na.rm = TRUE), \n    .groups = \"drop\"\n  )\n\n# Create line plot of phx and LA\nggplot(weekly_la_phx, aes(x = week, y = mean_no2, color = city)) +\n  geom_line(size = 1.2) +\n  geom_point(size = 2.5) +\n  \n  # Add CA lockdown start date dashed line\n  geom_vline(xintercept = as.Date(\"2020-03-19\"),\n             linetype = \"dashed\", color = \"gray30\", size = 1) +\n\n  # Add CA lockdown start date label\n  annotate(\"label\", x = as.Date(\"2020-03-20\"), y = 35, \n           label = \"CA Lockdown \\n Starts\", hjust = 0, size = 3) +\n           \n  labs(\n    title = \"Weekly NO2 Trends in 2020: Los Angeles vs. Phoenix\",\n    x = \"Date\",\n    y = \"Weekly Avg Max NO2 (ppb)\",\n    color = \"City\"\n  ) +\n  theme_minimal() +\n  scale_color_manual(values = c(\"dodgerBlue\", \"brown\"))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTip\n\n\n\nLook at the lines before the dashed vertical line. Are they moving together? If Phoenix is rising while LA is falling (or vice versa) before the lockdown, can we truly blame the lockdown for the difference we see afterward?\n\n\nNotes: PARALLEL TRENDS VIOLATED!"
  },
  {
    "objectID": "course-materials/labs/week3_notes.html#did-2-dona-ana-county-nm-and-el-paso-county-tx",
    "href": "course-materials/labs/week3_notes.html#did-2-dona-ana-county-nm-and-el-paso-county-tx",
    "title": "Lab 3: Difference-in-Difference",
    "section": "DiD # 2: Dona Ana County, NM and El Paso County, TX",
    "text": "DiD # 2: Dona Ana County, NM and El Paso County, TX\nTo improve our study, we will look at two counties that share a border: Dona Ana County, NM (Treated) and El Paso County, TX (Control). Because they share the same desert air and weather patterns, El Paso is a much ‚Äúcleaner‚Äù control group for Dona Ana.\n\nOutcome (\\(Y\\)): Nitrogen Dioxide (\\(NO_2\\)), which is primarily produced by vehicle traffic\nTreatment Group: Dona Ana County, NM (Early, strict lockdown)\nControl Group: El Paso County, TX (Later, shorter lockdown)\n\nWe will start with reading in our New Mexico/ Texas data, and checking the parallel trends assumption for these two counties.\n\n\nNM_2019 &lt;- read_csv(here::here(\"week3\", \"data\", \"nm_2019.csv\")) %&gt;% mutate(state = \"NM\", year = 2019, \n      `County FIPS Code`  = as.numeric(`County FIPS Code`))                                                                                   \n\nNM_2020 &lt;- read_csv(here::here(\"week3\", \"data\", \"nm_2020.csv\")) %&gt;%  mutate(state = \"NM\", year = 2020,                       `County FIPS Code`  = as.numeric(`County FIPS Code`))\n\nTX_2019 &lt;- read_csv(here::here(\"week3\", \"data\", \"tx_2019.csv\")) %&gt;% \n  mutate(state = \"TX\", year = 2019)\n\nTX_2020 &lt;- read_csv(here::here(\"week3\", \"data\", \"tx_2020.csv\")) %&gt;% \n  mutate(state = \"TX\", year = 2020)\n\n\n# Combine all into one master data frame\ndid_data_nm_tx &lt;- bind_rows(NM_2019, NM_2020, TX_2019, TX_2020) %&gt;% clean_names()\n\n\nDiD # 2: Parallel Trends Assumption\n\ndid_weekly &lt;- did_data_nm_tx %&gt;%\n  mutate(\n    date_obj = mdy(date),# made into date object\n    week = floor_date(date_obj, unit = \"week\") # set week to be floor of each week\n  ) %&gt;%\n\n  filter(year == 2020, month(date_obj) %in% c(2, 3, 4)) %&gt;%\n  group_by(week, state) %&gt;%\n  summarize(\n    mean_no2 = mean(daily_max_1_hour_no2_concentration, na.rm = TRUE), \n    .groups = \"drop\"\n  )\n\nggplot(did_weekly, aes(x = week, y = mean_no2, color = state)) +\n\n  geom_line(size = 1.2) +\n\n  geom_point(size = 3) +\n  \n  # Vertical line for the NM Lockdown (March 23, 2020)\n  geom_vline(xintercept = as.Date(\"2020-03-23\"),\n             linetype = \"dashed\", color = \"gray30\", size = 1) +\n  annotate(\"label\", x = as.Date(\"2020-03-24\"), y = 35,\n           label = \"NM Lockdown Starts\", hjust = 0, size = 2) +\n  labs(\n    title = \"Weekly 2020 NO2 Trends: Dona Ana County (NM) Vs El Paso County (TX)\",\n    x = \"Date\",\n    y = \"Weekly Avg NO2 (ppb)\",\n    color = \"State\"\n  ) +\n  theme_minimal() +\n  theme(plot.title = element_text(size = 7))+\n  scale_color_brewer(palette = \"Set1\")\n\n\n\n\n\n\n\n\n\nNotice how much more ‚Äúin sync‚Äù these two lines are compared to LA and Phoenix. Why is a border comparison (same weather, same air) usually considered ‚Äúcleaner‚Äù than comparing two distant cities?\n\nAnswer: They help satisfy the ‚Äúall else equal‚Äù assumption\nWhen comparing LA vs.¬†Phoenix, you have many differences that could explain NO2 changes besides the lockdown: - different geography/ weather - different baseline pollution - different economic strucutre\nWhy border comparisons help: - breathe same air - same weather system -\n\n\nGetting ready for DiD\nIn order to include treatment and time as part of our DiD, we need to create dummy variables for them. We will create a variable treated that will be = 1 for New Mexico (the ‚Äútreated‚Äù group in our scenario will be a stricter lockdown), and will be = 0 for Texas (the ‚Äúcontrol‚Äù group in our scenario will be the more relaxed lockdown.)\nOur is_post variable will denote weather or not the observation is after the intervention (in this case, the start of the lockdown in New Mexico.). is_post will = 0 for observations before March 23,2020, and will = 1 for observations after March 23, 2020.\n\ndid_data_nm_tx &lt;- did_data_nm_tx %&gt;%\n  mutate(\n    date_obj = mdy(date), # make date a date object\n    #treatment = NM, control = TX\n    treated = ifelse(state == \"New Mexico\", 1, 0),\n\n    # Post = after Mar 23\n    is_post = ifelse(date_obj &gt;= as.Date(\"2020-03-23\"), 1, 0)\n  )\n\n\n\nEstimating the treatment effect\nThe simplest form of estimating the treatment effect can be denoted by :\n\\((Y_{\\text{treat, post}} - Y_{\\text{treat, pre}}) - (Y_{\\text{control, post}} - Y_{\\text{control, pre}})\\)\nAnother way to estimate DiD (and the way we will often use in this course), is to use regression.\n\\[y = \\beta_0 + \\beta_1 \\cdot \\text{treat} + \\beta_2 \\cdot \\text{year} + \\beta_3 \\cdot (\\text{treat} \\times \\text{year})\\]\nLet‚Äôs start by manually calculating the treatment effect using the first formula.\n\ny_trt_post &lt;- did_data_nm_tx %&gt;% filter(treated == 1, is_post  == 1, year == 2020) %&gt;%\n  summarise(mean(daily_max_1_hour_no2_concentration)) %&gt;% pull()\n\ny_trt_pre &lt;- did_data_nm_tx %&gt;% filter(treated == 1, is_post  == 0, year == 2020) %&gt;%\n  summarise(mean(daily_max_1_hour_no2_concentration)) %&gt;% pull()\n\ny_con_post &lt;- did_data_nm_tx %&gt;% filter(treated == 0, is_post  == 1, year == 2020) %&gt;%\n  summarise(mean(daily_max_1_hour_no2_concentration)) %&gt;% pull()\n\ny_con_pre &lt;- did_data_nm_tx %&gt;% filter(treated == 0, is_post  == 0, year == 2020) %&gt;%\n  summarise(mean(daily_max_1_hour_no2_concentration)) %&gt;% pull()\n\ntrt_effect &lt;- (y_trt_post- y_trt_pre) - (y_con_post - y_con_pre)\n\nprint(paste(\"The treatment effect is\", round(trt_effect,4)))\n\n[1] \"The treatment effect is 0.6052\"\n\n\nNow, lets use a regression to estimate our treatment effect and see if we get a similar result!\n\\[y = \\beta_0 + \\beta_1 \\cdot \\text{treat} + \\beta_2 \\cdot \\text{year} + \\beta_3 \\cdot (\\text{treat} \\times \\text{year})\\]\n\ntx_nm_did &lt;- lm(daily_max_1_hour_no2_concentration ~ treated * is_post, \n                 data = filter(did_data_nm_tx, year == 2020))\n\nsummary(tx_nm_did)\n\n\nCall:\nlm(formula = daily_max_1_hour_no2_concentration ~ treated * is_post, \n    data = filter(did_data_nm_tx, year == 2020))\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-27.165 -11.336  -1.815   8.484  48.735 \n\nCoefficients:\n                Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)      34.6480     1.5967  21.700  &lt; 2e-16 ***\ntreated         -10.1542     2.2158  -4.583 5.48e-06 ***\nis_post          -4.5830     1.8320  -2.502   0.0126 *  \ntreated:is_post   0.6052     2.5292   0.239   0.8110    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 13.83 on 670 degrees of freedom\nMultiple R-squared:  0.1241,    Adjusted R-squared:  0.1202 \nF-statistic: 31.65 on 3 and 670 DF,  p-value: &lt; 2.2e-16\n\n\nNote : (outcome ~ treated * is post) is same as (outcome ~ treated + is_post + treated:is_post). The * is just shorthand in R that automatically includes both main effects AND the interaction term.\n\ntreated - Controls for baseline differences Without this, you assume NM and TX started at the same NO2 levels, which your graph shows is false (TX was ~10 ppb higher). What it does: Captures the pre-existing gap between treatment and control groups.\nis_post - Controls for time trends Without this, you assume NO2 stayed constant over time, which is also false (both states showed changes). What it does: Captures changes over time that affect both groups equally (seasonal patterns, national COVID behavior changes, etc.).\ntreated:is_post - The DiD estimator This is your causal effect. What it does: Measures the additional change in NM beyond what happened in TX over the same time period.\n\n\n\n\n\n\n\n\nCoefficient\nWhat It Measures\n\n\n\n\n(Intercept)\nAverage NO2 in Texas before lockdown\n\n\ntreated\nDifference between NM and TX before lockdown (baseline gap)\n\n\nis_post\nChange in Texas from before to after (control group trend)\n\n\ntreated:is_post\nDiD estimator - the causal effect of NM‚Äôs lockdown on NO2\n\n\n\nInterpretation:\n12.4% of the variation in daily NO2 concentrations is explained by treatment status, time period, and their interaction 87.6% of variation remains unexplained - likely due to weather, day-of-week effects, other pollution sources, measurement error, etc.\n\n\n\n\n\n\nTipComprehension Check\n\n\n\nWhat does this treatment effect of .6052 mean? Talk with a partner!\n\n\nAnswer: New Mexico‚Äôs lockdown was associated with NO2 levels that were 0.6052 ppb HIGHER than expected compared to the control group (Texas) trend.\nIn other words:\nIf Texas‚Äôs NO2 dropped by 10 ppb during this period New Mexico‚Äôs NO2 only dropped by ~9.4 ppb (10 - 0.6052) The lockdown had a smaller effect on NO2 than the baseline trend would predict\nWhy the positive coefficient? : - texas still had local restrictions - Pre-Treatment Trends - If NM was already on a downward trajectory before the lockdown\nThis could mean:\n\nLocal lockdowns in TX were just as effective\nVoluntary behavior changes were similar across the border\nThe policy difference wasn‚Äôt as stark in practice Other factors (weather, economic changes) drove most of the reduction\n\nFuture considerations: Dont aggregate data, different time period\n\n\n\n\n\n\nWarning\n\n\n\nDoes our 2020 data tell the whole story? What if the drop we were experiencing in NO2 were due to seasonal trends, and were bound to happen with or without a lockdown?"
  },
  {
    "objectID": "course-materials/labs/week3_notes.html#discussion-questions",
    "href": "course-materials/labs/week3_notes.html#discussion-questions",
    "title": "Lab 3: Difference-in-Difference",
    "section": "Discussion Questions:",
    "text": "Discussion Questions:\n\nBefore the lockdown, we see week-to-week fluctuations in NO2 in New Mexico and Texas. Do you think that Texas make a compelling counterfactual which satisfies the main causal identification assumption for DiD?\n\nParallel pre-treatment trends are the foundation of the DiD assumption.The DiD method assumes that without the lockdown, both groups would have continued on the same trend. If the lines aren‚Äôt parallel before treatment: You can‚Äôt isolate the treatment effect - Any post-treatment difference could just be a continuation of pre-existing divergent trends, not the lockdown\n\nLook at your summary(tx_nm_did) output. Which specific coefficient represents the ‚ÄúTreatment Effect‚Äù (the actual impact of the lockdown)? Is this coefficient statistically significant at the 5% level (\\(p &lt; 0.05\\))? Explain what that \\(p\\)-value tells us about the relationship between the lockdown and \\(NO_2\\).\n\nWhat this p-value tells us:\nThere‚Äôs an 81% probability we‚Äôd observe this result (or more extreme) purely by chance if the lockdown had zero effect We cannot reject the null hypothesis that the lockdown had no effect on NO2. In plain English: The data provides no evidence that New Mexico‚Äôs lockdown reduced NO2 concentrations more than the trend observed in Texas\n\nThe DiD framework relies on a ‚ÄúCounterfactual‚Äù‚Äîthe idea of what would have happened to New Mexico if it hadn‚Äôt locked down. In this lab, what serves as our counterfactual?\n\nTexas (specifically the border region) is our counterfactual. It represents what would have happened to New Mexico‚Äôs NO2 levels if NM had NOT implemented the lockdown. We observe Texas‚Äôs actual trend and assume NM would have followed a similar path without treatment. Would a dust storm on March 25th ruin our DiD estimate? No, it would NOT ruin the estimate! Here‚Äôs why:\nA dust storm affects BOTH groups equally (they share the same airshed)\nIf a massive dust storm hit both El Paso and Dona Ana on March 25th, would that ruin our DiD estimate? Why or why not? (Hint: Think about whether the storm affects both groups equally).\n\nGovernment mandates (lockdowns) are the ‚ÄúTreatment,‚Äù but the actual ‚ÄúCause‚Äù of lower \\(NO_2\\) is fewer cars on the road. If people in El Paso (the ‚ÄúControl‚Äù group) got scared and stayed home just as much as people in New Mexico (the ‚ÄúTreated‚Äù group) despite the lack of a legal mandate, what would happen to our DiD estimate? Would it get larger or smaller?\n\nIf El Paso residents stayed home voluntarily just as much as NM residents despite no legal mandate: Our DiD estimate would get SMALLER (closer to zero). Here‚Äôs the logic: What we‚Äôre measuring:\nTreatment effect = Impact of the POLICY (the mandate), not just behavior change If El Paso has the same behavior change without the mandate ‚Üí The mandate itself had little additional effect"
  },
  {
    "objectID": "course-materials/week4.html#lab-materials",
    "href": "course-materials/week4.html#lab-materials",
    "title": "Week 4: Fixed Effects & Matching Methods",
    "section": " Lab Materials",
    "text": "Lab Materials\n\n\n\n\n\n\n\nLab Materials to Download\nLab Solution\n\n\n\n\nWeek 4 Lab\nWeek 4 Lab Solution"
  },
  {
    "objectID": "course-materials/week5.html#lab-materials",
    "href": "course-materials/week5.html#lab-materials",
    "title": "Week 5:",
    "section": " Lab Materials",
    "text": "Lab Materials\n\n\n\n\n\n\n\nLab Materials to Download\nLab Solution\n\n\n\n\nLab 5 Template qmd  Lab 5 Data\nWeek 5 Lab Solution"
  },
  {
    "objectID": "course-materials/week7.html#lab-materials",
    "href": "course-materials/week7.html#lab-materials",
    "title": "Week 7:",
    "section": " Lab Materials",
    "text": "Lab Materials\n\n\n\n\n\n\n\nLab Materials to Download\nLab Solution"
  },
  {
    "objectID": "course-materials/week6.html#lab-materials",
    "href": "course-materials/week6.html#lab-materials",
    "title": "Week 6: Instrumental Variable Design",
    "section": " Lab Materials",
    "text": "Lab Materials\n\n\n\n\n\n\n\nLab Materials to Download\nLab Solution"
  },
  {
    "objectID": "course-materials/assignments/HW2_START.html",
    "href": "course-materials/assignments/HW2_START.html",
    "title": "üå¨Ô∏èüó≥ Assignment 2: Wind Turbines, Matching, and Difference-in-Differences",
    "section": "",
    "text": "Assignment instructions\nWorking with classmates to troubleshoot code and concepts is encouraged. If you collaborate, list collaborators at the top of your submission.\nAll written responses must be written independently (in your own words).\nKeep your work readable: Use clear headings and label plot elements thoughtfully.\nAssignment submission (YOUR NAME): ______________________________________\n\n\n\nIntroduction\nIn this assignment you will be doing political weather forecasting except the ‚Äústorms‚Äù we care about are electoral swings that might follow local wind turbine development.\nIn Stokes (2015), the idea is that a policy with diffuse benefits (cleaner electricity) can create concentrated local costs (turbines nearby), and those local opponents may ‚Äúsend a signal‚Äù at the ballot box (i.e., NIMBYISM). Your job is to use two statistical tools:\n\nMatching: Can we create a more apples-to-apples comparison between precincts that did vs.¬†did not end up near turbine proposals?\nFixed effects + Difference-in-Differences: Can we use repeated elections to estimate how within-precinct changes in turbine exposure relate to changes in incumbent vote share?\n\n\n\n\nLearning goal: Replicate the matching and fixed effects analyses from study:\n\nStokes (2015): ‚ÄúElectoral Backlash against Climate Policy: A Natural Experiment on Retrospective Voting and Local Resistance to Public Policy.\n\n\nStudy: Stokes (2015) - Article\nData source: Dataverse-Stokes2015\n\n\n\n\n\n\n\nNOTE: Replication of study estimates will be approximate. An alternative matching procedure and fixed effects estimation package are utilized in this assignment for illustration purposes.\n\n\n\n\n\n\nSetup: Load libraries\n\nLoad libraries (+ install if needed)\n\n\nlibrary(tidyverse)\nlibrary(here)\nlibrary(janitor)\nlibrary(jtools)\n\nlibrary(gtsummary)\nlibrary(gt)\n\nlibrary(MatchIt) # matching\nlibrary(cobalt)  # balance + love plots\n\nlibrary(fixest) # fast fixed effects\nlibrary(scales) # plotting\n\n\n\n\nPart 1: Study Background\n\n1A. Dive into the details of the study design and evaluation plan\n\nGoal: Get familiar with the study setting, environmental issue, and policy under evaluation.\n\n\n\n\n\n\n\nNOTE: Read over study to inform your response to the assignment questions. For this assignment we will skip-over sections that describe the Instrumental Variables identification strategy. We will cover instrumental variable designs weeks 6-7.\n\n\n\n1A.Q1 Summarize the environmental policy issue, the outcome of interest, and the intervention being evaluated. Be sure to include a brief description of each of the following key elements of the study: unit of analysis, outcome, treatment, comparison group):\nResponse: _________________________\n1A.Q2 Why might turbine proposals be correlated with baseline political preferences or rural areas? Provide 2 plausible mechanisms, and explain why that creates confounding.\nResponse: _________________________\n\n\n\n1B. Break down the causal inference strategy and identify threats to identification:\n1B.Q1 What is the key identifying assumption for a fixed effects / Difference-in-Difference design? Explain how this assumption when satisfied provides evidence of causal effect:\nResponse: _________________________\n1B.Q2 What is the reason for using a fixed effects approach from a causal inference perspective? Summarize within the context of study (in your own words).\nResponse: _________________________\n1B.Q3 What part of the SUTVA assumption is most likely violated in the context of this study design (and why)?\nResponse: _________________________\n1B.Q4 Why does spillover matter when estimating an unbiased treatment effect?\nResponse: _________________________\n1B.Q5 How do the authors assess the risk of spillovers, and what analytic choice do they make to attempt to mitigate the risk that spillover biases the causal estimate?\nResponse: _________________________\n\n\n\n\nPart 2: Matching\n\nWe will start by evaluating the 2007 survey (cross-sectional) data. Treatment is defined by whether a precinct is near a turbine proposal (within 3 km).\n\nGoal: Match precincts using pre-treatment covariates and then estimate the effect of proposed wind turbines on incumbent vote share.\n\n\n2A. Load data for matching\n\nRead in data file stokes15_survey2007.csv\nCode precinct_id and district_id as factors\nTake a look at the data\n\n\nmatch_data &lt;- \n\n2A.Q1 Intuition check: Why match? Explain rationale for using this method.\nResponse: _________________________\n\n\n\n2B. Check imbalance (before matching)\n\nCreate a covariate balance table comparing treated and control precincts\nTreatment indicator: proposed_turbine_3km\nInclude pre-treatment covariates: log_home_val_07, p_uni_degree, log_median_inc, log_pop_denc\nUse the tbl_summary() function from the {gtsummary} package.\n\n\nmatch_data %&gt;%\n\n2B.Q1 Summarize the table output: Which covariates look balanced/imbalanced?\nResponse: _________________________\n2B.Q2 Describe in your own words why these covariates might be expected to confound the treatment estimate:\nResponse (2-4 sentences): _________________________\n\n2B.Q3 Intuition check: What type of data do you need to conduct a matching analysis?\nResponse: _________________________\n\n\n\nConduct matching estimation using the {MatchIt} package:\nüìú Documentation - MatchIt\nLearning goals:\n\nApproximate the Mahalanobis matching method used in Stokes (2015)\nImplement another common matching approach called propensity score matching\n\n\n\n\n\n\n\nNOTE: In the replication code associated with Stokes (2015) the {AER} package is used for Mahalanobis matching. In this assignment we use the {MatchIt} package. The results are comparable but will not be exactly the same.\n\n\n\n\n\n\n2C. Mahalanobis nearest-neighbor matching\n\nConduct Mahalanobis matching\n\nUse nearest-neighbor match without replacement using Mahalanobis distance\nUse 1-to-1 matching (match one control unit to each treatment unit)\nExtract the matched data using match.data()\n\n\nset.seed(2412026)\n\nmatch_model &lt;- matchit(\n     # Treatment_indicator ~  Pre_treatment_covariates\n  data = match_data, \n  method = \"nearest\",       # Nearest neighbor matching\n  distance = \"mahalanobis\", # Mahalanobis distance\n  ratio = 1,                # Match one control unit to one treatment unit (1:1 matching)\n  replace = FALSE           # Control observations are not replaced\n)\n\n# Extract matched data\nmatched_data &lt;- match.data(match_model)\n\n\nsummary(match_model)\n\n2C.Q1 Using the summary() output: Which covariate had the largest and smallest Std. Mean Diff. before matching. Next, compare largest/smallest Std. Mean Diff. after matching.\nResponse: _________________________\n\n\n\n2D. Create a ‚Äúlove plot‚Äù using love.plot() ‚ù§Ô∏è\nüìú Documentation - cobalt\n\nPlot mean differences for data before & after matching across all pre-treatment covariates\nThis is an effective way to evaluate how effective matching was at achieving balance.\n\n\n\nMake a love plot of standardized mean differences (SMDs) before vs after matching.\nInclude a threshold line at 0.1.\nIn love plot display mean.diffs\n\n\nnew_names &lt;- data.frame(\n    old = c(\"log_home_val_07\", \"p_uni_degree\", \"log_median_inc\", \"log_pop_denc\"),\n    new = c(\"Home Value (log)\", \"Percent University Degree\",\n            \"Median Income (log)\", \"Population Density (log)\"))\n\n# Love plot\nlove.plot(match_model, stats = \"mean.diffs\",\n          thresholds = c(m = 0.1),\n          var.names = new_names)\n\n2D.Q1 Interpret the love plot in your own words:\nResponse: _________________________\n\n\n\nPropensity score matching\n\n\n\n2E. Propensity Score Matching (PSM)\n\nEstimate 1:1 nearest-neighbor Propensity Score Matching\nSame code as above except change distance = \"logit\"\n\n\nset.seed(2412026)\n\npropensity_scores &lt;- \n\n\n\n\nCreate table displaying covariate balance using cobalt::bal.tab()\nüìú Documentation - cobalt\nUse bal.tab() to report balance before and after matching.\n\nbal.tab(propensity_scores, \n        var.names = new_names) \n\n2E.Q1 Compare Mahalanobis vs propensity score matching. Which method did a better job at achieving balance?\nResponse: _________________________\n\n\n\n2F. Estimate an effect in the matched sample\nUsing the matched data (Mahalanobis method), estimate the effect of treatment on the change in incumbent vote share (change_liberal).\n\nreg_match &lt;- \n\n#summ(, model.fit = FALSE)\n\n2F.Q1 Have you identified a causal estimate using this approach: Why or why not?\nResponse: _________________________\n2F.Q2 When using a matching method, what is the main threat to causal identification?\nResponse: _________________________\n2F.Q3 Describe why the treatment estimate represents the Average Treatment for the Treated (ATT) and explain why this is the case relative to estimation of the Average Treatment Effect (ATE).\nResponse: _________________________\n\n\n\n\nPart 3: Panel Data, Fixed Effects, and Difference-in-Difference\nData source: Dataverse-Stokes2015\n\n\n3A: Read in the panel data + code variables precinct_id and year as factors\n\npanel_data &lt;- \n\n# HINT: Try running `tabyl(panel_data$year)`. Review article to make sense of the row numbers (n).\n\n3A.Q1: Why are there 18,558 rows in panel_data?\nResponse: _________________________\n\n# How many years are included in the panel?\n\n# How many precincts are there?\n\n3A.Q2: How many unique precincts are ever treated (i.e., proposed & operational)?\nResponse: _________________________\n\npanel_data %&gt;%\n  group_by(precinct_id) %&gt;%\n  summarise(\n    ever_proposed    = any(proposed_turbine == 1, na.rm = TRUE),\n    ever_operational = any(operational_turbine == 1, na.rm = TRUE),\n    .groups = \"drop\") %&gt;%\n  summarise(\n    n_ever_proposed    = sum(ever_proposed),\n    n_ever_operational = sum(ever_operational))\n\n\n\n\n3B. Plot and evaluate parallel trends: Replicate Figure.2 (Stokes, 2015)\n\nCreate indicators for whether each precinct is ever treated by 2011 (treat_p, treat_o; separate indicator for proposals and operational turbines).\nPlot mean incumbent vote share by year for treated vs control precincts (with 95% CIs).\nFacet by turbine type (proposed & operational)\n\nStep 1: Prepare data\n\ntrends_data &lt;- panel_data %&gt;%\n  group_by(precinct_id) %&gt;%\n  mutate(\n    treat_p = as.integer(any(proposed_turbine == 1, na.rm = TRUE)),  # ever proposed (in any year)\n    treat_o = as.integer(any(operational_turbine == 1, na.rm = TRUE))) %&gt;% # ever operational (in any year)\n  ungroup() %&gt;% \n  pivot_longer(c(treat_p, treat_o),\n               names_to = \"turbine_type\", values_to = \"treat\") %&gt;% \n  mutate(\n      turbine_type = factor(turbine_type,\n                            levels = c(\"treat_p\", \"treat_o\"),\n                            labels = c(\"Proposed turbines\", \"Operational turbines\")),  \n    status = if_else(treat == 1, \"Treated\", \"Control\"),\n    year   = factor(year))\n\nStep 2: Create trends plot\n\npd &lt;- position_dodge(width = 0.15)\n\ntrends_data %&gt;%\n  group_by(turbine_type, status, year) %&gt;%\n  summarise(\n    mean = mean(perc_lib, na.rm = TRUE),\n    n    = sum(!is.na(perc_lib)),\n    se   = sd(perc_lib, na.rm = TRUE) / sqrt(n), \n    ci   = qt(.975, df = pmax(n - 1, 1)) * se,\n    .groups = \"drop\") %&gt;%\nggplot(aes(year, mean, color = status, group = status)) +\n  geom_line(position = pd, linewidth = 1.2) +\n  geom_point(position = pd, size = 2.6) +\n  geom_errorbar(\n    aes(ymin = mean - ci, ymax = mean + ci),\n    position = pd, width = .12, linewidth = .7, color = \"black\") +\n  facet_wrap(~ turbine_type, nrow = 1) +\n  scale_color_manual(values = c(Control = \"#0072B2\", Treated = \"#B22222\")) +\n  scale_y_continuous(labels = percent_format(accuracy = 1)) +\n  coord_cartesian(ylim = c(.20, .57)) +\n  labs(\n    title = \"Figure 2. Trends in the Governing Party‚Äôs Vote Share\",\n    x = \"Election Year\",\n    y = \"Liberal Party Vote Share\",\n    color = NULL) +\n  theme_minimal(base_size = 14) +\n  theme(\n    panel.grid.minor = element_blank(),\n    legend.position = \"bottom\",\n    strip.text = element_text(face = \"bold\"))\n\n3B.Q1: Write a short paragraph assessing the parallel trends assumption for each outcome.\nResponse (4-6 sentences): _________________________\n\n\n\nEstimating Fixed Effects Models (DiD) for proposals\n\\[\n\\text{Y}_{it}\n=  \\alpha_0 +\n\\beta \\cdot (\\text{proposed_turbine}_{it})\n+ \\gamma_i\n+ \\delta_t\n+ \\varepsilon_{it}\n\\]\n\n\\(Y_{it}\\) is the vote share for the Liberal Party in precinct i in time t\n\\(\\beta\\) is the treatment effect of a turbine being proposed within a precinct\n\\(\\gamma_i\\) is the precinct fixed effect\n\\(\\delta_t\\) is the year fixed effect\n\n\n\n\nExample 1: Randomly sample 40 precincts\n\nTo illustrate the ‚Äúdummy variable method‚Äù of estimating fixed effects using the the general lm() function we are going to randomly sample 40 precincts (20 ‚Äútreated‚Äù precincts with proposed turbines).\nIf we attempted to use this approach with the full sample estimating all 6185 (n-1) precinct-level coefficients is impractical (it would take a long time).\n\n\nset.seed(40002026)\n\nprecinct_frame &lt;- panel_data %&gt;%\n  group_by(precinct_id) %&gt;%\n  summarise(\n    proposed_turbine_any = as.integer(any(proposed_turbine == 1, na.rm = TRUE)),\n    .groups = \"drop\"\n  )\n\nids_40 &lt;- precinct_frame %&gt;%\n  group_by(proposed_turbine_any) %&gt;%\n  slice_sample(n = 20) %&gt;%\n  ungroup() %&gt;%\n  select(precinct_id)\n\nsample_40_precincts &lt;- panel_data %&gt;%\n  semi_join(ids_40, by = \"precinct_id\")\n\n\n\n\n3C: Estimate a fixed effects model using lm() with fixed effects added for precinct and year using the sample of 40 precincts just created.\n\nmodel1_ff &lt;- \n\n# summ(*** , model.fit = FALSE)\n\n\n# summ(*** , model.fit = FALSE, [. . .])\n\n3C.Q1: Intuition check: Is the signal-to-noise ratio for the treatment estimate greater than 2-to-1?\nResponse: _________________________\n\nHINT: Add the argument digits = 3 to the summ() function above\n\n3C.Q2: Re-run the summ() function using the heteroscedasticiy robust standard error adjustment (robust = TRUE). Did the standard error (S.E.) estimates change? Explain why.\nResponse: _________________________\n3C.Q3: Compare results of the model above to the findings from the fixed effects analysis in the Stokes (2015) study. Why might the results be similar or different?\nResponse: _________________________\n3C.Q4: In your own words, explain why it is advantageous from a causal inference perspective to include year and precinct fixed effects. Explain how between-level and within-level variance is relevant to the problem of omitted variable bias (OVB).\nResponse (2-4 sentences): _________________________\n\n\n\n3D. Now using the full sample, estimate the treatment effect of wind turbine proposals on incumbent vote share. Use feols() from the {fixest} package to estimate the fixed effects.\nSee vignette here: fixest walkthrough\n\nmodel2_ff &lt;- \n\nsummary()\n\n3D.Q1: Interpret the model results and translate findings to be clear to an audience that may not have a background in causal inference (Econometrics) methods.\nIn panel data settings, why is clustering by precinct important (i.e., cluster = ~precinct_id) ?‚Äù\nResponse (4-6 sentences): _________________________\n\n\n\n3E. Estimate the treatment effect of operational wind turbines on incumbent vote share. Use the same approach as the previous model.\n\nmodel3_ff &lt;- \n\nsummary()\n\n3E.Q1: Interpret the model3_ff results as clearly and concisely as you can.\nResponse: _________________________\n3E.Q2: Why do you think the effect of proposed wind turbines is different from operational wind turbines. Develop your own theory about why incumbent vote share is affected in this way. Use the Stokes (2015) study to inform your response as needed.\nResponse: _________________________"
  },
  {
    "objectID": "course-materials/labs/week4.html#lab-outline",
    "href": "course-materials/labs/week4.html#lab-outline",
    "title": "Lab Week 4: Statistical Matching + Fixed Effects",
    "section": "Lab Outline",
    "text": "Lab Outline\n\nLoad in the packages and data\nTake a look at focal variables\nChecking covariate imbalance (pre‚Äëmatching).\nConduct a matching analysis (Mahalanobis matching)\nEvaluate balance after matching\nEstimate regression models for each outcome using the matched sample\nPractice fixed effects estimation"
  },
  {
    "objectID": "course-materials/labs/week4.html#applied-study-data-source",
    "href": "course-materials/labs/week4.html#applied-study-data-source",
    "title": "Lab Week 4: Statistical Matching + Fixed Effects",
    "section": "Applied study & data source",
    "text": "Applied study & data source\nThis lab uses open-access replication data from:\n\nAhmadia, G. N., Glew, L., Provost, M., Gill, D., Hidayat, N. I., Mangubhai, S., Purwanto, P., Fox, H. E. (2015). Integrating impact evaluation in the design and implementation of monitoring marine protected areas. Philosophical Transactions of the Royal Society B.\n\nBig idea: MPAs are not randomly placed. They may be placed in ‚Äúbetter‚Äù locations which means that treated and control groups are unlikely to make an apples-to-apples comparison. AKA, we are up against the nearly universal causal inference problem with non-experimental data, treatment assignment selection bias.\nAhmadia et al.¬†(2015) uses statistical matching to construct a more apples-to-apples comparison group. In this lab exercise we will approximately replicate the main matching analysis conducted in this study.\n\n\n\n\n\n\nNote\n\n\n\nNOTE: When matching, the authors imposed additional restrictions (caliper-style constraints) on some habitat variables. We do not fully replicate those constraints here.\n\n\n\n\nReading reference:\n\nMatchIt Vignette: https://kosukeimai.github.io/MatchIt/articles/MatchIt.html"
  },
  {
    "objectID": "course-materials/labs/week4.html#load-packages-study-data",
    "href": "course-materials/labs/week4.html#load-packages-study-data",
    "title": "Lab Week 4: Statistical Matching + Fixed Effects",
    "section": "0. Load packages + study data",
    "text": "0. Load packages + study data\n\nlibrary(tidyverse)   # data wrangling + plotting\nlibrary(here)        # portable file paths (project-root relative)\nlibrary(janitor)     # clean variable names\nlibrary(gtsummary)   # clean summary tables\nlibrary(gt)          # optional: render gtsummary nicely\nlibrary(MatchIt)     # matching estimation\nlibrary(cobalt)      # balance checks + love plots\nlibrary(jtools)      # clean regression output summaries\n\nRead in the cleaned survey data\n\ndata_clean &lt;- read_csv(here(\"week4\", \"Ahmadia_2015_DataClean.csv\"), show_col_types = FALSE)"
  },
  {
    "objectID": "course-materials/labs/week4.html#create-variable-description-table-for-key-variables-in-the-matching-analysis",
    "href": "course-materials/labs/week4.html#create-variable-description-table-for-key-variables-in-the-matching-analysis",
    "title": "Lab Week 4: Statistical Matching + Fixed Effects",
    "section": "1. Create variable description table for key variables in the matching analysis",
    "text": "1. Create variable description table for key variables in the matching analysis\n\n\n\n\n\n\n\n\nVariable Descriptions (Treatment, Outcomes, Matching Covariates) - Ahmadia (2015)\n\n\nLabel\nDescription\n\n\n\n\ntreated_mpa (Treatment)\nSite is inside a Marine Protected Area (1 = inside MPA , 0 = outside MPA).\n\n\nbiomass_fisheries (Outcome)\nBiomass of key fisheries families (Serranidae, Lutjanidae, Haemulidae) measured in kg per hectare (kg/ha).\n\n\nbiomass_ecological (Outcome)\nBiomass of herbivorous fish families (Acanthuridae, Scaridae, Siganidae) measured in kg per hectare (kg/ha).\n\n\ndist_deep_water (Covariate)\nCovariate: Distance to deep water (50 m depth contour), in meters (m).\n\n\nssta_freq (Covariate)\nFrequency of sea-surface temperature anomalies (SSTA)\n\n\nreef_exposure (Covariate)\nReef wave exposure (Exposed, Semi-exposed, Sheltered).\n\n\nreef_slope (Covariate)\nReef slope (Flat, Slope, Wall)\n\n\nreef_type (Covariate)\nReef type (Patch, Fringing, Barrier, Atoll).\n\n\ndist_mangroves (Covariate)\nDistance to nearest mangrove habitat, in meters (m).\n\n\ndist_fishing_settlement (Covariate)\nDistance to nearest fishing settlement, in meters (m).\n\n\ndist_market (Covariate)\nDistance to primary market, in meters (m).\n\n\npollution_risk (Covariate)\nWatershed pollution risk. (1 = Low; 2 = Medium; 3 = High).\n\n\nmonsoon_direction (Covariate)\nMonsoon wind exposure direction. (Northwest (NW), Southeast (SE))."
  },
  {
    "objectID": "course-materials/labs/week4.html#check-imbalance-before-matching-covariate-balance-table",
    "href": "course-materials/labs/week4.html#check-imbalance-before-matching-covariate-balance-table",
    "title": "Lab Week 4: Statistical Matching + Fixed Effects",
    "section": "2. Check imbalance before matching (covariate balance table)",
    "text": "2. Check imbalance before matching (covariate balance table)\nGoal: Compare covariate distributions between treated_mpa and control sites before matching.\n\ndata_clean %&gt;%\n  select(\n    treated_mpa, dist_to_deep_water, ssta_freq, reef_exposure,\n    reef_slope, reef_type,\n    dist_mangroves, dist_fish_settl, dist_market,\n    pollution_risk, monsoon_direction) %&gt;%\n  tbl_summary(\n    by = treated_mpa,\n    statistic = list(\n      all_continuous() ~ \"{mean} ({sd})\",\n      all_categorical() ~ \"{n} ({p}%)\"\n    )\n  ) %&gt;%\n  modify_header(label ~ \"**Covariate**\") %&gt;%\n  modify_spanning_header(c(\"stat_1\", \"stat_2\") ~ \"**Group**\")\n\n\n\n\n\n\n\n\n\n\n\n\nCovariate\n\nGroup\n\n\n\nControl (non‚ÄëMPA)\nN = 531\nMPA site\nN = 1081\n\n\n\n\ndist_to_deep_water\n656 (905)\n627 (913)\n\n\nssta_freq\n25 (8)\n25 (5)\n\n\nreef_exposure\n\n\n\n\n\n\n¬†¬†¬†¬†Exposed\n35 (66%)\n92 (85%)\n\n\n¬†¬†¬†¬†Semi-exposed\n18 (34%)\n16 (15%)\n\n\nreef_slope\n\n\n\n\n\n\n¬†¬†¬†¬†Flat\n7 (13%)\n10 (9.3%)\n\n\n¬†¬†¬†¬†Slope\n45 (85%)\n90 (83%)\n\n\n¬†¬†¬†¬†Wall\n1 (1.9%)\n8 (7.4%)\n\n\nreef_type\n\n\n\n\n\n\n¬†¬†¬†¬†Barrier\n8 (15%)\n10 (9.3%)\n\n\n¬†¬†¬†¬†Fringing\n45 (85%)\n93 (86%)\n\n\n¬†¬†¬†¬†Patch\n0 (0%)\n5 (4.6%)\n\n\ndist_mangroves\n9,679 (14,229)\n4,618 (5,165)\n\n\ndist_fish_settl\n25,646 (28,199)\n32,505 (31,537)\n\n\ndist_market\n124,152 (49,763)\n139,918 (66,406)\n\n\npollution_risk\n\n\n\n\n\n\n¬†¬†¬†¬†High\n0 (0%)\n1 (0.9%)\n\n\n¬†¬†¬†¬†Low\n33 (62%)\n84 (78%)\n\n\n¬†¬†¬†¬†Medium\n20 (38%)\n23 (21%)\n\n\nmonsoon_direction\n\n\n\n\n\n\n¬†¬†¬†¬†NW\n19 (36%)\n62 (57%)\n\n\n¬†¬†¬†¬†SE\n34 (64%)\n46 (43%)\n\n\n\n1 Mean (SD); n (%)\n\n\n\n\n\n\n\n\n\n‚úÖ Your turn (write answers in the lab)\nQ1. Based on the balance table, name two covariates that look most different between MPA and non-MPA sites.\nResponse: _________________________\nQ2. Pick one of those imbalanced covariates and explain why it might confound the estimate of the effect of MPAs on fish biomass.\nResponse: _________________________"
  },
  {
    "objectID": "course-materials/labs/week4.html#matching-plan-what-are-we-trying-to-estimate",
    "href": "course-materials/labs/week4.html#matching-plan-what-are-we-trying-to-estimate",
    "title": "Lab Week 4: Statistical Matching + Fixed Effects",
    "section": "Matching plan (what are we trying to estimate?)",
    "text": "Matching plan (what are we trying to estimate?)\nIn {MatchIt}, most distance-based matching procedures (like nearest-neighbor matching) target is to estimate:\n\nATT: Average Treatment effect on the Treated\n\nWhy? When using matching methods we typically keep treated units and then select control units that most closely match them.\n\n‚úÖ Your turn\nQ4. What does ATT mean in the context of this MPA evaluation setting?\nResponse: _________________________"
  },
  {
    "objectID": "course-materials/labs/week4.html#mahalanobis-matching",
    "href": "course-materials/labs/week4.html#mahalanobis-matching",
    "title": "Lab Week 4: Statistical Matching + Fixed Effects",
    "section": "3. Mahalanobis Matching",
    "text": "3. Mahalanobis Matching\nMatching criteria used in Ahmadia (2015)\n\nmethod = \"nearest-neighbor\": Nearest neighbor matching\ndistance = \"mahalanobis\": Mahalanobis distance\nratio = 2: Two controls matched to each treated unit (2:1 ratio; control/treated)\nreplace = TRUE: Controls can be reused (matched to multiple treated units)\n\n\nNOTE: This is the main matching method implemented in Ahmadia (2015) with the exception of the caliper-style constraints.\n\n\n# 1) Set a seed for reproducible matching\nset.seed(2412026)\n\n# 2) Fit a nearest-neighbor Mahalanobis matching model\nmatch_model &lt;- matchit(\n  treated_mpa ~ dist_to_deep_water + ssta_freq + reef_exposure +\n    reef_slope + reef_type + dist_mangroves + dist_fish_settl +\n    dist_market + pollution_risk + monsoon_direction,\n  data = data_clean,\n  method = \"nearest\",         # Nearest neighbor matching\n  distance = \"mahalanobis\",   # Mahalanobis distance \n  ratio = 2,                  # 2:1 control/treated ratio)\n  replace = TRUE )            # With replacement \n\nmatch_model\n\nA `matchit` object\n - method: 2:1 nearest neighbor matching with replacement\n - distance: Mahalanobis - number of obs.: 161 (original), 145 (matched)\n - target estimand: ATT\n - covariates: dist_to_deep_water, ssta_freq, reef_exposure, reef_slope, reef_type, dist_mangroves, dist_fish_settl, dist_market, pollution_risk, monsoon_direction\n\n\n\n# Extract matched dataset\nmatched_data &lt;- match.data(match_model)\n\n\n# Inspect matching summary\nsummary(match_model)\n\n\nCall:\nmatchit(formula = treated_mpa ~ dist_to_deep_water + ssta_freq + \n    reef_exposure + reef_slope + reef_type + dist_mangroves + \n    dist_fish_settl + dist_market + pollution_risk + monsoon_direction, \n    data = data_clean, method = \"nearest\", distance = \"mahalanobis\", \n    replace = TRUE, ratio = 2)\n\nSummary of Balance for All Data:\n                          Means Treated Means Control Std. Mean Diff.\ndist_to_deep_water             627.0960      655.7936         -0.0314\nssta_freq                       25.4444       24.5094          0.1830\nreef_exposureExposed             0.8519        0.6604          0.5390\nreef_exposureSemi-exposed        0.1481        0.3396         -0.5390\nreef_slopeFlat                   0.0926        0.1321         -0.1362\nreef_slopeSlope                  0.8333        0.8491         -0.0422\nreef_slopeWall                   0.0741        0.0189          0.2108\nreef_typeBarrier                 0.0926        0.1509         -0.2013\nreef_typeFringing                0.8611        0.8491          0.0349\nreef_typePatch                   0.0463        0.0000          0.2203\ndist_mangroves                4618.4108     9679.3287         -0.9799\ndist_fish_settl              32505.0449    25646.2642          0.2175\ndist_market                 139918.0030   124152.3646          0.2374\npollution_riskHigh               0.0093        0.0000          0.0967\npollution_riskLow                0.7778        0.6226          0.3732\npollution_riskMedium             0.2130        0.3774         -0.4016\nmonsoon_directionNW              0.5741        0.3585          0.4360\nmonsoon_directionSE              0.4259        0.6415         -0.4360\n                          Var. Ratio eCDF Mean eCDF Max\ndist_to_deep_water            1.0168    0.0271   0.0924\nssta_freq                     0.4116    0.0749   0.1651\nreef_exposureExposed               .    0.1915   0.1915\nreef_exposureSemi-exposed          .    0.1915   0.1915\nreef_slopeFlat                     .    0.0395   0.0395\nreef_slopeSlope                    .    0.0157   0.0157\nreef_slopeWall                     .    0.0552   0.0552\nreef_typeBarrier                   .    0.0584   0.0584\nreef_typeFringing                  .    0.0121   0.0121\nreef_typePatch                     .    0.0463   0.0463\ndist_mangroves                0.1317    0.0755   0.1707\ndist_fish_settl               1.2507    0.0780   0.1988\ndist_market                   1.7808    0.1061   0.3515\npollution_riskHigh                 .    0.0093   0.0093\npollution_riskLow                  .    0.1551   0.1551\npollution_riskMedium               .    0.1644   0.1644\nmonsoon_directionNW                .    0.2156   0.2156\nmonsoon_directionSE                .    0.2156   0.2156\n\nSummary of Balance for Matched Data:\n                          Means Treated Means Control Std. Mean Diff.\ndist_to_deep_water             627.0960      344.4758          0.3097\nssta_freq                       25.4444       25.5370         -0.0181\nreef_exposureExposed             0.8519        0.8380          0.0391\nreef_exposureSemi-exposed        0.1481        0.1620         -0.0391\nreef_slopeFlat                   0.0926        0.0556          0.1278\nreef_slopeSlope                  0.8333        0.9120         -0.2112\nreef_slopeWall                   0.0741        0.0324          0.1591\nreef_typeBarrier                 0.0926        0.0880          0.0160\nreef_typeFringing                0.8611        0.9120         -0.1473\nreef_typePatch                   0.0463        0.0000          0.2203\ndist_mangroves                4618.4108     3903.1137          0.1385\ndist_fish_settl              32505.0449    23907.6888          0.2726\ndist_market                 139918.0030   138886.2537          0.0155\npollution_riskHigh               0.0093        0.0000          0.0967\npollution_riskLow                0.7778        0.7963         -0.0445\npollution_riskMedium             0.2130        0.2037          0.0226\nmonsoon_directionNW              0.5741        0.5093          0.1311\nmonsoon_directionSE              0.4259        0.4907         -0.1311\n                          Var. Ratio eCDF Mean eCDF Max Std. Pair Dist.\ndist_to_deep_water            3.0661    0.1022   0.2176          0.6162\nssta_freq                     1.4678    0.0244   0.0926          0.7032\nreef_exposureExposed               .    0.0139   0.0139          0.1434\nreef_exposureSemi-exposed          .    0.0139   0.0139          0.1434\nreef_slopeFlat                     .    0.0370   0.0370          0.1278\nreef_slopeSlope                    .    0.0787   0.0787          0.2112\nreef_slopeWall                     .    0.0417   0.0417          0.1591\nreef_typeBarrier                   .    0.0046   0.0046          0.0160\nreef_typeFringing                  .    0.0509   0.0509          0.1473\nreef_typePatch                     .    0.0463   0.0463          0.2203\ndist_mangroves                1.6708    0.0714   0.1759          0.7967\ndist_fish_settl               1.0892    0.1147   0.3056          0.6229\ndist_market                   2.0937    0.1334   0.3889          0.5956\npollution_riskHigh                 .    0.0093   0.0093          0.0967\npollution_riskLow                  .    0.0185   0.0185          0.0445\npollution_riskMedium               .    0.0093   0.0093          0.0226\nmonsoon_directionNW                .    0.0648   0.0648          0.2247\nmonsoon_directionSE                .    0.0648   0.0648          0.2247\n\nSample Sizes:\n              Control Treated\nAll             53.       108\nMatched (ESS)   17.69     108\nMatched         37.       108\nUnmatched       16.         0\nDiscarded        0.         0\n\n\n\n‚úÖ Your turn\nQ5. Report how many treated units were matched and how many control sites were included in the matched data.\nResponse: _________________________\nQ6. Find the covariate with the largest standardized mean difference (SMD) after matching. Which covariate is it?\nResponse: _________________________\nVisualize balance on the covariates using plot() with type = \"density\"\n\nplot(match_model, type = \"density\", interactive = FALSE,\n     which.xs = ~dist_to_deep_water + ssta_freq + reef_exposure)"
  },
  {
    "objectID": "course-materials/labs/week4.html#evaluate-balance-after-matching-love-plot-balance-table",
    "href": "course-materials/labs/week4.html#evaluate-balance-after-matching-love-plot-balance-table",
    "title": "Lab Week 4: Statistical Matching + Fixed Effects",
    "section": "4. Evaluate balance after matching (love plot + balance table)",
    "text": "4. Evaluate balance after matching (love plot + balance table)\n\n# Create nicer variable labels for plots/tables\nnice_names &lt;- data.frame(\n  old = c(\n    \"dist_to_deep_water\", \"ssta_freq\", \"reef_exposure\", \"reef_slope\",\n    \"reef_type\", \"dist_mangroves\", \"dist_fish_settl\", \"dist_market\",\n    \"pollution_risk\", \"monsoon_direction\"),\n  new = c(\n    \"Distance to deep water\", \"SSTA frequency\", \"Reef exposure\", \"Reef slope\",\n    \"Reef type\", \"Distance to mangroves (m)\", \"Distance to fishing settlement (m)\",\n    \"Distance to market (m)\", \"Watershed pollution risk\", \"Monsoon direction (SE/NW)\"))\n\nCreate a Love plot to visualize standardized mean differences (before & after matching)\n\nlove.plot(\n  match_model,\n  stats = \"mean.diffs\",\n  thresholds = c(m = 0.1),\n  var.names = nice_names)"
  },
  {
    "objectID": "course-materials/labs/week4.html#estimate-differences-in-outcomes-using-the-matched-sample",
    "href": "course-materials/labs/week4.html#estimate-differences-in-outcomes-using-the-matched-sample",
    "title": "Lab Week 4: Statistical Matching + Fixed Effects",
    "section": "5. Estimate differences in outcomes using the matched sample",
    "text": "5. Estimate differences in outcomes using the matched sample\n\nWhen matching with replacement, some control sites were matched multiple times and receive larger weights.\n\nSo we estimate outcome differences using the weights provided by match.data().\n\nRegression (Outcome 1): Key fisheries families biomass\n\nreg_ecological &lt;- lm(\n  biomass_ecological ~ treated_mpa,\n  data = matched_data,\n  weights = weights)\n\nsumm(reg_ecological, model.fit = FALSE)\n\n\n\n\n\nObservations\n145\n\n\nDependent variable\nbiomass_ecological\n\n\nType\nOLS linear regression\n\n\n\n\n  \n\n\n\n\n\nEst.\nS.E.\nt val.\np\n\n\n\n\n(Intercept)\n290.77\n106.02\n2.74\n0.01\n\n\ntreated_mpaMPA site\n79.50\n122.84\n0.65\n0.52\n\n\n\n Standard errors: OLS\n\n\n\n\n\n\n\n\n\n\n\nRegression (Outcome 2): Key fisheries families biomass\n\nreg_fisheries &lt;- lm(\n  biomass_fisheries ~ treated_mpa,\n  data = matched_data,\n  weights = weights)\n\nsumm(reg_fisheries, model.fit = FALSE)\n\n\n\n\n\nObservations\n145\n\n\nDependent variable\nbiomass_fisheries\n\n\nType\nOLS linear regression\n\n\n\n\n  \n\n\n\n\n\nEst.\nS.E.\nt val.\np\n\n\n\n\n(Intercept)\n64.23\n52.47\n1.22\n0.22\n\n\ntreated_mpaMPA site\n107.78\n60.79\n1.77\n0.08\n\n\n\n Standard errors: OLS\n\n\n\n\n\n\n\n\n\n\n\nView biomass differences by treatment group as simple weighted mean comparisons:\n\nmatched_data %&gt;%\n  group_by(treated_mpa) %&gt;%\n  summarize(\n    n_obs = n(),\n    avg_biomass_ecological = weighted.mean(biomass_ecological, w = weights),\n    avg_biomass_fisheries  = weighted.mean(biomass_fisheries,  w = weights),\n    .groups = \"drop\") %&gt;%\n  gt() %&gt;%\n  tab_header(title = \"Weighted mean outcomes in matched sample\")\n\n\n\n\n\n\n\nWeighted mean outcomes in matched sample\n\n\ntreated_mpa\nn_obs\navg_biomass_ecological\navg_biomass_fisheries\n\n\n\n\nControl (non‚ÄëMPA)\n37\n290.7664\n64.2348\n\n\nMPA site\n108\n370.2682\n172.0131\n\n\n\n\n\n\n\n\n‚úÖ Your turn\nQ7. Interpret one coefficient from the regressions above (choose ecological or fisheries). Write the interpretation of coefficient in plain language.\nResponse: _________________________\nQ8. What is the biggest remaining threat to causal interpretation of regression coefficients after matching?\nResponse: _________________________"
  },
  {
    "objectID": "course-materials/labs/week4.html#fixed-effects-estimation---an-applied-example",
    "href": "course-materials/labs/week4.html#fixed-effects-estimation---an-applied-example",
    "title": "Lab Week 4: Statistical Matching + Fixed Effects",
    "section": "6. Fixed Effects Estimation - An Applied Example",
    "text": "6. Fixed Effects Estimation - An Applied Example\nReplication of fixed effects estimator from applied study:\nDudney, J., Willing, C. E., Das, A. J., Latimer, A. M., Nesmith, J. C., & Battles, J. J. (2021). Nonlinear shifts in infectious rust disease due to climate change. Nature communications, 12(1), 5102.\n\n\n\n\n\n\n\nIn this section we will look at a simple fixed effects (FE) example using panel-style data, where the same units (plots) are observed over multiple years (1995, 2016).\n\n\nThe key issue: observations from the same plot are likely to share stable characteristics (soil type, microclimate, slope, management history) that we don‚Äôt measure well‚Äîand those unobserved factors can bias a na√Øve regression."
  },
  {
    "objectID": "course-materials/labs/week4.html#read-in-the-data-change-fixed-effext-to-factor-variables",
    "href": "course-materials/labs/week4.html#read-in-the-data-change-fixed-effext-to-factor-variables",
    "title": "Lab Week 4: Statistical Matching + Fixed Effects",
    "section": "Read in the data & change fixed effext to factor variables",
    "text": "Read in the data & change fixed effext to factor variables\n\nBy Coding plot and year as factors lets us include them as indicator (dummy) variables in an FE regression.\n\n\ndata_fe &lt;- read_csv(here(\"week4\", \"Dudney2021_study_data.csv\"), show_col_types = FALSE) %&gt;% \n  mutate(year=factor(year), plot=factor(plot))"
  },
  {
    "objectID": "course-materials/labs/week4.html#estimate-a-no-fixed-effects-baseline-model",
    "href": "course-materials/labs/week4.html#estimate-a-no-fixed-effects-baseline-model",
    "title": "Lab Week 4: Statistical Matching + Fixed Effects",
    "section": "Estimate a ‚Äúno fixed effects‚Äù baseline model",
    "text": "Estimate a ‚Äúno fixed effects‚Äù baseline model\n\nmod1_nofe &lt;- lm(perinc ~ vpd + I(vpd^2) + dbh + density,\n          data = data_fe)\n\nsumm(mod1_nofe, model.fit = FALSE, digits = 3)\n\n\n\n\n\nObservations\n294\n\n\nDependent variable\nperinc\n\n\nType\nOLS linear regression\n\n\n\n\n  \n\n\n\n\n\nEst.\nS.E.\nt val.\np\n\n\n\n\n(Intercept)\n-0.615\n0.122\n-5.027\n0.000\n\n\nvpd\n0.126\n0.022\n5.712\n0.000\n\n\nI(vpd^2)\n-0.005\n0.001\n-5.119\n0.000\n\n\ndbh\n-0.001\n0.001\n-2.499\n0.013\n\n\ndensity\n-0.000\n0.000\n-3.287\n0.001\n\n\n\n Standard errors: OLS"
  },
  {
    "objectID": "course-materials/labs/week4.html#estimate-the-fixed-effects-model-add-in-the-fixed-effects-for-plot-year",
    "href": "course-materials/labs/week4.html#estimate-the-fixed-effects-model-add-in-the-fixed-effects-for-plot-year",
    "title": "Lab Week 4: Statistical Matching + Fixed Effects",
    "section": "Estimate the fixed effects model: Add in the fixed effects for plot & year",
    "text": "Estimate the fixed effects model: Add in the fixed effects for plot & year\n\nplot fixed effects absorb all time-invariant differences across plots (e.g., baseline soil quality)\nyear fixed effects absorb common shocks shared by all plots in a given year (e.g., a region-wide drought year)\n\n\nfe_lm &lt;- lm(perinc ~ vpd + I(vpd^2) + dbh + density + plot + year,\n          data = data_fe)\n\nsumm(fe_lm, model.fit = FALSE, digits = 3)\n\n\n\n\n\nObservations\n294\n\n\nDependent variable\nperinc\n\n\nType\nOLS linear regression\n\n\n\n\n  \n\n\n\n\n\nEst.\nS.E.\nt val.\np\n\n\n\n\n(Intercept)\n-1.199\n0.770\n-1.556\n0.122\n\n\nvpd\n0.227\n0.093\n2.449\n0.016\n\n\nI(vpd^2)\n-0.010\n0.002\n-4.209\n0.000\n\n\ndbh\n-0.001\n0.001\n-0.973\n0.332\n\n\ndensity\n-0.001\n0.000\n-2.275\n0.024\n\n\nplot2\n0.092\n0.177\n0.516\n0.606\n\n\nplot3\n0.140\n0.180\n0.778\n0.438\n\n\nplot4\n0.118\n0.186\n0.634\n0.527\n\n\nplot5\n0.181\n0.185\n0.978\n0.330\n\n\nplot6\n0.136\n0.126\n1.079\n0.283\n\n\nplot7\n0.007\n0.101\n0.066\n0.948\n\n\nplot8\n0.198\n0.262\n0.757\n0.450\n\n\nplot9\n0.070\n0.149\n0.472\n0.638\n\n\nplot10\n0.156\n0.117\n1.336\n0.184\n\n\nplot11\n0.147\n0.169\n0.871\n0.385\n\n\nplot13\n0.160\n0.167\n0.957\n0.340\n\n\nplot14\n0.344\n0.122\n2.816\n0.006\n\n\nplot15\n0.107\n0.198\n0.542\n0.589\n\n\nplot16\n0.207\n0.222\n0.933\n0.352\n\n\nplot17\n0.161\n0.121\n1.329\n0.186\n\n\nplot18\n0.125\n0.187\n0.668\n0.505\n\n\nplot19\n0.107\n0.194\n0.554\n0.580\n\n\nplot20\n0.138\n0.204\n0.677\n0.500\n\n\nplot21\n0.144\n0.179\n0.805\n0.422\n\n\nplot22\n0.197\n0.158\n1.249\n0.214\n\n\nplot23\n0.090\n0.141\n0.638\n0.525\n\n\nplot24\n0.112\n0.158\n0.704\n0.483\n\n\nplot25\n0.142\n0.230\n0.618\n0.537\n\n\nplot26\n0.186\n0.223\n0.831\n0.408\n\n\nplot28\n0.170\n0.190\n0.894\n0.373\n\n\nplot29\n0.078\n0.155\n0.504\n0.615\n\n\nplot30\n0.266\n0.230\n1.157\n0.249\n\n\nplot31\n0.131\n0.160\n0.815\n0.416\n\n\nplot32\n0.103\n0.157\n0.656\n0.513\n\n\nplot34\n0.118\n0.194\n0.610\n0.543\n\n\nplot35\n0.265\n0.261\n1.017\n0.311\n\n\nplot36\n0.025\n0.121\n0.207\n0.836\n\n\nplot37\n0.131\n0.165\n0.799\n0.426\n\n\nplot38\n0.747\n0.088\n8.439\n0.000\n\n\nplot39\n-0.012\n0.091\n-0.134\n0.894\n\n\nplot40\n0.082\n0.090\n0.915\n0.362\n\n\nplot41\n0.231\n0.121\n1.907\n0.059\n\n\nplot42\n0.423\n0.263\n1.607\n0.110\n\n\nplot43\n0.366\n0.142\n2.584\n0.011\n\n\nplot44\n0.296\n0.202\n1.462\n0.146\n\n\nplot45\n0.038\n0.116\n0.333\n0.740\n\n\nplot46\n0.248\n0.143\n1.733\n0.085\n\n\nplot47\n0.543\n0.143\n3.795\n0.000\n\n\nplot48\n0.107\n0.133\n0.801\n0.425\n\n\nplot49\n0.059\n0.092\n0.643\n0.521\n\n\nplot51\n0.577\n0.114\n5.039\n0.000\n\n\nplot52\n0.178\n0.256\n0.695\n0.488\n\n\nplot53\n0.152\n0.252\n0.604\n0.547\n\n\nplot54\n0.323\n0.113\n2.857\n0.005\n\n\nplot55\n0.204\n0.192\n1.060\n0.291\n\n\nplot56\n0.203\n0.267\n0.761\n0.448\n\n\nplot57\n0.172\n0.180\n0.957\n0.340\n\n\nplot58\n0.020\n0.114\n0.179\n0.858\n\n\nplot59\n0.221\n0.219\n1.010\n0.314\n\n\nplot60\n0.105\n0.214\n0.489\n0.625\n\n\nplot61\n0.136\n0.119\n1.135\n0.258\n\n\nplot62\n0.141\n0.224\n0.629\n0.531\n\n\nplot63\n0.234\n0.188\n1.242\n0.216\n\n\nplot64\n0.201\n0.299\n0.674\n0.502\n\n\nplot65\n0.083\n0.186\n0.449\n0.654\n\n\nplot66\n0.120\n0.130\n0.924\n0.357\n\n\nplot67\n0.084\n0.124\n0.683\n0.496\n\n\nplot68\n0.074\n0.090\n0.822\n0.412\n\n\nplot69\n0.201\n0.161\n1.248\n0.214\n\n\nplot70\n0.276\n0.113\n2.449\n0.016\n\n\nplot71\n0.126\n0.201\n0.630\n0.530\n\n\nplot72\n0.221\n0.174\n1.266\n0.208\n\n\nplot73\n0.044\n0.123\n0.356\n0.722\n\n\nplot74\n0.069\n0.130\n0.529\n0.598\n\n\nplot75\n0.043\n0.123\n0.347\n0.729\n\n\nplot76\n0.155\n0.252\n0.618\n0.538\n\n\nplot77\n0.207\n0.228\n0.905\n0.367\n\n\nplot78\n0.295\n0.205\n1.439\n0.152\n\n\nplot79\n0.145\n0.234\n0.619\n0.537\n\n\nplot80\n0.266\n0.252\n1.057\n0.292\n\n\nplot82\n0.239\n0.273\n0.876\n0.383\n\n\nplot83\n0.237\n0.261\n0.908\n0.365\n\n\nplot84\n0.260\n0.270\n0.965\n0.336\n\n\nplot85\n0.298\n0.297\n1.001\n0.319\n\n\nplot86\n0.120\n0.209\n0.576\n0.565\n\n\nplot87\n0.088\n0.124\n0.710\n0.479\n\n\nplot88\n0.296\n0.093\n3.199\n0.002\n\n\nplot89\n0.175\n0.125\n1.400\n0.164\n\n\nplot90\n0.018\n0.101\n0.175\n0.861\n\n\nplot91\n0.017\n0.102\n0.166\n0.868\n\n\nplot92\n0.078\n0.150\n0.522\n0.602\n\n\nplot93\n0.170\n0.154\n1.100\n0.273\n\n\nplot94\n-0.001\n0.095\n-0.012\n0.990\n\n\nplot95\n0.234\n0.141\n1.666\n0.098\n\n\nplot96\n0.083\n0.096\n0.858\n0.392\n\n\nplot97\n0.216\n0.150\n1.440\n0.152\n\n\nplot98\n0.180\n0.162\n1.108\n0.270\n\n\nplot99\n0.125\n0.159\n0.785\n0.434\n\n\nplot100\n0.019\n0.117\n0.165\n0.869\n\n\nplot101\n0.088\n0.119\n0.745\n0.457\n\n\nplot102\n0.010\n0.114\n0.084\n0.933\n\n\nplot103\n0.171\n0.174\n0.980\n0.329\n\n\nplot104\n0.157\n0.173\n0.909\n0.365\n\n\nplot105\n0.117\n0.171\n0.687\n0.493\n\n\nplot106\n0.102\n0.090\n1.140\n0.256\n\n\nplot107\n0.038\n0.112\n0.334\n0.739\n\n\nplot108\n0.086\n0.120\n0.716\n0.475\n\n\nplot110\n0.092\n0.153\n0.601\n0.549\n\n\nplot111\n0.074\n0.096\n0.767\n0.444\n\n\nplot112\n0.201\n0.105\n1.912\n0.058\n\n\nplot113\n0.090\n0.128\n0.704\n0.482\n\n\nplot114\n0.270\n0.089\n3.038\n0.003\n\n\nplot115\n0.008\n0.109\n0.075\n0.941\n\n\nplot116\n0.111\n0.121\n0.918\n0.360\n\n\nplot117\n0.057\n0.108\n0.530\n0.597\n\n\nplot118\n0.169\n0.167\n1.014\n0.312\n\n\nplot119\n0.074\n0.167\n0.444\n0.658\n\n\nplot120\n0.139\n0.182\n0.767\n0.444\n\n\nplot121\n0.191\n0.092\n2.073\n0.040\n\n\nplot122\n0.033\n0.132\n0.252\n0.802\n\n\nplot123\n0.072\n0.113\n0.634\n0.527\n\n\nplot124\n0.080\n0.142\n0.565\n0.573\n\n\nplot125\n0.381\n0.224\n1.695\n0.092\n\n\nplot126\n0.106\n0.126\n0.845\n0.400\n\n\nplot127\n0.172\n0.138\n1.245\n0.215\n\n\nplot128\n0.505\n0.104\n4.851\n0.000\n\n\nplot129\n0.104\n0.171\n0.607\n0.545\n\n\nplot130\n0.292\n0.196\n1.490\n0.138\n\n\nplot131\n0.195\n0.175\n1.118\n0.266\n\n\nplot132\n0.254\n0.094\n2.705\n0.008\n\n\nplot133\n0.258\n0.145\n1.783\n0.077\n\n\nplot134\n0.386\n0.269\n1.434\n0.154\n\n\nplot135\n0.103\n0.132\n0.777\n0.438\n\n\nplot136\n0.053\n0.095\n0.553\n0.581\n\n\nplot137\n0.136\n0.221\n0.615\n0.539\n\n\nplot138\n0.244\n0.198\n1.232\n0.220\n\n\nplot139\n0.095\n0.154\n0.616\n0.539\n\n\nplot140\n0.295\n0.094\n3.143\n0.002\n\n\nplot141\n0.158\n0.150\n1.058\n0.292\n\n\nplot142\n0.139\n0.184\n0.756\n0.451\n\n\nplot143\n0.105\n0.117\n0.896\n0.372\n\n\nplot145\n-0.020\n0.090\n-0.218\n0.828\n\n\nplot146\n0.342\n0.268\n1.276\n0.204\n\n\nplot147\n0.367\n0.275\n1.335\n0.184\n\n\nplot148\n0.214\n0.129\n1.664\n0.098\n\n\nplot149\n0.231\n0.131\n1.757\n0.081\n\n\nplot150\n0.245\n0.199\n1.233\n0.220\n\n\nplot151\n0.119\n0.196\n0.607\n0.545\n\n\nplot152\n0.340\n0.099\n3.426\n0.001\n\n\nplot153\n0.186\n0.128\n1.448\n0.150\n\n\nplot154\n0.065\n0.103\n0.636\n0.526\n\n\nyear2016\n-0.050\n0.072\n-0.693\n0.490\n\n\n\n Standard errors: OLS\n\n\n\n\n\n\n\n\n\n\n\n\n‚úÖ Your turn\nQ9. Compare the coefficient on vpd in the no-FE model vs the FE model. Did the estimate get larger, smaller, or change sign? What does that suggest about confounding from unobserved plot differences?\nResponse: _________________________\nQ10. Why include year fixed effects? Give one concrete example of a ‚Äúyear shock‚Äù that could bias estimates if not controlled (choose a different example than provided above).\nResponse: _________________________\nQ11. Why include plot fixed effects? Give one concrete example of a ‚Äútime-invariant plot difference‚Äù that could bias estimates if not controlled (choose a different example than provided above).\nResponse: _________________________"
  },
  {
    "objectID": "course-materials/labs/week4_notes.html#lab-outline",
    "href": "course-materials/labs/week4_notes.html#lab-outline",
    "title": "Lab Week 4: Statistical Matching + Fixed Effects",
    "section": "Lab Outline",
    "text": "Lab Outline\n\nLoad in the packages and data\nTake a look at focal variables\nChecking covariate imbalance (pre‚Äëmatching).\nConduct a matching analysis (Mahalanobis matching)\nEvaluate balance after matching\nEstimate regression models for each outcome using the matched sample\nPractice fixed effects estimation"
  },
  {
    "objectID": "course-materials/labs/week4_notes.html#applied-study-data-source",
    "href": "course-materials/labs/week4_notes.html#applied-study-data-source",
    "title": "Lab Week 4: Statistical Matching + Fixed Effects",
    "section": "Applied study & data source",
    "text": "Applied study & data source\nThis lab uses open-access replication data from:\n\nAhmadia, G. N., Glew, L., Provost, M., Gill, D., Hidayat, N. I., Mangubhai, S., Purwanto, P., Fox, H. E. (2015). Integrating impact evaluation in the design and implementation of monitoring marine protected areas. Philosophical Transactions of the Royal Society B.\n\nBig idea: MPAs are not randomly placed. They may be placed in ‚Äúbetter‚Äù locations which means that treated and control groups are unlikely to make an apples-to-apples comparison. AKA, we are up against the nearly universal causal inference problem with non-experimental data, treatment assignment selection bias.\nAhmadia et al.¬†(2015) uses statistical matching to construct a more apples-to-apples comparison group. In this lab exercise we will approximately replicate the main matching analysis conducted in this study.\n\n\n\n\n\n\nNote\n\n\n\nNOTE: When matching, the authors imposed additional restrictions (caliper-style constraints) on some habitat variables. We do not fully replicate those constraints here.\n\n\n\n\nReading reference:\n\nMatchIt Vignette: https://kosukeimai.github.io/MatchIt/articles/MatchIt.html"
  },
  {
    "objectID": "course-materials/labs/week4_notes.html#load-packages-study-data",
    "href": "course-materials/labs/week4_notes.html#load-packages-study-data",
    "title": "Lab Week 4: Statistical Matching + Fixed Effects",
    "section": "0. Load packages + study data",
    "text": "0. Load packages + study data\n\nlibrary(tidyverse)   # data wrangling + plotting\nlibrary(here)        # portable file paths (project-root relative)\nlibrary(janitor)     # clean variable names\nlibrary(gtsummary)   # clean summary tables\nlibrary(gt)          # optional: render gtsummary nicely\nlibrary(MatchIt)     # matching estimation\nlibrary(cobalt)      # balance checks + love plots\nlibrary(jtools)      # clean regression output summaries\n\nRead in the cleaned survey data\n\ndata_clean &lt;- read_csv(here(\"week4\", \"Ahmadia_2015_DataClean.csv\"), show_col_types = FALSE) # show col type = False quiets the message on printing column types"
  },
  {
    "objectID": "course-materials/labs/week4_notes.html#create-variable-description-table-for-key-variables-in-the-matching-analysis",
    "href": "course-materials/labs/week4_notes.html#create-variable-description-table-for-key-variables-in-the-matching-analysis",
    "title": "Lab Week 4: Statistical Matching + Fixed Effects",
    "section": "1. Create variable description table for key variables in the matching analysis",
    "text": "1. Create variable description table for key variables in the matching analysis\n\n\n\n\n\n\n\n\nVariable Descriptions (Treatment, Outcomes, Matching Covariates) - Ahmadia (2015)\n\n\nLabel\nDescription\n\n\n\n\ntreated_mpa (Treatment)\nSite is inside a Marine Protected Area (1 = inside MPA , 0 = outside MPA).\n\n\nbiomass_fisheries (Outcome)\nBiomass of key fisheries families (Serranidae, Lutjanidae, Haemulidae) measured in kg per hectare (kg/ha).\n\n\nbiomass_ecological (Outcome)\nBiomass of herbivorous fish families (Acanthuridae, Scaridae, Siganidae) measured in kg per hectare (kg/ha).\n\n\ndist_deep_water (Covariate)\nCovariate: Distance to deep water (50 m depth contour), in meters (m).\n\n\nssta_freq (Covariate)\nFrequency of sea-surface temperature anomalies (SSTA)\n\n\nreef_exposure (Covariate)\nReef wave exposure (Exposed, Semi-exposed, Sheltered).\n\n\nreef_slope (Covariate)\nReef slope (Flat, Slope, Wall)\n\n\nreef_type (Covariate)\nReef type (Patch, Fringing, Barrier, Atoll).\n\n\ndist_mangroves (Covariate)\nDistance to nearest mangrove habitat, in meters (m).\n\n\ndist_fishing_settlement (Covariate)\nDistance to nearest fishing settlement, in meters (m).\n\n\ndist_market (Covariate)\nDistance to primary market, in meters (m).\n\n\npollution_risk (Covariate)\nWatershed pollution risk. (1 = Low; 2 = Medium; 3 = High).\n\n\nmonsoon_direction (Covariate)\nMonsoon wind exposure direction. (Northwest (NW), Southeast (SE))."
  },
  {
    "objectID": "course-materials/labs/week4_notes.html#check-imbalance-before-matching-covariate-balance-table",
    "href": "course-materials/labs/week4_notes.html#check-imbalance-before-matching-covariate-balance-table",
    "title": "Lab Week 4: Statistical Matching + Fixed Effects",
    "section": "2. Check imbalance before matching (covariate balance table)",
    "text": "2. Check imbalance before matching (covariate balance table)\nGoal: Compare covariate distributions between treated_mpa and control sites before matching.\n\ndata_clean %&gt;%\n  select(\n    treated_mpa, dist_to_deep_water, ssta_freq, reef_exposure,\n    reef_slope, reef_type,\n    dist_mangroves, dist_fish_settl, dist_market,\n    pollution_risk, monsoon_direction) %&gt;%\n  tbl_summary(\n    by = treated_mpa,\n    statistic = list(\n      all_continuous() ~ \"{mean} ({sd})\",\n      all_categorical() ~ \"{n} ({p}%)\"\n    )\n  ) %&gt;%\n  modify_header(label ~ \"**Covariate**\") %&gt;%\n  modify_spanning_header(c(\"stat_1\", \"stat_2\") ~ \"**Group**\")\n\n\n\n\n\n\n\n\n\n\n\n\nCovariate\n\nGroup\n\n\n\nControl (non‚ÄëMPA)\nN = 531\nMPA site\nN = 1081\n\n\n\n\ndist_to_deep_water\n656 (905)\n627 (913)\n\n\nssta_freq\n25 (8)\n25 (5)\n\n\nreef_exposure\n\n\n\n\n\n\n¬†¬†¬†¬†Exposed\n35 (66%)\n92 (85%)\n\n\n¬†¬†¬†¬†Semi-exposed\n18 (34%)\n16 (15%)\n\n\nreef_slope\n\n\n\n\n\n\n¬†¬†¬†¬†Flat\n7 (13%)\n10 (9.3%)\n\n\n¬†¬†¬†¬†Slope\n45 (85%)\n90 (83%)\n\n\n¬†¬†¬†¬†Wall\n1 (1.9%)\n8 (7.4%)\n\n\nreef_type\n\n\n\n\n\n\n¬†¬†¬†¬†Barrier\n8 (15%)\n10 (9.3%)\n\n\n¬†¬†¬†¬†Fringing\n45 (85%)\n93 (86%)\n\n\n¬†¬†¬†¬†Patch\n0 (0%)\n5 (4.6%)\n\n\ndist_mangroves\n9,679 (14,229)\n4,618 (5,165)\n\n\ndist_fish_settl\n25,646 (28,199)\n32,505 (31,537)\n\n\ndist_market\n124,152 (49,763)\n139,918 (66,406)\n\n\npollution_risk\n\n\n\n\n\n\n¬†¬†¬†¬†High\n0 (0%)\n1 (0.9%)\n\n\n¬†¬†¬†¬†Low\n33 (62%)\n84 (78%)\n\n\n¬†¬†¬†¬†Medium\n20 (38%)\n23 (21%)\n\n\nmonsoon_direction\n\n\n\n\n\n\n¬†¬†¬†¬†NW\n19 (36%)\n62 (57%)\n\n\n¬†¬†¬†¬†SE\n34 (64%)\n46 (43%)\n\n\n\n1 Mean (SD); n (%)\n\n\n\n\n\n\n\n\n\n‚úÖ Your turn (write answers in the lab)\nQ1. Based on the balance table, name two covariates that look most different between MPA and non-MPA sites.\nResponse: - monsoon_direction: 57% of MPA sites face NW vs only 36% of control sites (21 percentage point difference) - reef_exposure: 85% of MPA sites are ‚ÄúExposed‚Äù vs only 66% of control sites (19 percentage point difference) - dist_mangroves: control = mean of 9679 meters, treat = mean of 4618 meters - dist_market: control = mean of 124152 meters, treat = mean of 139,918 meters\nQ2. Pick one of those imbalanced covariates and explain why it might confound the estimate of the effect of MPAs on fish biomass.\n\nmonsoon_direction: moonsoon exposures effect nutrient upwelling, larval connectivity, water quality, etc. If NW-facing reefs naturally have higher fish biomass due to favorable oceanographic conditions (regardless of protection status), then comparing MPA sites (57% NW) to control sites (36% NW) will confound our estimate. We might attribute higher biomass to MPA protection when it‚Äôs actually due to the beneficial environmental conditions associated with NW monsoon exposure.\nreef_exposure: Reef exposure refers to how open a reef is to wave action and ocean currents. More exposed reefs typically experience greater water circulation, better delivery of nutrients, higher oxygen levels\ndist_mangroves: Juvenile fish use mangroves as protected nursery grounds where they grow before migrating to coral reefs as adults. Mangroves provide shelter from predators and abundant food resources. Reefs closer to mangroves receive a steady supply of recruiting fish from these nursery areas. These physical conditions can naturally support higher fish biomass independent of any protection effects\ndist_marktet: reefs closers to market face higher fishing pressure because Easier access means more frequent fishing trips, Fresh fish can reach markets faster, More fishers will target accessible sites. Since MPA sites are systematically farther from markets (140 km vs 124 km), they may have higher fish biomass even in the absence of MPA protection simply because they‚Äôre less accessible to fishers."
  },
  {
    "objectID": "course-materials/labs/week4_notes.html#matching-plan-what-are-we-trying-to-estimate",
    "href": "course-materials/labs/week4_notes.html#matching-plan-what-are-we-trying-to-estimate",
    "title": "Lab Week 4: Statistical Matching + Fixed Effects",
    "section": "Matching plan (what are we trying to estimate?)",
    "text": "Matching plan (what are we trying to estimate?)\nIn {MatchIt}, most distance-based matching procedures (like nearest-neighbor matching) target is to estimate:\n\nATT: Average Treatment effect on the Treated\n\nWhy? When using matching methods we typically keep treated units and then select control units that most closely match them.\n\n‚úÖ Your turn\nQ4. What does ATT mean in the context of this MPA evaluation setting?\nResponse: ‚ÄúHow does the observed fish biomass at MPA sites compare to what the fish biomass at those same sites would have been if they had NOT been designated as MPAs?‚Äù\nTreated units = The 108 MPA sites that received protection ATT = The average difference in fish biomass for these 108 sites between:\n\nTheir actual outcome (with MPA protection), versus\nTheir counterfactual outcome (what would have happened without MPA protection)"
  },
  {
    "objectID": "course-materials/labs/week4_notes.html#mahalanobis-matching",
    "href": "course-materials/labs/week4_notes.html#mahalanobis-matching",
    "title": "Lab Week 4: Statistical Matching + Fixed Effects",
    "section": "3. Mahalanobis Matching",
    "text": "3. Mahalanobis Matching\nMatching criteria used in Ahmadia (2015)\n\nmethod = \"nearest-neighbor\": Nearest neighbor matching\ndistance = \"mahalanobis\": Mahalanobis distance\nratio = 2: Two controls matched to each treated unit (2:1 ratio; control/treated)\nreplace = TRUE: Controls can be reused (matched to multiple treated units)\n\n\nNOTE: This is the main matching method implemented in Ahmadia (2015) with the exception of the caliper-style constraints.\n\n\n# 1) Set a seed for reproducible matching\nset.seed(2412026)\n\n# 2) Fit a nearest-neighbor Mahalanobis matching model\nmatch_model &lt;- matchit(\n  treated_mpa ~ dist_to_deep_water + ssta_freq + reef_exposure +\n    reef_slope + reef_type + dist_mangroves + dist_fish_settl +\n    dist_market + pollution_risk + monsoon_direction,\n  data = data_clean,\n  method = \"nearest\",         # Nearest neighbor matching\n  distance = \"mahalanobis\",   # Mahalanobis distance \n  ratio = 2,                  # 2:1 control/treated ratio)\n  replace = TRUE )            # A single control site can be matched to multiple MPA sites, need this for control ratio\n\nmatch_model\n\nA `matchit` object\n - method: 2:1 nearest neighbor matching with replacement\n - distance: Mahalanobis - number of obs.: 161 (original), 145 (matched)\n - target estimand: ATT\n - covariates: dist_to_deep_water, ssta_freq, reef_exposure, reef_slope, reef_type, dist_mangroves, dist_fish_settl, dist_market, pollution_risk, monsoon_direction\n\n\n\n# Extract matched dataset\nmatched_data &lt;- match.data(match_model)\n\n\n# Inspect matching summary\nsummary(match_model)\n\n\nCall:\nmatchit(formula = treated_mpa ~ dist_to_deep_water + ssta_freq + \n    reef_exposure + reef_slope + reef_type + dist_mangroves + \n    dist_fish_settl + dist_market + pollution_risk + monsoon_direction, \n    data = data_clean, method = \"nearest\", distance = \"mahalanobis\", \n    replace = TRUE, ratio = 2)\n\nSummary of Balance for All Data:\n                          Means Treated Means Control Std. Mean Diff.\ndist_to_deep_water             627.0960      655.7936         -0.0314\nssta_freq                       25.4444       24.5094          0.1830\nreef_exposureExposed             0.8519        0.6604          0.5390\nreef_exposureSemi-exposed        0.1481        0.3396         -0.5390\nreef_slopeFlat                   0.0926        0.1321         -0.1362\nreef_slopeSlope                  0.8333        0.8491         -0.0422\nreef_slopeWall                   0.0741        0.0189          0.2108\nreef_typeBarrier                 0.0926        0.1509         -0.2013\nreef_typeFringing                0.8611        0.8491          0.0349\nreef_typePatch                   0.0463        0.0000          0.2203\ndist_mangroves                4618.4108     9679.3287         -0.9799\ndist_fish_settl              32505.0449    25646.2642          0.2175\ndist_market                 139918.0030   124152.3646          0.2374\npollution_riskHigh               0.0093        0.0000          0.0967\npollution_riskLow                0.7778        0.6226          0.3732\npollution_riskMedium             0.2130        0.3774         -0.4016\nmonsoon_directionNW              0.5741        0.3585          0.4360\nmonsoon_directionSE              0.4259        0.6415         -0.4360\n                          Var. Ratio eCDF Mean eCDF Max\ndist_to_deep_water            1.0168    0.0271   0.0924\nssta_freq                     0.4116    0.0749   0.1651\nreef_exposureExposed               .    0.1915   0.1915\nreef_exposureSemi-exposed          .    0.1915   0.1915\nreef_slopeFlat                     .    0.0395   0.0395\nreef_slopeSlope                    .    0.0157   0.0157\nreef_slopeWall                     .    0.0552   0.0552\nreef_typeBarrier                   .    0.0584   0.0584\nreef_typeFringing                  .    0.0121   0.0121\nreef_typePatch                     .    0.0463   0.0463\ndist_mangroves                0.1317    0.0755   0.1707\ndist_fish_settl               1.2507    0.0780   0.1988\ndist_market                   1.7808    0.1061   0.3515\npollution_riskHigh                 .    0.0093   0.0093\npollution_riskLow                  .    0.1551   0.1551\npollution_riskMedium               .    0.1644   0.1644\nmonsoon_directionNW                .    0.2156   0.2156\nmonsoon_directionSE                .    0.2156   0.2156\n\nSummary of Balance for Matched Data:\n                          Means Treated Means Control Std. Mean Diff.\ndist_to_deep_water             627.0960      344.4758          0.3097\nssta_freq                       25.4444       25.5370         -0.0181\nreef_exposureExposed             0.8519        0.8380          0.0391\nreef_exposureSemi-exposed        0.1481        0.1620         -0.0391\nreef_slopeFlat                   0.0926        0.0556          0.1278\nreef_slopeSlope                  0.8333        0.9120         -0.2112\nreef_slopeWall                   0.0741        0.0324          0.1591\nreef_typeBarrier                 0.0926        0.0880          0.0160\nreef_typeFringing                0.8611        0.9120         -0.1473\nreef_typePatch                   0.0463        0.0000          0.2203\ndist_mangroves                4618.4108     3903.1137          0.1385\ndist_fish_settl              32505.0449    23907.6888          0.2726\ndist_market                 139918.0030   138886.2537          0.0155\npollution_riskHigh               0.0093        0.0000          0.0967\npollution_riskLow                0.7778        0.7963         -0.0445\npollution_riskMedium             0.2130        0.2037          0.0226\nmonsoon_directionNW              0.5741        0.5093          0.1311\nmonsoon_directionSE              0.4259        0.4907         -0.1311\n                          Var. Ratio eCDF Mean eCDF Max Std. Pair Dist.\ndist_to_deep_water            3.0661    0.1022   0.2176          0.6162\nssta_freq                     1.4678    0.0244   0.0926          0.7032\nreef_exposureExposed               .    0.0139   0.0139          0.1434\nreef_exposureSemi-exposed          .    0.0139   0.0139          0.1434\nreef_slopeFlat                     .    0.0370   0.0370          0.1278\nreef_slopeSlope                    .    0.0787   0.0787          0.2112\nreef_slopeWall                     .    0.0417   0.0417          0.1591\nreef_typeBarrier                   .    0.0046   0.0046          0.0160\nreef_typeFringing                  .    0.0509   0.0509          0.1473\nreef_typePatch                     .    0.0463   0.0463          0.2203\ndist_mangroves                1.6708    0.0714   0.1759          0.7967\ndist_fish_settl               1.0892    0.1147   0.3056          0.6229\ndist_market                   2.0937    0.1334   0.3889          0.5956\npollution_riskHigh                 .    0.0093   0.0093          0.0967\npollution_riskLow                  .    0.0185   0.0185          0.0445\npollution_riskMedium               .    0.0093   0.0093          0.0226\nmonsoon_directionNW                .    0.0648   0.0648          0.2247\nmonsoon_directionSE                .    0.0648   0.0648          0.2247\n\nSample Sizes:\n              Control Treated\nAll             53.       108\nMatched (ESS)   17.69     108\nMatched         37.       108\nUnmatched       16.         0\nDiscarded        0.         0\n\n\nNotes: - what is diff between first and second df? first is our original, second is matched - matching worked well for reef exposure, distance to mangroves, distance to market - distance to deep water got worse\nMahalanobis distance optimizes overall similarity across ALL covariates simultaneously. The algorithm decided that:\n\nGetting reef_exposure right (85% ‚Üí 84%) was more important\nGetting reef type right was more important = Getting ssta_freq right was more important ‚Ä¶even if it meant sacrificing balance on dist_to_deep_water\n\nsample sizes: - 37 unique control sites were used as matches - all 108 treatment sites got matched - 16 controls were too dissimilar and not used - (ESS) = adjustment for reusing control sites. ESS = 17.69 means you‚Äôre getting about half the statistical power you‚Äôd get from 37 truly independent controls\n\n‚úÖ Your turn\nQ5. Report how many treated units were matched and how many control sites were included in the matched data.\nResponse: 108 treated units were mastched, and 37 control sites were used to match them\nQ6. Find the covariate with the largest standardized mean difference (SMD) after matching. Which covariate is it?\nResponse: dist_to_deep_water\nVisualize balance on the covariates using plot() with type = \"density\"\n\nplot(match_model, type = \"density\", interactive = FALSE,\n     which.xs = ~dist_to_deep_water + ssta_freq + reef_exposure)\n\n\n\n\n\n\n\n\n**Notes:* - goal is to make distributions match as much as possible - Do we see improvment? - All = before matching"
  },
  {
    "objectID": "course-materials/labs/week4_notes.html#evaluate-balance-after-matching-love-plot-balance-table",
    "href": "course-materials/labs/week4_notes.html#evaluate-balance-after-matching-love-plot-balance-table",
    "title": "Lab Week 4: Statistical Matching + Fixed Effects",
    "section": "4. Evaluate balance after matching (love plot + balance table)",
    "text": "4. Evaluate balance after matching (love plot + balance table)\n\n# Create nicer variable labels for plots/tables\nnice_names &lt;- data.frame(\n  old = c(\n    \"dist_to_deep_water\", \"ssta_freq\", \"reef_exposure\", \"reef_slope\",\n    \"reef_type\", \"dist_mangroves\", \"dist_fish_settl\", \"dist_market\",\n    \"pollution_risk\", \"monsoon_direction\"),\n  new = c(\n    \"Distance to deep water\", \"SSTA frequency\", \"Reef exposure\", \"Reef slope\",\n    \"Reef type\", \"Distance to mangroves (m)\", \"Distance to fishing settlement (m)\",\n    \"Distance to market (m)\", \"Watershed pollution risk\", \"Monsoon direction (SE/NW)\"))\n\nCreate a Love plot to visualize standardized mean differences (before & after matching)\n\nlove.plot(\n  match_model,\n  stats = \"mean.diffs\",\n  thresholds = c(m = 0.1),\n  var.names = nice_names)\n\n\n\n\n\n\n\n\nInterperting: - solid line at 0 = perfect balance - dashed lines at +- 0.1 = common threshold for ‚Äúacceptable‚Äù balance"
  },
  {
    "objectID": "course-materials/labs/week4_notes.html#estimate-differences-in-outcomes-using-the-matched-sample",
    "href": "course-materials/labs/week4_notes.html#estimate-differences-in-outcomes-using-the-matched-sample",
    "title": "Lab Week 4: Statistical Matching + Fixed Effects",
    "section": "5. Estimate differences in outcomes using the matched sample",
    "text": "5. Estimate differences in outcomes using the matched sample\n\nWhen matching with replacement, some control sites were matched multiple times and receive larger weights.\n\nSo we estimate outcome differences using the weights provided by match.data().\n\nNOTE: have studnets look at matched_data and look at weights column.\nRegression (Outcome 1): Herbivorous fisheries families biomass\n\nreg_ecological &lt;- lm(\n  biomass_ecological ~ treated_mpa,\n  data = matched_data,\n  weights = weights)\n\nsumm(reg_ecological, model.fit = FALSE)\n\n\n\n\n\nObservations\n145\n\n\nDependent variable\nbiomass_ecological\n\n\nType\nOLS linear regression\n\n\n\n\n  \n\n\n\n\n\nEst.\nS.E.\nt val.\np\n\n\n\n\n(Intercept)\n290.77\n106.02\n2.74\n0.01\n\n\ntreated_mpaMPA site\n79.50\n122.84\n0.65\n0.52\n\n\n\n Standard errors: OLS\n\n\n\n\n\n\n\n\n\n\n\ninterp: - Intercept = This is the average ecological fish biomass at matched control sites. Control reefs (non-MPAs) have about 291 kg/ha of fish biomass\n\ntreated_mpaMPA site = This is ATT estimate (the causal effect of MPA protection). MPA sites have, on average, 79.50 kg/ha MORE ecological fish biomass than similar control sites\nnot statistically significant\n\nRegression (Outcome 2): Key fisheries families biomass\n\nreg_fisheries &lt;- lm(\n  biomass_fisheries ~ treated_mpa,\n  data = matched_data,\n  weights = weights)\n\nsumm(reg_fisheries, model.fit = FALSE)\n\n\n\n\n\nObservations\n145\n\n\nDependent variable\nbiomass_fisheries\n\n\nType\nOLS linear regression\n\n\n\n\n  \n\n\n\n\n\nEst.\nS.E.\nt val.\np\n\n\n\n\n(Intercept)\n64.23\n52.47\n1.22\n0.22\n\n\ntreated_mpaMPA site\n107.78\n60.79\n1.77\n0.08\n\n\n\n Standard errors: OLS\n\n\n\n\n\n\n\n\n\n\n\ninterp: - Intercept = This is the average ecological fish biomass at matched control sites. Control reefs (non-MPAs) have about 64 kg/ha of key fish biomass\n\ntreated_mpaMPA site = This is ATT estimate (the causal effect of MPA protection). MPA sites have, on average, 107 kg/ha MORE key fish biomass than similar control sites\nmarginally significant\n\nView biomass differences by treatment group as simple weighted mean comparisons:\n\nmatched_data %&gt;%\n  group_by(treated_mpa) %&gt;%\n  summarize(\n    n_obs = n(),\n    avg_biomass_ecological = weighted.mean(biomass_ecological, w = weights),\n    avg_biomass_fisheries  = weighted.mean(biomass_fisheries,  w = weights),\n    .groups = \"drop\") %&gt;%\n  gt() %&gt;%\n  tab_header(title = \"Weighted mean outcomes in matched sample\")\n\n\n\n\n\n\n\nWeighted mean outcomes in matched sample\n\n\ntreated_mpa\nn_obs\navg_biomass_ecological\navg_biomass_fisheries\n\n\n\n\nControl (non‚ÄëMPA)\n37\n290.7664\n64.2348\n\n\nMPA site\n108\n370.2682\n172.0131\n\n\n\n\n\n\n\nMPAs appear to have a MUCH stronger effect on commercially valuable fish (+168%) than on overall ecological fish (+27%).\nMPAs more than double the biomass of commercially valuable fish species (+168%), with evidence just shy of conventional statistical significance (p = 0.08). This suggests MPAs are particularly effective at restoring populations of fish that are targeted by fishing pressure.\n\n‚úÖ Your turn\nQ7. Interpret one coefficient from the regressions above (choose ecological or fisheries). Write the interpretation of coefficient in plain language.\nResponse: treated_mpaMPA site = This is ATT estimate (the causal effect of MPA protection). MPA sites have, on average, 107 kg/ha MORE key fish biomass than similar control sites\nQ8. What is the biggest remaining threat to causal interpretation of regression coefficients after matching?\nResponse: Unobserved confounding variables. We can only match what we measured. But there could be other important confounders we didn‚Äôt measure or include that affect both which sites become an MPA and fish biomass levels."
  },
  {
    "objectID": "course-materials/labs/week4_notes.html#fixed-effects-estimation---an-applied-example",
    "href": "course-materials/labs/week4_notes.html#fixed-effects-estimation---an-applied-example",
    "title": "Lab Week 4: Statistical Matching + Fixed Effects",
    "section": "6. Fixed Effects Estimation - An Applied Example",
    "text": "6. Fixed Effects Estimation - An Applied Example\nReplication of fixed effects estimator from applied study:\nDudney, J., Willing, C. E., Das, A. J., Latimer, A. M., Nesmith, J. C., & Battles, J. J. (2021). Nonlinear shifts in infectious rust disease due to climate change. Nature communications, 12(1), 5102.\n\n\n\n\n\n\n\nIn this section we will look at a simple fixed effects (FE) example using panel-style data, where the same units (plots) are observed over multiple years (1995, 2016).\n\n\nThe key issue: observations from the same plot are likely to share stable characteristics (soil type, microclimate, slope, management history) that we don‚Äôt measure well‚Äîand those unobserved factors can bias a na√Øve regression."
  },
  {
    "objectID": "course-materials/labs/week4_notes.html#read-in-the-data-change-fixed-effext-to-factor-variables",
    "href": "course-materials/labs/week4_notes.html#read-in-the-data-change-fixed-effext-to-factor-variables",
    "title": "Lab Week 4: Statistical Matching + Fixed Effects",
    "section": "Read in the data & change fixed effext to factor variables",
    "text": "Read in the data & change fixed effext to factor variables\n\nBy Coding plot and year as factors lets us include them as indicator (dummy) variables in an FE regression.\n\n\ndata_fe &lt;- read_csv(here(\"week4\", \"Dudney2021_study_data.csv\"), show_col_types = FALSE) %&gt;% \n  mutate(year=factor(year), plot=factor(plot))"
  },
  {
    "objectID": "course-materials/labs/week4_notes.html#estimate-a-no-fixed-effects-baseline-model",
    "href": "course-materials/labs/week4_notes.html#estimate-a-no-fixed-effects-baseline-model",
    "title": "Lab Week 4: Statistical Matching + Fixed Effects",
    "section": "Estimate a ‚Äúno fixed effects‚Äù baseline model",
    "text": "Estimate a ‚Äúno fixed effects‚Äù baseline model\n\nmod1_nofe &lt;- lm(perinc ~ vpd + I(vpd^2) + dbh + density,\n          data = data_fe) # I(vpd^2) - VPD squared (quadratic term)\n\n # Allows for non-linear relationship between VPD and outcome\n # Allows effect to change across VPD range\n\nsumm(mod1_nofe, model.fit = FALSE, digits = 3)\n\n\n\n\n\nObservations\n294\n\n\nDependent variable\nperinc\n\n\nType\nOLS linear regression\n\n\n\n\n  \n\n\n\n\n\nEst.\nS.E.\nt val.\np\n\n\n\n\n(Intercept)\n-0.615\n0.122\n-5.027\n0.000\n\n\nvpd\n0.126\n0.022\n5.712\n0.000\n\n\nI(vpd^2)\n-0.005\n0.001\n-5.119\n0.000\n\n\ndbh\n-0.001\n0.001\n-2.499\n0.013\n\n\ndensity\n-0.000\n0.000\n-3.287\n0.001\n\n\n\n Standard errors: OLS\n\n\n\n\n\n\n\n\n\n\n\ninterpretation:\n\nvpd coefficient: +0.126 (p &lt; 0.001) -&gt; Positive linear term ‚Üí growth initially increases with VPD\nI(vpd^2) coefficient: -0.005 (p &lt; 0.001) : Negative quadratic term ‚Üí but the effect diminishes and eventually reverses at high VPD\ndbh: -0.001 (p = 0.013) : Negative coefficient ‚Üí Larger trees have slightly lower percent increase. For every 1 cm increase in diameter, percent increase drops by 0.001 (0.1%)\ndensity: -0.000 (p = 0.001) negative coefficient ‚Üí Higher stand density reduces individual tree growth"
  },
  {
    "objectID": "course-materials/labs/week4_notes.html#estimate-the-fixed-effects-model-add-in-the-fixed-effects-for-plot-year",
    "href": "course-materials/labs/week4_notes.html#estimate-the-fixed-effects-model-add-in-the-fixed-effects-for-plot-year",
    "title": "Lab Week 4: Statistical Matching + Fixed Effects",
    "section": "Estimate the fixed effects model: Add in the fixed effects for plot & year",
    "text": "Estimate the fixed effects model: Add in the fixed effects for plot & year\n\nplot fixed effects absorb all time-invariant differences across plots (e.g., baseline soil quality)\nyear fixed effects absorb common shocks shared by all plots in a given year (e.g., a region-wide drought year)\n\n\nfe_lm &lt;- lm(perinc ~ vpd + I(vpd^2) + dbh + density + plot + year,\n          data = data_fe)\n\nsumm(fe_lm, model.fit = FALSE, digits = 3)\n\n\n\n\n\nObservations\n294\n\n\nDependent variable\nperinc\n\n\nType\nOLS linear regression\n\n\n\n\n  \n\n\n\n\n\nEst.\nS.E.\nt val.\np\n\n\n\n\n(Intercept)\n-1.199\n0.770\n-1.556\n0.122\n\n\nvpd\n0.227\n0.093\n2.449\n0.016\n\n\nI(vpd^2)\n-0.010\n0.002\n-4.209\n0.000\n\n\ndbh\n-0.001\n0.001\n-0.973\n0.332\n\n\ndensity\n-0.001\n0.000\n-2.275\n0.024\n\n\nplot2\n0.092\n0.177\n0.516\n0.606\n\n\nplot3\n0.140\n0.180\n0.778\n0.438\n\n\nplot4\n0.118\n0.186\n0.634\n0.527\n\n\nplot5\n0.181\n0.185\n0.978\n0.330\n\n\nplot6\n0.136\n0.126\n1.079\n0.283\n\n\nplot7\n0.007\n0.101\n0.066\n0.948\n\n\nplot8\n0.198\n0.262\n0.757\n0.450\n\n\nplot9\n0.070\n0.149\n0.472\n0.638\n\n\nplot10\n0.156\n0.117\n1.336\n0.184\n\n\nplot11\n0.147\n0.169\n0.871\n0.385\n\n\nplot13\n0.160\n0.167\n0.957\n0.340\n\n\nplot14\n0.344\n0.122\n2.816\n0.006\n\n\nplot15\n0.107\n0.198\n0.542\n0.589\n\n\nplot16\n0.207\n0.222\n0.933\n0.352\n\n\nplot17\n0.161\n0.121\n1.329\n0.186\n\n\nplot18\n0.125\n0.187\n0.668\n0.505\n\n\nplot19\n0.107\n0.194\n0.554\n0.580\n\n\nplot20\n0.138\n0.204\n0.677\n0.500\n\n\nplot21\n0.144\n0.179\n0.805\n0.422\n\n\nplot22\n0.197\n0.158\n1.249\n0.214\n\n\nplot23\n0.090\n0.141\n0.638\n0.525\n\n\nplot24\n0.112\n0.158\n0.704\n0.483\n\n\nplot25\n0.142\n0.230\n0.618\n0.537\n\n\nplot26\n0.186\n0.223\n0.831\n0.408\n\n\nplot28\n0.170\n0.190\n0.894\n0.373\n\n\nplot29\n0.078\n0.155\n0.504\n0.615\n\n\nplot30\n0.266\n0.230\n1.157\n0.249\n\n\nplot31\n0.131\n0.160\n0.815\n0.416\n\n\nplot32\n0.103\n0.157\n0.656\n0.513\n\n\nplot34\n0.118\n0.194\n0.610\n0.543\n\n\nplot35\n0.265\n0.261\n1.017\n0.311\n\n\nplot36\n0.025\n0.121\n0.207\n0.836\n\n\nplot37\n0.131\n0.165\n0.799\n0.426\n\n\nplot38\n0.747\n0.088\n8.439\n0.000\n\n\nplot39\n-0.012\n0.091\n-0.134\n0.894\n\n\nplot40\n0.082\n0.090\n0.915\n0.362\n\n\nplot41\n0.231\n0.121\n1.907\n0.059\n\n\nplot42\n0.423\n0.263\n1.607\n0.110\n\n\nplot43\n0.366\n0.142\n2.584\n0.011\n\n\nplot44\n0.296\n0.202\n1.462\n0.146\n\n\nplot45\n0.038\n0.116\n0.333\n0.740\n\n\nplot46\n0.248\n0.143\n1.733\n0.085\n\n\nplot47\n0.543\n0.143\n3.795\n0.000\n\n\nplot48\n0.107\n0.133\n0.801\n0.425\n\n\nplot49\n0.059\n0.092\n0.643\n0.521\n\n\nplot51\n0.577\n0.114\n5.039\n0.000\n\n\nplot52\n0.178\n0.256\n0.695\n0.488\n\n\nplot53\n0.152\n0.252\n0.604\n0.547\n\n\nplot54\n0.323\n0.113\n2.857\n0.005\n\n\nplot55\n0.204\n0.192\n1.060\n0.291\n\n\nplot56\n0.203\n0.267\n0.761\n0.448\n\n\nplot57\n0.172\n0.180\n0.957\n0.340\n\n\nplot58\n0.020\n0.114\n0.179\n0.858\n\n\nplot59\n0.221\n0.219\n1.010\n0.314\n\n\nplot60\n0.105\n0.214\n0.489\n0.625\n\n\nplot61\n0.136\n0.119\n1.135\n0.258\n\n\nplot62\n0.141\n0.224\n0.629\n0.531\n\n\nplot63\n0.234\n0.188\n1.242\n0.216\n\n\nplot64\n0.201\n0.299\n0.674\n0.502\n\n\nplot65\n0.083\n0.186\n0.449\n0.654\n\n\nplot66\n0.120\n0.130\n0.924\n0.357\n\n\nplot67\n0.084\n0.124\n0.683\n0.496\n\n\nplot68\n0.074\n0.090\n0.822\n0.412\n\n\nplot69\n0.201\n0.161\n1.248\n0.214\n\n\nplot70\n0.276\n0.113\n2.449\n0.016\n\n\nplot71\n0.126\n0.201\n0.630\n0.530\n\n\nplot72\n0.221\n0.174\n1.266\n0.208\n\n\nplot73\n0.044\n0.123\n0.356\n0.722\n\n\nplot74\n0.069\n0.130\n0.529\n0.598\n\n\nplot75\n0.043\n0.123\n0.347\n0.729\n\n\nplot76\n0.155\n0.252\n0.618\n0.538\n\n\nplot77\n0.207\n0.228\n0.905\n0.367\n\n\nplot78\n0.295\n0.205\n1.439\n0.152\n\n\nplot79\n0.145\n0.234\n0.619\n0.537\n\n\nplot80\n0.266\n0.252\n1.057\n0.292\n\n\nplot82\n0.239\n0.273\n0.876\n0.383\n\n\nplot83\n0.237\n0.261\n0.908\n0.365\n\n\nplot84\n0.260\n0.270\n0.965\n0.336\n\n\nplot85\n0.298\n0.297\n1.001\n0.319\n\n\nplot86\n0.120\n0.209\n0.576\n0.565\n\n\nplot87\n0.088\n0.124\n0.710\n0.479\n\n\nplot88\n0.296\n0.093\n3.199\n0.002\n\n\nplot89\n0.175\n0.125\n1.400\n0.164\n\n\nplot90\n0.018\n0.101\n0.175\n0.861\n\n\nplot91\n0.017\n0.102\n0.166\n0.868\n\n\nplot92\n0.078\n0.150\n0.522\n0.602\n\n\nplot93\n0.170\n0.154\n1.100\n0.273\n\n\nplot94\n-0.001\n0.095\n-0.012\n0.990\n\n\nplot95\n0.234\n0.141\n1.666\n0.098\n\n\nplot96\n0.083\n0.096\n0.858\n0.392\n\n\nplot97\n0.216\n0.150\n1.440\n0.152\n\n\nplot98\n0.180\n0.162\n1.108\n0.270\n\n\nplot99\n0.125\n0.159\n0.785\n0.434\n\n\nplot100\n0.019\n0.117\n0.165\n0.869\n\n\nplot101\n0.088\n0.119\n0.745\n0.457\n\n\nplot102\n0.010\n0.114\n0.084\n0.933\n\n\nplot103\n0.171\n0.174\n0.980\n0.329\n\n\nplot104\n0.157\n0.173\n0.909\n0.365\n\n\nplot105\n0.117\n0.171\n0.687\n0.493\n\n\nplot106\n0.102\n0.090\n1.140\n0.256\n\n\nplot107\n0.038\n0.112\n0.334\n0.739\n\n\nplot108\n0.086\n0.120\n0.716\n0.475\n\n\nplot110\n0.092\n0.153\n0.601\n0.549\n\n\nplot111\n0.074\n0.096\n0.767\n0.444\n\n\nplot112\n0.201\n0.105\n1.912\n0.058\n\n\nplot113\n0.090\n0.128\n0.704\n0.482\n\n\nplot114\n0.270\n0.089\n3.038\n0.003\n\n\nplot115\n0.008\n0.109\n0.075\n0.941\n\n\nplot116\n0.111\n0.121\n0.918\n0.360\n\n\nplot117\n0.057\n0.108\n0.530\n0.597\n\n\nplot118\n0.169\n0.167\n1.014\n0.312\n\n\nplot119\n0.074\n0.167\n0.444\n0.658\n\n\nplot120\n0.139\n0.182\n0.767\n0.444\n\n\nplot121\n0.191\n0.092\n2.073\n0.040\n\n\nplot122\n0.033\n0.132\n0.252\n0.802\n\n\nplot123\n0.072\n0.113\n0.634\n0.527\n\n\nplot124\n0.080\n0.142\n0.565\n0.573\n\n\nplot125\n0.381\n0.224\n1.695\n0.092\n\n\nplot126\n0.106\n0.126\n0.845\n0.400\n\n\nplot127\n0.172\n0.138\n1.245\n0.215\n\n\nplot128\n0.505\n0.104\n4.851\n0.000\n\n\nplot129\n0.104\n0.171\n0.607\n0.545\n\n\nplot130\n0.292\n0.196\n1.490\n0.138\n\n\nplot131\n0.195\n0.175\n1.118\n0.266\n\n\nplot132\n0.254\n0.094\n2.705\n0.008\n\n\nplot133\n0.258\n0.145\n1.783\n0.077\n\n\nplot134\n0.386\n0.269\n1.434\n0.154\n\n\nplot135\n0.103\n0.132\n0.777\n0.438\n\n\nplot136\n0.053\n0.095\n0.553\n0.581\n\n\nplot137\n0.136\n0.221\n0.615\n0.539\n\n\nplot138\n0.244\n0.198\n1.232\n0.220\n\n\nplot139\n0.095\n0.154\n0.616\n0.539\n\n\nplot140\n0.295\n0.094\n3.143\n0.002\n\n\nplot141\n0.158\n0.150\n1.058\n0.292\n\n\nplot142\n0.139\n0.184\n0.756\n0.451\n\n\nplot143\n0.105\n0.117\n0.896\n0.372\n\n\nplot145\n-0.020\n0.090\n-0.218\n0.828\n\n\nplot146\n0.342\n0.268\n1.276\n0.204\n\n\nplot147\n0.367\n0.275\n1.335\n0.184\n\n\nplot148\n0.214\n0.129\n1.664\n0.098\n\n\nplot149\n0.231\n0.131\n1.757\n0.081\n\n\nplot150\n0.245\n0.199\n1.233\n0.220\n\n\nplot151\n0.119\n0.196\n0.607\n0.545\n\n\nplot152\n0.340\n0.099\n3.426\n0.001\n\n\nplot153\n0.186\n0.128\n1.448\n0.150\n\n\nplot154\n0.065\n0.103\n0.636\n0.526\n\n\nyear2016\n-0.050\n0.072\n-0.693\n0.490\n\n\n\n Standard errors: OLS\n\n\n\n\n\n\n\n\n\n\n\n\n‚úÖ Your turn\nQ9. Compare the coefficient on vpd in the no-FE model vs the FE model. Did the estimate get larger, smaller, or change sign? What does that suggest about confounding from unobserved plot differences?\nResponse: Got larger. 0.126 -&gt; .227.\nWithout plot FE, the model was comparing across plots, which created bias: Plots with higher VPD tend to have other characteristics that reduce growth, which was biasing the VPD effect downward toward zero.\nQ10. Why include year fixed effects? Give one concrete example of a ‚Äúyear shock‚Äù that could bias estimates if not controlled (choose a different example than provided above).\nResponse: Maybe there was a pest outbreak a certain year that reduced growth\nQ11. Why include plot fixed effects? Give one concrete example of a ‚Äútime-invariant plot difference‚Äù that could bias estimates if not controlled (choose a different example than provided above).\nResponse: Some plots located near springs while others are farther away."
  },
  {
    "objectID": "course-materials/labs/week5.html",
    "href": "course-materials/labs/week5.html",
    "title": "Lab Week 5: Regression Discontinuity Designs (RDD)",
    "section": "",
    "text": "Regression Discontinuity Design is a natural experimental method used to estimate the causal effects of a treatment by exploiting a ‚Äúcutoff‚Äù or ‚Äúthreshold‚Äù in an assignment variable.\n\nThe running variable (\\(X\\)): A continuous measure used for assignment\nThe Cutoff (\\(c\\)): The threshold rule\nTreatment rule (\\(D\\)): What changes at the cutoff (policy / intervention)"
  },
  {
    "objectID": "course-materials/labs/week5.html#rdd-review",
    "href": "course-materials/labs/week5.html#rdd-review",
    "title": "Lab Week 5: Regression Discontinuity Designs (RDD)",
    "section": "",
    "text": "Regression Discontinuity Design is a natural experimental method used to estimate the causal effects of a treatment by exploiting a ‚Äúcutoff‚Äù or ‚Äúthreshold‚Äù in an assignment variable.\n\nThe running variable (\\(X\\)): A continuous measure used for assignment\nThe Cutoff (\\(c\\)): The threshold rule\nTreatment rule (\\(D\\)): What changes at the cutoff (policy / intervention)"
  },
  {
    "objectID": "course-materials/labs/week5.html#study-review-environmental-regulation-and-economic-performance-evidence-from-forest-conservation-in-mexico",
    "href": "course-materials/labs/week5.html#study-review-environmental-regulation-and-economic-performance-evidence-from-forest-conservation-in-mexico",
    "title": "Lab Week 5: Regression Discontinuity Designs (RDD)",
    "section": "Study Review: ‚ÄúEnvironmental Regulation and Economic Performance: Evidence from Forest Conservation in Mexico‚Äù",
    "text": "Study Review: ‚ÄúEnvironmental Regulation and Economic Performance: Evidence from Forest Conservation in Mexico‚Äù\nThis lab replicates the analysis from Neal (2024), which examines how effective government-protected forest areas are at preventing deforestation globally from 2000-2022. Using satellite data covering 7.4 billion forest observations and a regression discontinuity design, the study addresses a critical policy question:\n\nDoes legal protection actually prevent forests from being cleared?\n\nWe will use simulated data to approximate the study estimates. The results should not be interpreted literally as simulated data necessarily includes assumptions that omit complexity found in the original study data.\n\nThe Selection Bias Problem\nGovernments don‚Äôt randomly assign protection. They typically protect forests that are already less likely to be cleared‚Äîremote areas, steep terrain, or poor soil quality. Simply comparing deforestation rates inside versus outside protected areas would be misleading.\n\n\nThe RDD Solution\nBy comparing forests immediately on either side of protected area boundaries (within ~1.5km), the study isolates the causal effect of legal protection. Two forests 50 meters apart on opposite sides of a boundary make for a good comparison except for protection status.\n\n\nThe Findings\nProtected areas are only about 30% effective on average at preventing deforestation. While some nations like New Zealand (89% effective), Finland (77%), and Canada (63%) show high effectiveness, many countries with critical biodiversity have relatively low protection efficacy: Indonesia (11%), the Democratic Republic of Congo (4%), Bolivia (4%), and Venezuela (0%)."
  },
  {
    "objectID": "course-materials/labs/week5.html#study-replication",
    "href": "course-materials/labs/week5.html#study-replication",
    "title": "Lab Week 5: Regression Discontinuity Designs (RDD)",
    "section": "Study Replication",
    "text": "Study Replication\nUsing Simulated_Deforestation_Data6.csv, we will replicate Neal‚Äôs approach.\n\n0. Load packages + study data\n\nlibrary(tidyverse)\nlibrary(rdrobust)\nlibrary(here)\nlibrary(jtools)\nlibrary(janitor)\nlibrary(patchwork)\nlibrary(gt)\n\nRead in the simulated data\n\nsim_data &lt;- read_csv(here(\"week5\", \"Simulated_Deforestation_Data6.csv\"), show_col_types = FALSE)\n\n\n\n1. Create variable description table for key variables in the analysis\n\n\n\n\n\n\n\n\nVariable Descriptions (Treatment, Outcome, and RDD Covariates)\n\n\nReplication of Neal (2024) Forest Protection Study\n\n\nVariable Name\nDefinition\n\n\n\n\nprotected (Treatment)\nSite is inside a Protected Area (1 = inside, 0 = outside)\n\n\ndeforest (Outcome)\nAnnual rate of forest loss (0 to 1) detected via satellite imagery.\n\n\ndistance (Running Variable)\nThe shortest distance (in km) from the forest pixel to the nearest protected area boundary. Negative = Outside, Positive = Inside.\n\n\nslope\nSteepness of the terrain.\n\n\nroad_proximity\nDistance to the nearest road (km).\n\n\nwater_proximity\nDistance to the nearest water source (km).\n\n\nsoil_nutrition\nCategorical scale (0-2) of soil quality for agriculture.\n\n\ncountry\nThe nation where the forest plot is located.\n\n\nslope_cat\nBinary indicator for high slope vs. low slope for stratified analysis.\n\n\nroad_cat\nBinary indicator for proximity to roads (Near vs. Far).\n\n\nsoil_cat\nBinary indicator for high fertility vs. low fertility soil.\n\n\n\n\n\n\n\n\n\n2. Visualize the discontinuity using binned means (bin size = 1)\n\ndata_binned &lt;- sim_data %&gt;%\n  mutate(distance_bin = cut(distance, breaks = seq(-20, 20, by = 1), include.lowest = TRUE)) %&gt;%\n  group_by(distance_bin, protected) %&gt;%\n  summarize(\n    avg_distance = mean(distance), # Averaged binned distance\n    avg_deforest = mean(deforest),\n    .groups = \"drop\"\n  )\n\n\n\n3. Plot using binned data\n\nrdd_lm &lt;- ggplot(data_binned, aes(x = avg_distance, y = avg_deforest, color = as.factor(protected))) +\n  geom_point(alpha = 0.5) +\n  geom_smooth(method = \"lm\", aes(group = protected)) +\n  geom_vline(xintercept = 0, linetype = \"dashed\", color = \"black\") +\n  labs(title = \"Regression Discontinuity Plot (Binned by Distance)\",\n       x = \"Running Variable (Distance)\", y = \"Deforestation Rate\",\n       color = \"Protected Area\") +\n  theme_minimal()\n\n\nrdd_lm\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTipComprehension Check\n\n\n\nInterpret the plot above. What are the two lines representing and what do their trends reveal? Explain what the positive and negative values mean here (refer to the variable description for distance if you are unsure!).\nWhat would it mean if there were NO discontinuity at distance = 0? How would that change your interpretation of protected area effectiveness?\n\n\n\n\n4. Run RDD Analysis using OLS\n\nrdd_ols &lt;- lm(\n    deforest ~  # outcome\n    protected + # treatment effect \n    distance +  # running variable\n    protected*distance + # allows slope to vary\n    slope_cat + road_cat + water_cat + soil_cat, # &lt;&lt;&lt; CONTROLS\n              data = sim_data)\n\n# Display summary of regression results\nsumm(rdd_ols, digits = 3, \n     model.info = FALSE, model.fit = FALSE)\n\n\n\n\n\n\n\nEst.\n\n\nS.E.\n\n\nt val.\n\n\np\n\n\n\n\n\n\n(Intercept)\n\n\n0.194\n\n\n0.001\n\n\n266.385\n\n\n0.000\n\n\n\n\nprotected\n\n\n-0.100\n\n\n0.000\n\n\n-232.183\n\n\n0.000\n\n\n\n\ndistance\n\n\n-0.003\n\n\n0.000\n\n\n-129.687\n\n\n0.000\n\n\n\n\nslope_cat\n\n\n-0.013\n\n\n0.000\n\n\n-61.804\n\n\n0.000\n\n\n\n\nroad_cat\n\n\n-0.009\n\n\n0.000\n\n\n-38.902\n\n\n0.000\n\n\n\n\nwater_cat\n\n\n-0.006\n\n\n0.001\n\n\n-9.466\n\n\n0.000\n\n\n\n\nsoil_cat\n\n\n0.009\n\n\n0.000\n\n\n39.823\n\n\n0.000\n\n\n\n\nprotected:distance\n\n\n0.002\n\n\n0.000\n\n\n62.829\n\n\n0.000\n\n\n\n\n\n\n Standard errors: OLS\n\n\n\n\n\n\n\n\n\n\n\n\nTipComprehension Check\n\n\n\nBased on the regression results, what is the estimated causal effect of protected area status on deforestation? Does this match what you see in the plot? What does the interaction coefficient represent?\n\n\n\n\n4. Estimate & Visualize RDD using {rdrobust}\n\nRDD Robust Estimation Method (local polynomial regression):\nLocal polynomial regression is a method used to give more weight to observations near a specific point‚Äî in this case, the RDD threshold. Instead of using OLS, it fits separate non-linear regressions on either side of the cutoff using a subset of the data near the cutoff (i.e., bandwidth).\n\n\nInterpreting output:\nDefault estimation options used by the rdrobust() function:\nBandwidth Optimization (BW type: mserd): Bandwidth is optimized to balance accuracy & bias.\nBandwidth Estimate (BW est. (h) = 2.207): The estimated range around the cutoff used to subset the data to estimate the treatment effect.\nKernel (Triangular): Gives higher weight to data points close to the cutoff.\nVariance Estimation (VCE method: NN): Instead of assuming equal variance across all observations, the error estimates are adjusted to account for variability near the cutoff.\n\n\nTake a random sample (To adjust for memory-limit & speed)\n\nglobal_samp &lt;- sim_data %&gt;%\n     sample_n(size = nrow(sim_data) * 1.0) # &lt;&lt;&lt; e.g., .5 for 50%\n\n\n\n\n5. Estimate Global RDD\n\nglobal_rdd &lt;- rdrobust(\n  y = global_samp$deforest, # Outcome\n  x = global_samp$distance, # Running variable\n  covs = global_samp %&gt;% select(slope_cat, road_cat, water_cat, soil_cat), # Controls\n  c = 0, # Cutoff at 0 (protected area boundary)\n  kernel = \"triangular\"\n)\n\n# Print summary of results\nsummary(global_rdd)\n\nCovariate-adjusted Sharp RD estimates using local polynomial regression.\n\nNumber of Obs.               600000\nBW type                       mserd\nKernel                   Triangular\nVCE method                       NN\n\nNumber of Obs.               299889       300111\nEff. Number of Obs.           32971        33065\nOrder est. (p)                    1            1\nOrder bias  (q)                   2            2\nBW est. (h)                   2.207        2.207\nBW bias (b)                   4.851        4.851\nrho (h/b)                     0.455        0.455\nUnique Obs.                  299889       300111\n\n=====================================================================\n                   Point    Robust Inference\n                Estimate         z     P&gt;|z|      [ 95% C.I. ]       \n---------------------------------------------------------------------\n     RD Effect    -0.073   -58.640     0.000    [-0.075 , -0.070]    \n=====================================================================\n\n\n\n\n\n\n\n\nTipComprehension Check\n\n\n\nThe RD Effect here is -0.073, while in the previous regression table the protected coefficient was -0.100. Why are they similar but not exactly the same? Which one should we trust more?\nWhat advantages does this ‚Äòlocal polynomial regression‚Äô approach have over the simple linear regression we saw before?\n\n\n\n\n6. Visualize the RDD discontinuity using rdplot():\nThis plot presents the local polynomial regression curves fit on either side of the cutoff.\n\nrdd_poly &lt;- rdplot(\n  y = sim_data$deforest,\n  x = sim_data$distance,\n  c = 0,\n  binselect = \"es\",\n  title = \"Regression Discontinuity - Global Deforestation Rate\",\n  x.label = \"Distance to Protected Area Border (km)\",\n  y.label = \"Deforestation Rate\"\n)\n\n\n\n\n\n\n\nrdd_poly\n\nCall: rdplot\n\nNumber of Obs.               600000\nKernel                      Uniform\n\nNumber of Obs.               299889          300111\nEff. Number of Obs.          299889          300111\nOrder poly. fit (p)               4               4\nBW poly. fit (h)             20.000          20.000\nNumber of bins scale          1.000           1.000\n\n\nLet‚Äôs look at our results side by side.\n\nrdd_lm + rdd_poly$rdplot \n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTipComprehension Check\n\n\n\nCompare the two plots. What key differences do you notice between the left plot (binned linear) and the right plot (local polynomial)? Which one better captures the pattern in the data?\n\n\n\n\n7. Estimate separate RDD models by country using a loop function üîÑ\n\n\n\n\n\n\nTipWhat is an iterator or loop function?\n\n\n\nlapply() loops across the input levels for country and applies the function run_country_rdd\n\n\n\nrun_country_rdd &lt;- function(country_name) {\n  df_country &lt;- sim_data %&gt;% filter(country == country_name)\n\n    rdd_model &lt;- rdrobust(\n      y = df_country$deforest,\n      x = df_country$distance,\n      covs = df_country %&gt;% select(slope_cat, road_cat, water_cat, soil_cat),\n      c = 0, \n      p = 1,\n      kernel = \"triangular\"\n    )\n}\n\n# Apply the function to all countries\nrdd_6country &lt;- lapply(unique(sim_data$country), run_country_rdd)\n\n# Print summary for `DRC`\nsummary(rdd_6country[[3]]) \n\nCovariate-adjusted Sharp RD estimates using local polynomial regression.\n\nNumber of Obs.               100000\nBW type                       mserd\nKernel                   Triangular\nVCE method                       NN\n\nNumber of Obs.                50031        49969\nEff. Number of Obs.            6693         6733\nOrder est. (p)                    1            1\nOrder bias  (q)                   2            2\nBW est. (h)                   2.667        2.667\nBW bias (b)                   6.075        6.075\nrho (h/b)                     0.439        0.439\nUnique Obs.                   50031        49969\n\n=====================================================================\n                   Point    Robust Inference\n                Estimate         z     P&gt;|z|      [ 95% C.I. ]       \n---------------------------------------------------------------------\n     RD Effect     0.001     1.460     0.144    [-0.001 , 0.004]     \n=====================================================================\n\n\n\n\n8. Generate county level discontinuity plots\n\n# Create list to store plots\nplot_list &lt;- list()\n\n# Loop across 6 countries and plot\nfor (country_name in unique(sim_data$country)) {\n  df_country &lt;- sim_data %&gt;% filter(country == country_name)\n  \n p &lt;- rdplot(\n      y = df_country$deforest,\n      x = df_country$distance,\n      c = 0,\n      binselect = \"es\",\n      title = paste(country_name),\n      x.label=\"\", y.label = \"\")\n  \n p &lt;- p$rdplot +\n     labs(x=\" \",y=\"\")\n \n  plot_list[[country_name]] &lt;- p  # Store each plot in the list\n  }\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n9. Print combined RDD Plots by country\n\nfinal_plot &lt;- wrap_plots(plot_list) + \n  plot_layout(ncol = 2) +  \n  plot_annotation(\n    title = \"Deforestation Rate\",\n    caption = \"Source: Simulation Data\"\n  )\n\nfinal_plot\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTipComprehension Check\n\n\n\nHow do the results of these countries compare with the results from Neal 2024?"
  },
  {
    "objectID": "course-materials/labs/week5.html#load-packages-study-data",
    "href": "course-materials/labs/week5.html#load-packages-study-data",
    "title": "Lab Week 5: Regression Discontinuity Designs (RDD)",
    "section": "0. Load packages + study data",
    "text": "0. Load packages + study data\n\nlibrary(tidyverse)\nlibrary(rdrobust)\nlibrary(here)\nlibrary(jtools)\nlibrary(janitor)\nlibrary(patchwork)\nlibrary(gt)\n\nRead in the simulated data\n\nsim_data &lt;- read_csv(here(\"week4\", \"Simulated_Deforestation_Data5.csv\"), show_col_types = FALSE)\n\n\n1. Create variable description table for key variables in the analysis\n\n\n\n\n\n\n\n\nVariable Descriptions (Treatment, Outcome, and RDD Covariates)\n\n\nReplication of Neal (2024) Forest Protection Study\n\n\nVariable Name\nDefinition\n\n\n\n\nprotected (Treatment)\nSite is inside a Protected Area (1 = inside, 0 = outside)\n\n\ndeforest (Outcome)\nAnnual rate of forest loss (0 to 1) detected via satellite imagery.\n\n\ndistance (Running Variable)\nThe shortest distance (in km) from the forest pixel to the nearest protected area boundary. Negative = Outside, Positive = Inside.\n\n\nslope\nSteepness of the terrain.\n\n\nroad_proximity\nDistance to the nearest road (km).\n\n\nwater_proximity\nDistance to the nearest water source (km).\n\n\nsoil_nutrition\nCategorical scale (0-2) of soil quality for agriculture.\n\n\ncountry\nThe nation where the forest plot is located.\n\n\nslope_cat\nBinary indicator for high slope vs. low slope for stratified analysis.\n\n\nroad_cat\nBinary indicator for proximity to roads (Near vs. Far).\n\n\nsoil_cat\nBinary indicator for high fertility vs. low fertility soil."
  },
  {
    "objectID": "course-materials/labs/week5.html#visualize-the-discontinuity-using-binned-means-bin-size-1",
    "href": "course-materials/labs/week5.html#visualize-the-discontinuity-using-binned-means-bin-size-1",
    "title": "Lab Week 5: Regression Discontinuity Designs (RDD)",
    "section": "2. Visualize the discontinuity using binned means (bin size = 1)",
    "text": "2. Visualize the discontinuity using binned means (bin size = 1)\n\ndata_binned &lt;- sim_data %&gt;%\n  mutate(distance_bin = cut(distance, breaks = seq(-20, 20, by = 1), include.lowest = TRUE)) %&gt;%\n  group_by(distance_bin, protected) %&gt;%\n  summarize(\n    avg_distance = mean(distance), # Averaged binned distance\n    avg_deforest = mean(deforest),\n    .groups = \"drop\"\n  )\n\n\n3. Plot using binned data\n\nrdd_lm &lt;- ggplot(data_binned, aes(x = avg_distance, y = avg_deforest, color = as.factor(protected))) +\n  geom_point(alpha = 0.5) +\n  geom_smooth(method = \"lm\", aes(group = protected), se = TRUE) +\n  geom_vline(xintercept = 0, linetype = \"dashed\", color = \"black\") +\n  labs(title = \"Regression Discontinuity Plot (Binned by Distance)\",\n       x = \"Running Variable (Distance)\", y = \"Deforestation Rate\",\n       color = \"Protected Area\") +\n  theme_minimal()\n\n\nrdd_lm\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTipComprehension Check\n\n\n\nInterpret the plot above. What are the two lines representing and what do their trends reveal? Explain what the positive and negative values mean here (refer to the variable description for distance if you are unsure!).\nWhat would it mean if there were NO discontinuity at distance = 0? How would that change your interpretation of protected area effectiveness?\n\n\n\n\n4. Run RDD Analysis using OLS\n\nrdd_ols &lt;- lm(\n    deforest ~  # outcome\n    protected + # treatment effect \n    distance +  # running variable\n    protected*distance + # allows slope to vary\n    slope_cat + road_cat + water_cat + soil_cat, # &lt;&lt;&lt; CONTROLS\n              data = sim_data)\n\n# Display summary of regression results\nsumm(rdd_ols, digits = 3, \n     model.info = FALSE, model.fit = FALSE)\n\n\n\n\n\n\n\nEst.\n\n\nS.E.\n\n\nt val.\n\n\np\n\n\n\n\n\n\n(Intercept)\n\n\n0.298\n\n\n0.001\n\n\n475.575\n\n\n0.000\n\n\n\n\nprotected\n\n\n-0.094\n\n\n0.000\n\n\n-253.637\n\n\n0.000\n\n\n\n\ndistance\n\n\n-0.002\n\n\n0.000\n\n\n-101.864\n\n\n0.000\n\n\n\n\nslope_cat\n\n\n0.019\n\n\n0.000\n\n\n103.900\n\n\n0.000\n\n\n\n\nroad_cat\n\n\n-0.104\n\n\n0.000\n\n\n-526.436\n\n\n0.000\n\n\n\n\nwater_cat\n\n\n0.013\n\n\n0.001\n\n\n23.983\n\n\n0.000\n\n\n\n\nsoil_cat\n\n\n-0.018\n\n\n0.000\n\n\n-87.815\n\n\n0.000\n\n\n\n\nprotected:distance\n\n\n-0.002\n\n\n0.000\n\n\n-68.857\n\n\n0.000\n\n\n\n\n\n\n Standard errors: OLS\n\n\n\n\n\n\n\n\n\n\n\n\nTipComprehension Check\n\n\n\nBased on the regression results, what is the estimated causal effect of protected area status on deforestation? Does this match what you see in the plot?\n\n\n\n\n4. Estimate & Visualize RDD using {rdrobust}\n\nRDD Robust Estimation Method (local polynomial regression):\nLocal polynomial regression is a method used to give more weight to observations near a specific point‚Äî in this case, the RDD threshold. Instead of using OLS, it fits separate non-linear regressions on either side of the cutoff using a subset of the data near the cutoff (i.e., bandwidth).\n\n\nInterpreting output:\nDefault estimation options used by the rdrobust() function:\nBandwidth Optimization (BW type: mserd): Bandwidth is optimized to balance accuracy & bias.\nBandwidth Estimate (BW est. (h) = 5.729): The estimated range around the cutoff used to subset the data to estimate the treatment effect.\nKernel (Triangular): Gives higher weight to data points close to the cutoff.\nVariance Estimation (VCE method: NN): Instead of assuming equal variance across all observations, the error estimates are adjusted to account for variability near the cutoff.\n\n\nTake a random sample (To adjust for memory-limit & speed)\n\nglobal_samp &lt;- sim_data %&gt;%\n     sample_n(size = nrow(sim_data) * 1.0) # &lt;&lt;&lt; e.g., .5 for 50%\n\n\n\n\n5. Estimate Global RDD\n\nglobal_rdd &lt;- rdrobust(\n  y = global_samp$deforest, # Outcome\n  x = global_samp$distance, # Running variable\n  covs = global_samp %&gt;% select(slope_cat, road_cat, water_cat, soil_cat), # Controls\n  c = 0, # Cutoff at 0 (protected area boundary)\n  kernel = \"triangular\"\n)\n\n# Print summary of results\nsummary(global_rdd)\n\nCovariate-adjusted Sharp RD estimates using local polynomial regression.\n\nNumber of Obs.               600000\nBW type                       mserd\nKernel                   Triangular\nVCE method                       NN\n\nNumber of Obs.               299889       300111\nEff. Number of Obs.           85646        86071\nOrder est. (p)                    1            1\nOrder bias  (q)                   2            2\nBW est. (h)                   5.729        5.729\nBW bias (b)                   9.034        9.034\nrho (h/b)                     0.634        0.634\nUnique Obs.                  299889       300111\n\n=====================================================================\n                   Point    Robust Inference\n                Estimate         z     P&gt;|z|      [ 95% C.I. ]       \n---------------------------------------------------------------------\n     RD Effect    -0.092   -99.939     0.000    [-0.094 , -0.090]    \n=====================================================================\n\n\n\n\n\n\n\n\nTipComprehension Check\n\n\n\nThe RD Effect here is -0.092, while in the previous regression table the ‚Äòprotected‚Äô coefficient was -0.094. Why are they similar but not exactly the same? Which one should we trust more?\nWhat advantages does this ‚Äòlocal polynomial regression‚Äô approach have over the simple linear regression we saw before?\n\n\n\n\n6. Visualize the RDD discontinuity using rdplot():\nThis plot presents the local polynomial regression curves fit on either side of the cutoff.\n\nrdd_poly &lt;- rdplot(\n  y = sim_data$deforest,\n  x = sim_data$distance,\n  c = 0,\n  binselect = \"es\",\n  title = \"Regression Discontinuity - Global Deforestation Rate\",\n  x.label = \"Distance to Protected Area Border (km)\",\n  y.label = \"Deforestation Rate\"\n)\n\n\n\n\n\n\n\nrdd_poly\n\nCall: rdplot\n\nNumber of Obs.               600000\nKernel                      Uniform\n\nNumber of Obs.               299889          300111\nEff. Number of Obs.          299889          300111\nOrder poly. fit (p)               4               4\nBW poly. fit (h)             20.000          20.000\nNumber of bins scale          1.000           1.000\n\n\nLet‚Äôs look at our results side by side.\n\nrdd_lm + rdd_poly$rdplot \n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTipComprehension Check\n\n\n\nCompare the two plots. What key differences do you notice between the left plot (binned linear) and the right plot (local polynomial)? Which one better captures the pattern in the data?\n\n\n\n\n7. Estimate separate RDD models by country using a loop function üîÑ\n\n\n\n\n\n\nTipWhat is an iterator or loop function?\n\n\n\nlapply() loops across the input levels for country and applies the function run_country_rdd\n\n\n\nrun_country_rdd &lt;- function(country_name) {\n  df_country &lt;- sim_data %&gt;% filter(country == country_name)\n\n    rdd_model &lt;- rdrobust(\n      y = df_country$deforest,\n      x = df_country$distance,\n      covs = df_country %&gt;% select(slope_cat, road_cat, water_cat, soil_cat),\n      c = 0, \n      p = 1,\n      kernel = \"triangular\"\n    )\n}\n\n# Apply the function to all countries\nrdd_6country &lt;- lapply(unique(sim_data$country), run_country_rdd)\n\n# Print summary for `DRC`\nsummary(rdd_6country[[3]]) \n\nCovariate-adjusted Sharp RD estimates using local polynomial regression.\n\nNumber of Obs.               100000\nBW type                       mserd\nKernel                   Triangular\nVCE method                       NN\n\nNumber of Obs.                50031        49969\nEff. Number of Obs.           15992        16021\nOrder est. (p)                    1            1\nOrder bias  (q)                   2            2\nBW est. (h)                   6.369        6.369\nBW bias (b)                  11.882       11.882\nrho (h/b)                     0.536        0.536\nUnique Obs.                   50031        49969\n\n=====================================================================\n                   Point    Robust Inference\n                Estimate         z     P&gt;|z|      [ 95% C.I. ]       \n---------------------------------------------------------------------\n     RD Effect     0.001     1.049     0.294    [-0.001 , 0.005]     \n=====================================================================\n\n\n\n\n8. Generate county level discontinuity plots\n\n# Create list to store plots\nplot_list &lt;- list()\n\n# Loop across 6 countries and plot\nfor (country_name in unique(sim_data$country)) {\n  df_country &lt;- sim_data %&gt;% filter(country == country_name)\n  \n p &lt;- rdplot(\n      y = df_country$deforest,\n      x = df_country$distance,\n      c = 0,\n      binselect = \"es\",\n      title = paste(country_name),\n      x.label=\"\", y.label = \"\")\n  \n p &lt;- p$rdplot +\n     labs(x=\" \",y=\"\")\n \n  plot_list[[country_name]] &lt;- p  # Store each plot in the list\n  }\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n9. Print combined RDD Plots by country\n\nfinal_plot &lt;- wrap_plots(plot_list) + \n  plot_layout(ncol = 2) +  \n  plot_annotation(\n    title = \"Deforestation Rate\",\n    caption = \"Source: Simulation Data\"\n  )\n\nfinal_plot\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTipComprehension Check\n\n\n\nHow do the results of these countries compare with the results from Neal 2024?"
  },
  {
    "objectID": "course-materials/labs/week5.html#insert-question",
    "href": "course-materials/labs/week5.html#insert-question",
    "title": "Lab Week 5: Regression Discontinuity Designs (RDD)",
    "section": "INSERT QUESTION",
    "text": "INSERT QUESTION\n\n4. Run RDD Analysis using OLS\n\nrdd_ols &lt;- lm(\n    deforest ~  # outcome\n    protected + # treatment effect \n    distance +  # running variable\n    protected*distance + # allows slope to vary\n    slope_cat + road_cat + water_cat + soil_cat, # &lt;&lt;&lt; CONTROLS\n              data = sim_data)\n\n# Display summary of regression results\nsumm(rdd_ols, digits = 3, \n     model.info = FALSE, model.fit = FALSE)\n\n\n\n\n\n\n\nEst.\n\n\nS.E.\n\n\nt val.\n\n\np\n\n\n\n\n\n\n(Intercept)\n\n\n0.298\n\n\n0.001\n\n\n475.575\n\n\n0.000\n\n\n\n\nprotected\n\n\n-0.094\n\n\n0.000\n\n\n-253.637\n\n\n0.000\n\n\n\n\ndistance\n\n\n-0.002\n\n\n0.000\n\n\n-101.864\n\n\n0.000\n\n\n\n\nslope_cat\n\n\n0.019\n\n\n0.000\n\n\n103.900\n\n\n0.000\n\n\n\n\nroad_cat\n\n\n-0.104\n\n\n0.000\n\n\n-526.436\n\n\n0.000\n\n\n\n\nwater_cat\n\n\n0.013\n\n\n0.001\n\n\n23.983\n\n\n0.000\n\n\n\n\nsoil_cat\n\n\n-0.018\n\n\n0.000\n\n\n-87.815\n\n\n0.000\n\n\n\n\nprotected:distance\n\n\n-0.002\n\n\n0.000\n\n\n-68.857\n\n\n0.000\n\n\n\n\n\n\n Standard errors: OLS\n\n\n\n\n\n\nINSERT QUESTION\n\n\n4. Estimate & Visualize RDD using {rdrobust}\n\nRDD Robust Estimation Method (local polynomial regression):\nLocal polynomial regression is a method used to give more weight to observations near a specific point‚Äî in this case, the RDD threshold. Instead of using OLS, it fits separate non-linear regressions on either side of the cutoff using a subset of the data near the cutoff (i.e., bandwidth).\n\n\nInterpreting output:\nDefault estimation options used by the rdrobust() function:\nBandwidth Optimization (BW type: mserd): Bandwidth is optimized to balance accuracy & bias.\nBandwidth Estimate (BW est. (h) = 5.729): The estimated range around the cutoff used to subset the data to estimate the treatment effect.\nKernel (Triangular): Gives higher weight to data points close to the cutoff.\nVariance Estimation (VCE method: NN): Instead of assuming equal variance across all observations, the error estimates are adjusted to account for variability near the cutoff.\n\n\nTake a random sample (To adjust for memory-limit & speed)\n\nglobal_samp &lt;- sim_data %&gt;%\n     sample_n(size = nrow(sim_data) * 1.0) # &lt;&lt;&lt; e.g., .5 for 50%\n\n\n\n\n5. Estimate Global RDD\n\nglobal_rdd &lt;- rdrobust(\n  y = global_samp$deforest, # Outcome\n  x = global_samp$distance, # Running variable\n  covs = global_samp %&gt;% select(slope_cat, road_cat, water_cat, soil_cat), # Controls\n  c = 0, # Cutoff at 0 (protected area boundary)\n  kernel = \"triangular\"\n)\n\n# Print summary of results\nsummary(global_rdd)\n\nCovariate-adjusted Sharp RD estimates using local polynomial regression.\n\nNumber of Obs.               600000\nBW type                       mserd\nKernel                   Triangular\nVCE method                       NN\n\nNumber of Obs.               299889       300111\nEff. Number of Obs.           85646        86071\nOrder est. (p)                    1            1\nOrder bias  (q)                   2            2\nBW est. (h)                   5.729        5.729\nBW bias (b)                   9.034        9.034\nrho (h/b)                     0.634        0.634\nUnique Obs.                  299889       300111\n\n=====================================================================\n                   Point    Robust Inference\n                Estimate         z     P&gt;|z|      [ 95% C.I. ]       \n---------------------------------------------------------------------\n     RD Effect    -0.092   -99.939     0.000    [-0.094 , -0.090]    \n=====================================================================\n\n\n\n\n6. Visualize the RDD discontinuity using rdplot():\nThis plot presents the local polynomial regression curves fit on either side of the cutoff.\n\nrdplot(\n  y = sim_data$deforest,\n  x = sim_data$distance,\n  c = 0,\n  binselect = \"es\",\n  title = \"Regression Discontinuity - Global Deforestation Rate\",\n  x.label = \"Distance to Protected Area Border (km)\",\n  y.label = \"Deforestation Rate\"\n)\n\n\n\n\n\n\n\n\n\n\n7. Estimate separate RDD models by country using a loop function üîÑ\n\n\n\n\n\n\nTipWhat is an iterator or loop function?\n\n\n\nlapply() loops across the input levels for country and applies the function run_country_rdd\n\n\n\nrun_country_rdd &lt;- function(country_name) {\n  df_country &lt;- sim_data %&gt;% filter(country == country_name)\n\n    rdd_model &lt;- rdrobust(\n      y = df_country$deforest,\n      x = df_country$distance,\n      covs = df_country %&gt;% select(slope_cat, road_cat, water_cat, soil_cat),\n      c = 0, \n      p = 1,\n      kernel = \"triangular\"\n    )\n}\n\n# Apply the function to all countries\nrdd_6country &lt;- lapply(unique(sim_data$country), run_country_rdd)\n\n# Print summary for `DRC`\nsummary(rdd_6country[[3]]) \n\nCovariate-adjusted Sharp RD estimates using local polynomial regression.\n\nNumber of Obs.               100000\nBW type                       mserd\nKernel                   Triangular\nVCE method                       NN\n\nNumber of Obs.                50031        49969\nEff. Number of Obs.           15992        16021\nOrder est. (p)                    1            1\nOrder bias  (q)                   2            2\nBW est. (h)                   6.369        6.369\nBW bias (b)                  11.882       11.882\nrho (h/b)                     0.536        0.536\nUnique Obs.                   50031        49969\n\n=====================================================================\n                   Point    Robust Inference\n                Estimate         z     P&gt;|z|      [ 95% C.I. ]       \n---------------------------------------------------------------------\n     RD Effect     0.001     1.049     0.294    [-0.001 , 0.005]     \n=====================================================================\n\n\n\n\n8. Generate county level discontinuity plots\n\n# Create list to store plots\nplot_list &lt;- list()\n\n# Loop across 6 countries and plot\nfor (country_name in unique(sim_data$country)) {\n  df_country &lt;- sim_data %&gt;% filter(country == country_name)\n  \n p &lt;- rdplot(\n      y = df_country$deforest,\n      x = df_country$distance,\n      c = 0,\n      binselect = \"es\",\n      title = paste(country_name),\n      x.label=\"\", y.label = \"\")\n  \n p &lt;- p$rdplot +\n     labs(x=\" \",y=\"\")\n \n  plot_list[[country_name]] &lt;- p  # Store each plot in the list\n  }\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n9. Print combined RDD Plots\n\nfinal_plot &lt;- wrap_plots(plot_list) + \n  plot_layout(ncol = 2) +  \n  plot_annotation(\n    title = \"Deforestation Rate\",\n    caption = \"Source: Simulation Data\"\n  )\n\nfinal_plot"
  },
  {
    "objectID": "course-materials/labs/week5_notes.html",
    "href": "course-materials/labs/week5_notes.html",
    "title": "Lab Week 5: Regression Discontinuity Designs (RDD)",
    "section": "",
    "text": "Regression Discontinuity Design is a experimental method used to estimate the causal effects of a treatment by exploiting a ‚Äúcutoff‚Äù or ‚Äúthreshold‚Äù in an assignment variable.\n\nThe assignmnet variable (\\(X\\)): A continuous measure used for assignment\nThe Cutoff (\\(c\\)): The threshold rule\nTreatment rule (\\(D\\)): What changes at the cutoff (policy / intervention)\nThe causal effect is identified by the ‚Äújump‚Äù or ‚Äúdiscontinuity‚Äù in the outcome variable at the cutoff point."
  },
  {
    "objectID": "course-materials/labs/week5_notes.html#rdd-review",
    "href": "course-materials/labs/week5_notes.html#rdd-review",
    "title": "Lab Week 5: Regression Discontinuity Designs (RDD)",
    "section": "",
    "text": "Regression Discontinuity Design is a experimental method used to estimate the causal effects of a treatment by exploiting a ‚Äúcutoff‚Äù or ‚Äúthreshold‚Äù in an assignment variable.\n\nThe assignmnet variable (\\(X\\)): A continuous measure used for assignment\nThe Cutoff (\\(c\\)): The threshold rule\nTreatment rule (\\(D\\)): What changes at the cutoff (policy / intervention)\nThe causal effect is identified by the ‚Äújump‚Äù or ‚Äúdiscontinuity‚Äù in the outcome variable at the cutoff point."
  },
  {
    "objectID": "course-materials/labs/week5_notes.html#study-review-environmental-regulation-and-economic-performance-evidence-from-forest-conservation-in-mexico",
    "href": "course-materials/labs/week5_notes.html#study-review-environmental-regulation-and-economic-performance-evidence-from-forest-conservation-in-mexico",
    "title": "Lab Week 5: Regression Discontinuity Designs (RDD)",
    "section": "Study Review: ‚ÄúEnvironmental Regulation and Economic Performance: Evidence from Forest Conservation in Mexico‚Äù",
    "text": "Study Review: ‚ÄúEnvironmental Regulation and Economic Performance: Evidence from Forest Conservation in Mexico‚Äù\nThis lab uses open-access replication data from:\n\n\n\nThis lab replicates the analysis from Neal (2024), which examines how effective government-protected forest areas are at preventing deforestation globally from 2000-2022. Using satellite data covering 7.4 billion forest observations and a regression discontinuity design, the study addresses a critical policy question:\n\nDoes legal protection actually prevent forests from being cleared?\n\n\nThe Selection Bias Problem\nGovernments don‚Äôt randomly assign protection. They typically protect forests that are already less likely to be cleared ‚Äî remote areas, steep terrain, or poor soil quality. Simply comparing deforestation rates inside versus outside protected areas would be misleading.\n\n\nThe RDD Solution\nBy comparing forests immediately on either side of protected area boundaries (within ~1.5km), the study isolates the causal effect of legal protection. Two forests 50 meters apart on opposite sides of a boundary are virtually identical except for protection status.\n\n\nThe Findings\nProtected areas are only about 30% effective on average at preventing deforestation. While some nations like New Zealand (89% effective), Finland (77%), and Canada (63%) show high effectiveness, many countries with critical biodiversity have essentially ineffective protection: Indonesia (11%), the Democratic Republic of Congo (4%), Bolivia (4%), and Venezuela (0%)."
  },
  {
    "objectID": "course-materials/labs/week5_notes.html#study-replication",
    "href": "course-materials/labs/week5_notes.html#study-replication",
    "title": "Lab Week 5: Regression Discontinuity Designs (RDD)",
    "section": "Study Replication",
    "text": "Study Replication\nUsing Simulated_Deforestation_Data6.csv, we will replicate Neal‚Äôs approach.\n\n0. Load packages + study data\n\nlibrary(tidyverse)\nlibrary(rdrobust)\nlibrary(here)\nlibrary(jtools)\nlibrary(janitor)\nlibrary(patchwork)\nlibrary(gt)\n\nRead in the simulated data\n\nsim_data &lt;- read_csv(here(\"week4\", \"Simulated_Deforestation_Data6.csv\"), show_col_types = FALSE)\n\n\n\n1. Create variable description table for key variables in the analysis\n\n\n\n\n\n\n\n\nVariable Descriptions (Treatment, Outcome, and RDD Covariates)\n\n\nReplication of Neal (2024) Forest Protection Study\n\n\nVariable Name\nDefinition\n\n\n\n\nprotected (Treatment)\nSite is inside a Protected Area (1 = inside, 0 = outside)\n\n\ndeforest (Outcome)\nAnnual rate of forest loss (0 to 1) detected via satellite imagery.\n\n\ndistance (Running Variable)\nThe shortest distance (in km) from the forest pixel to the nearest protected area boundary. Negative = Outside, Positive = Inside.\n\n\nslope\nSteepness of the terrain.\n\n\nroad_proximity\nDistance to the nearest road (km).\n\n\nwater_proximity\nDistance to the nearest water source (km).\n\n\nsoil_nutrition\nCategorical scale (0-2) of soil quality for agriculture.\n\n\ncountry\nThe nation where the forest plot is located.\n\n\nslope_cat\nBinary indicator for high slope vs. low slope for stratified analysis.\n\n\nroad_cat\nBinary indicator for proximity to roads (Near vs. Far).\n\n\nsoil_cat\nBinary indicator for high fertility vs. low fertility soil.\n\n\n\n\n\n\n\n\n\n2. Visualize the discontinuity using binned means (bin size = 1)\n\ndata_binned &lt;- sim_data %&gt;%\n  mutate(distance_bin = cut(distance, breaks = seq(-20, 20, by = 1), include.lowest = TRUE)) %&gt;% # adds a bin column specifying which bin an observation is in, cut divides range of distance into intervals and specifies which bin each distance is in , include.lowest = TRUE makes sure -20 is included\n  group_by(distance_bin, protected) %&gt;%\n  summarize(\n    avg_distance = mean(distance), # Averaged binned distance\n    avg_deforest = mean(deforest),\n    .groups = \"drop\"\n  )\n\n\n\n3. Plot using binned data\n\nrdd_lm &lt;- ggplot(data_binned, aes(x = avg_distance, y = avg_deforest, color = as.factor(protected))) +\n  geom_point(alpha = 0.5) +\n  geom_smooth(method = \"lm\", aes(group = protected)) + # fit sep linear lines for each unique value of protected var\n  geom_vline(xintercept = 0, linetype = \"dashed\", color = \"black\") + # all line at our cutoff\n  labs(title = \"Regression Discontinuity Plot (Binned by Distance)\",\n       x = \"Running Variable (Distance)\", y = \"Deforestation Rate\",\n       color = \"Protected Area\") +\n  theme_minimal()\n\n\nrdd_lm\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTipComprehension Check\n\n\n\nInterpret the plot above. What are the two lines representing and what do their trends reveal? Explain what the positive and negative values mean here (refer to the variable description for distance if you are unsure!).\nWhat would it mean if there were NO discontinuity at distance = 0? How would that change your interpretation of protected area effectiveness?\n\n\nWhat the Two Lines Represent:\nRed line (Protected Area = 0): Areas outside the protected area (negative distances from the boundary) Teal line (Protected Area = 1): Areas inside the protected area (positive distances from the boundary)\nDistance Variable Interpretation:\n\nNegative values (left side): Distance outside/away from the protected area boundary\nZero (dashed line): The exact boundary of the protected area\nPositive values (right side): Distance inside the protected area\n\nWhat the Trends Reveal: - Both lines show deforestation rates declining as you move from left to right, but the key finding is:\nThere‚Äôs a clear JUMP/DISCONTINUITY at distance = 0\nJust outside the boundary (~0.25 deforestation rate) Just inside the boundary (~0.14 deforestation rate) This represents about a 44% reduction in deforestation right at the boundary (.25-.14/ .25)\nBoth sides show gradual declines, suggesting some underlying spatial trend, but the sharp drop AT the boundary is the critical evidence that protected area status itself reduces deforestation.\n\n\n4. Run RDD Analysis using OLS\n\nrdd_ols &lt;- lm(\n    deforest ~  # outcome\n    protected + # treatment effect \n    distance +  # running variable\n    protected*distance + # allows slope to vary\n    slope_cat + road_cat + water_cat + soil_cat, # &lt;&lt;&lt; CONTROLS\n              data = sim_data)\n\n# Display summary of regression results\nsumm(rdd_ols, digits = 3, \n     model.info = FALSE, model.fit = FALSE)\n\n\n\n\n\n\n\nEst.\n\n\nS.E.\n\n\nt val.\n\n\np\n\n\n\n\n\n\n(Intercept)\n\n\n0.194\n\n\n0.001\n\n\n266.385\n\n\n0.000\n\n\n\n\nprotected\n\n\n-0.100\n\n\n0.000\n\n\n-232.183\n\n\n0.000\n\n\n\n\ndistance\n\n\n-0.003\n\n\n0.000\n\n\n-129.687\n\n\n0.000\n\n\n\n\nslope_cat\n\n\n-0.013\n\n\n0.000\n\n\n-61.804\n\n\n0.000\n\n\n\n\nroad_cat\n\n\n-0.009\n\n\n0.000\n\n\n-38.902\n\n\n0.000\n\n\n\n\nwater_cat\n\n\n-0.006\n\n\n0.001\n\n\n-9.466\n\n\n0.000\n\n\n\n\nsoil_cat\n\n\n0.009\n\n\n0.000\n\n\n39.823\n\n\n0.000\n\n\n\n\nprotected:distance\n\n\n0.002\n\n\n0.000\n\n\n62.829\n\n\n0.000\n\n\n\n\n\n\n Standard errors: OLS\n\n\n\n\n\n\n\n\n\n\n\n\nTipComprehension Check\n\n\n\nBased on the regression results, what is the estimated causal effect of protected area status on deforestation? Does this match what you see in the plot? What does the interaction coefficient represent?\n\n\nBased on the regression results, the estimated causal effect of protected area status on deforestation is -0.094.\nWhat This Means: The protected coefficient = -0.094 represents the discontinuity at the boundary (distance = 0). It means:\nProtected areas reduce deforestation by 0.094 units (or 9.4 percentage points). This is highly statistically significant (p &lt; 0.000).\nDoes It Match the Plot? Yes, roughly! The regression controls for other variables (slope, roads, water, soil).\n\n\n4. Estimate & Visualize RDD using {rdrobust}\n\nRDD Robust Estimation Method (local polynomial regression):\nLocal polynomial regression is a method used to give more weight to observations near a specific point‚Äî in this case, the RDD threshold. Instead of using OLS, it fits separate non-linear regressions on either side of the cutoff using a subset of the data near the cutoff (i.e., bandwidth).\n\n\nInterpreting output:\nDefault estimation options used by the rdrobust() function:\n\nBandwidth Optimization (mserd) mserd = ‚ÄúMean Squared Error Robust Optimal Bandwidth‚Äù\n\nWhat it means: An algorithm that automatically picks the ‚Äúsweet spot‚Äù bandwidth for you. The trade-off:\n\nToo narrow (e.g., h = 1): Only uses observations very close to cutoff ‚Üí low bias but HIGH variance (noisy estimate)\nToo wide (e.g., h = 50): Uses lots of data ‚Üí low variance but HIGH bias (if relationship is curved, distant points distort the estimate)\n\nIt calculates the bandwidth that minimizes the combination of bias and variance Balances accuracy (getting the true effect) vs.¬†precision (stable estimate)\n\nBandwidth Estimate (h = 5.729) What it means: Only observations within distance between -5.729 and +5.729 are used to estimate the treatment effect. In your case:\n\nOut of 600,000 total observations, only those with distance values in [-5.729, +5.729] are actually used This focuses the analysis on the ‚Äúlocal‚Äù area around the boundary\nWhy 5.729 specifically? The optimization algorithm determined this gives us the best bias-variance trade-off for our particular data.\n\nKernel (Triangular)\n\nWhat it means: Not all observations within the bandwidth are weighted equally - closer points get more influence.\nWeight 1.0 | /\n| /\n| /\n0.0 |/______ -5.729 0 +5.729 Distance\n\nVariance Estimation (NN = Nearest Neighbor)\n\nWhat it means: The method for calculating standard errors that accounts for how variance might differ near vs.¬†far from the cutoff.\nThe problem: Observations very close to the cutoff might be more similar to each other (less variable). Standard OLS assumes equal variance everywhere (homoscedasticity)\nNN method:\nFor each observation, looks at its nearest neighbors to estimate local variance Adjusts standard errors based on how much variability exists in that local neighborhood Makes your p-values and confidence intervals more accurate\nWhy it matters: If variance is higher on one side of the cutoff, ignoring this would give you misleading significance tests.\n\n\nTake a random sample (To adjust for memory-limit & speed)\n\nglobal_samp &lt;- sim_data %&gt;%\n     sample_n(size = nrow(sim_data) * 1.0) # &lt;&lt;&lt; e.g., .5 for 50%\n\n\n\n\n5. Estimate Global RDD\n\nglobal_rdd &lt;- rdrobust(\n  y = global_samp$deforest, # Outcome\n  x = global_samp$distance, # Running variable\n  covs = global_samp %&gt;% select(slope_cat, road_cat, water_cat, soil_cat), # Controls\n  c = 0, # Cutoff at 0 (protected area boundary)\n  kernel = \"triangular\"\n)\n\n# Print summary of results\nsummary(global_rdd)\n\nCovariate-adjusted Sharp RD estimates using local polynomial regression.\n\nNumber of Obs.               600000\nBW type                       mserd\nKernel                   Triangular\nVCE method                       NN\n\nNumber of Obs.               299889       300111\nEff. Number of Obs.           32971        33065\nOrder est. (p)                    1            1\nOrder bias  (q)                   2            2\nBW est. (h)                   2.207        2.207\nBW bias (b)                   4.851        4.851\nrho (h/b)                     0.455        0.455\nUnique Obs.                  299889       300111\n\n=====================================================================\n                   Point    Robust Inference\n                Estimate         z     P&gt;|z|      [ 95% C.I. ]       \n---------------------------------------------------------------------\n     RD Effect    -0.073   -58.640     0.000    [-0.075 , -0.070]    \n=====================================================================\n\n\nInterp: - 2. BW bias (b) = 9.034 -&gt; This is a wider bandwidth used specifically to estimate the bias in your main estimate.\n\nrho (h/b) : ratio of the two bandwidths\n\n\n\n\n\n\n\nTipComprehension Check\n\n\n\nThe RD Effect here is -0.092, while in the previous regression table the protected coefficient was -0.094. Why are they similar but not exactly the same? Which one should we trust more?\nWhat advantages does this ‚Äòlocal polynomial regression‚Äô approach have over the simple linear regression we saw before?\n\n\nWhy Are They Similar But Not Exactly the Same? -0.094 (linear regression) vs -0.092 (local polynomial) - they differ by only 0.002! Four key reasons:\n\nDifferent data used\n\nLinear: ALL 600,000 observations Local polynomial: Only observations within distance ¬±5.729 (maybe 50,000-100,000 obs?)\n\nDifferent weighting\n\nLinear: All observations weighted equally Local polynomial: Triangular kernel gives more weight to observations closer to cutoff\n\nDifferent flexibility\n\nLinear: Forces straight lines across entire range Local polynomial: Allows curvature within the bandwidth\n\nBias correction\n\nLinear: Standard OLS Local polynomial: Includes bias correction procedures using the wider bandwidth (b = 9.034)\nWhich Should We Trust More?\nGenerally, trust the local polynomial (-0.092) more for RD designs, here‚Äôs why: Local Polynomial Advantages:\nRobustness to misspecification\n\nIf the relationship is curved far from the boundary, using all 600,000 observations could bias the linear estimate\n\nFocuses on the identifying assumption:\n\nRD relies on observations NEAR the cutoff being comparable (quasi-random assignment) Why should distance = -80 influence our estimate of what happens at distance = 0?\n\nLess dependent on functional form:\n\nLinear regression assumes straight lines are correct everywhere Local polynomial only assumes polynomials work well in a narrow window\n\nModern best practices:\n\nIncludes bias correction, robust variance estimation, optimal bandwidth selection These refinements improve accuracy and inference\n\n\n\n6. Visualize the RDD discontinuity using rdplot():\nThis plot presents the local polynomial regression curves fit on either side of the cutoff.\n\nrdd_poly &lt;- rdplot(\n  y = sim_data$deforest,\n  x = sim_data$distance,\n  c = 0,\n  binselect = \"es\",\n  title = \"Regression Discontinuity - Global Deforestation Rate\",\n  x.label = \"Distance to Protected Area Border (km)\",\n  y.label = \"Deforestation Rate\"\n)\n\n\n\n\n\n\n\nrdd_poly\n\nCall: rdplot\n\nNumber of Obs.               600000\nKernel                      Uniform\n\nNumber of Obs.               299889          300111\nEff. Number of Obs.          299889          300111\nOrder poly. fit (p)               4               4\nBW poly. fit (h)             20.000          20.000\nNumber of bins scale          1.000           1.000\n\n\nLet‚Äôs look at our results side by side.\n\nrdd_lm + rdd_poly$rdplot \n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTipComprehension Check\n\n\n\nCompare the two plots. What key differences do you notice between the left plot (binned linear) and the right plot (local polynomial)? Which one better captures the pattern in the data?\n\n\n\n\n7. Estimate separate RDD models by country using a loop function üîÑ\n\n\n\n\n\n\nTipWhat is an iterator or loop function?\n\n\n\nlapply() loops across the input levels for country and applies the function run_country_rdd\n\n\n\nrun_country_rdd &lt;- function(country_name) {\n  df_country &lt;- sim_data %&gt;% filter(country == country_name)\n\n    rdd_model &lt;- rdrobust(\n      y = df_country$deforest,\n      x = df_country$distance,\n      covs = df_country %&gt;% select(slope_cat, road_cat, water_cat, soil_cat),\n      c = 0, \n      p = 1,\n      kernel = \"triangular\"\n    )\n}\n\n# Apply the function to all countries\nrdd_6country &lt;- lapply(unique(sim_data$country), run_country_rdd)\n\n# Print summary for `DRC`\nsummary(rdd_6country[[3]]) \n\nCovariate-adjusted Sharp RD estimates using local polynomial regression.\n\nNumber of Obs.               100000\nBW type                       mserd\nKernel                   Triangular\nVCE method                       NN\n\nNumber of Obs.                50031        49969\nEff. Number of Obs.            6693         6733\nOrder est. (p)                    1            1\nOrder bias  (q)                   2            2\nBW est. (h)                   2.667        2.667\nBW bias (b)                   6.075        6.075\nrho (h/b)                     0.439        0.439\nUnique Obs.                   50031        49969\n\n=====================================================================\n                   Point    Robust Inference\n                Estimate         z     P&gt;|z|      [ 95% C.I. ]       \n---------------------------------------------------------------------\n     RD Effect     0.001     1.460     0.144    [-0.001 , 0.004]     \n=====================================================================\n\n\n\n\n8. Generate county level discontinuity plots\n\n# Create list to store plots\nplot_list &lt;- list()\n\n# Loop across 6 countries and plot\nfor (country_name in unique(sim_data$country)) {\n  df_country &lt;- sim_data %&gt;% filter(country == country_name)\n  \n p &lt;- rdplot(\n      y = df_country$deforest,\n      x = df_country$distance,\n      c = 0,\n      binselect = \"es\",\n      title = paste(country_name),\n      x.label=\"\", y.label = \"\")\n  \n p &lt;- p$rdplot +\n     labs(x=\" \",y=\"\")\n \n  plot_list[[country_name]] &lt;- p  # Store each plot in the list\n  }\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n9. Print combined RDD Plots by country\n\nfinal_plot &lt;- wrap_plots(plot_list) + # wrap plots from patchwork \n  plot_layout(ncol = 2) +  \n  plot_annotation(\n    title = \"Deforestation Rate\",\n    caption = \"Source: Simulation Data\"\n  )\n\nfinal_plot\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTipComprehension Check\n\n\n\nHow do the results of these countries compare with the results from Neal 2024?"
  }
]