[
  {
    "objectID": "resources.html",
    "href": "resources.html",
    "title": "All Course Materials + Additional Readings & Resources Organized by Topic",
    "section": "",
    "text": "Lecture 1\nLecture 2\nLecture 3\nLecture 4\nLecture 5\nLecture 6\nLecture 7\nLecture 8\nLecture 9 (Part 1, Part 2)\nLecture 10\n\n\n\n\n\nLab 1: Generalize Fast! Replication of analyses from Buntaine et al., 2024\nLab 2: Code-along ü¶û ‚Üí Tables, Figures, & Lobsters (OH MY!)\nAdditional Lab - Illustration of ‚ÄòUnbiased Estimator‚Äô Using {gganimate} | View GIF\nWeek 8 - Lab\nWeek 9 - Lab Part 1\nWeek 9 - Lab Part 2\n\n\n\n\n\nAssignment 1: California Spiny Lobster Abundance (Panulirus Interruptus)\nAssignment 2: Investigating Causal Inference Designs Applied in Environmental Science\n\n\n\n\n\nFinal Project Instructions\nStudy Group Assignments\nFinal Presentation Grading Rubric\nPresentation Question Form"
  },
  {
    "objectID": "resources.html#all-course-materials",
    "href": "resources.html#all-course-materials",
    "title": "All Course Materials + Additional Readings & Resources Organized by Topic",
    "section": "",
    "text": "Lecture 1\nLecture 2\nLecture 3\nLecture 4\nLecture 5\nLecture 6\nLecture 7\nLecture 8\nLecture 9 (Part 1, Part 2)\nLecture 10\n\n\n\n\n\nLab 1: Generalize Fast! Replication of analyses from Buntaine et al., 2024\nLab 2: Code-along ü¶û ‚Üí Tables, Figures, & Lobsters (OH MY!)\nAdditional Lab - Illustration of ‚ÄòUnbiased Estimator‚Äô Using {gganimate} | View GIF\nWeek 8 - Lab\nWeek 9 - Lab Part 1\nWeek 9 - Lab Part 2\n\n\n\n\n\nAssignment 1: California Spiny Lobster Abundance (Panulirus Interruptus)\nAssignment 2: Investigating Causal Inference Designs Applied in Environmental Science\n\n\n\n\n\nFinal Project Instructions\nStudy Group Assignments\nFinal Presentation Grading Rubric\nPresentation Question Form"
  },
  {
    "objectID": "resources.html#applied-studies-referenced-in-eds-241---organized-by-topic",
    "href": "resources.html#applied-studies-referenced-in-eds-241---organized-by-topic",
    "title": "All Course Materials + Additional Readings & Resources Organized by Topic",
    "section": "Applied studies referenced in EDS 241 - organized by topic!",
    "text": "Applied studies referenced in EDS 241 - organized by topic!\n\nData science - general topics\nReproducibility\n\nLowndes et al., 2017 - Our path to better science in less time using open data science tools\nGehlman & Loken, 2013 - The garden of forking paths: Why multiple comparisons can be a problem, even when there is no ‚Äúfishing expedition‚Äù or ‚Äúp-hacking‚Äù\n\nData viz\n\nAnimated Visualizations - ‚ÄòR Psychologist‚Äô - Kristoffer Magnusson\nLarsen, Meng, & Kendall 2018 - Causal analysis in control‚Äìimpact ecological studies with observational data\n\n\n\nRegression\nOrdinary Least Squares (OLS)\nGeneralized Linear Models (GLMs)\n\nCarruthers et al., 2008 - Generalized linear models: model selection, diagnostics, and overdispersion\n\n\n\nCausal inference - Econometrics\nCausal inference research conducted at Bren & UCSB\n\nSarah Anderson - Monitoring ‚Üí Compliance\nMark Buntaine - Citizen participation ‚Üí Env. Policy\nJoan Dudney - Climate change ‚Üí Infectious tree disease\nLeah Stokes - Voting ‚Üí Climate policy\nOlivier Deschenes - Climate change ‚Üí Agricultural output\nMatto Mildenberger - Wildfires ‚Üí Voting\nKyle Meng - Marine Reserves, Inequality, Ground water rights\n\n\n\nExperimental Design\n\nBuntaine et al., 2024 - Social competition drives collective action to reduce informal waste burning in Uganda\n\nEDS 241 - Glossary of Econometric and Statistical Terms\nJ-PAL power calculation tool\nEGAP power calculation tool\n\n\n\n\nDifference-in-Difference Design\n\nMoland et al., 2013 - Lobster and cod benefit from small-scale northern marine protected areas: inference from an empirical before‚Äì after control-impact study\n\n\n\nRegression Discontinuity Design\n\n\n\n\n\n\n\nInstrumental Variable Design"
  },
  {
    "objectID": "course-materials/lecture-slides/lecture1.1-slides.html#title-slide",
    "href": "course-materials/lecture-slides/lecture1.1-slides.html#title-slide",
    "title": "EDS 241",
    "section": "",
    "text": "EDS 123: Lecture 1\nToday‚Äôs topic here\n\nWeek 1 | January 6th, 2025"
  },
  {
    "objectID": "course-materials/lecture-slides/lecture1.1-slides.html#welcome",
    "href": "course-materials/lecture-slides/lecture1.1-slides.html#welcome",
    "title": "EDS 241",
    "section": "",
    "text": "Welcome to EDS 123!\n\n\nThis is the default text size\n\n\n\n\nI can apply a class selector from meds-slides-styles.scss to change the size of this text\n\n\n\nSimilarly, I can apply a class selector(s) to modify the appearance of a subset of text\n\nCheck out this demo presentation for examples of all the cool capabilities of Revealjs"
  },
  {
    "objectID": "course-materials/lecture-slides/lecture1.1-slides.html#block-level-styling",
    "href": "course-materials/lecture-slides/lecture1.1-slides.html#block-level-styling",
    "title": "EDS 241",
    "section": "",
    "text": "You can also apply styles to blocks of content\n\nWe can use divs to apply styling to all content within the div gates (:::):\n::: {.selector1 .selector2}\nSome content\n:::\n\nFor example:\n\nThis text is bolded\nThis text is italicized"
  },
  {
    "objectID": "course-materials/lecture-slides/lecture1.1-slides.html#embedded-code",
    "href": "course-materials/lecture-slides/lecture1.1-slides.html#embedded-code",
    "title": "EDS 241",
    "section": "",
    "text": "Here is some embedded & rendered code\n\n\nlibrary(palmerpenguins)\nlibrary(ggplot2)\n\nggplot(data = penguins, aes(x = bill_length_mm, y = bill_depth_mm, color = species)) +\n  geom_point()"
  },
  {
    "objectID": "course-materials/lecture-slides/lecture1.1-slides.html#rendered-code",
    "href": "course-materials/lecture-slides/lecture1.1-slides.html#rendered-code",
    "title": "EDS 241",
    "section": "",
    "text": "Alternatively, we can render just the output"
  },
  {
    "objectID": "course-materials/lecture-slides/lecture1.1-slides.html#columns",
    "href": "course-materials/lecture-slides/lecture1.1-slides.html#columns",
    "title": "EDS 241",
    "section": "",
    "text": "You can arrange content in columns\n\n\n\nLearn more about the MEDS program on the Bren website."
  },
  {
    "objectID": "course-materials/week5.html",
    "href": "course-materials/week5.html",
    "title": "Week 5:",
    "section": "",
    "text": "Lecture slides\nLab\nAnswer key\n\n\n\n\nLecture 5\nNA\nNA"
  },
  {
    "objectID": "course-materials/week5.html#lecture-materials",
    "href": "course-materials/week5.html#lecture-materials",
    "title": "Week 5:",
    "section": "",
    "text": "Lecture slides\nLab\nAnswer key\n\n\n\n\nLecture 5\nNA\nNA"
  },
  {
    "objectID": "course-materials/week5.html#assignment-reminders",
    "href": "course-materials/week5.html#assignment-reminders",
    "title": "Week 5:",
    "section": " Assignment Reminders",
    "text": "Assignment Reminders\n\n\n\n\n\n\n\n\n\nAssignment Type\nAssignment Title\nDate Assigned\nDate Due\n\n\n\n\nEnd of Class Survey\nEOC-Week5\n02/05/2025\n02/05/2025, 11:55pm\n\n\nReading - Mastering ‚ÄôMetrics\nA&P: Chapter 3 - Instrumental Variables\n02/05/2025\n02/05/2025\n\n\nReading - European Lobster MPA Study\nMoland et al., 2013\n01/29/2025\n\n\n\nRevise & Resubmit (Assignment 1)\nInstructions to Revise & Resubmit\n02/05/2025\n02/10/2025 (11:59pm)"
  },
  {
    "objectID": "course-materials/week5.html#extra-resources",
    "href": "course-materials/week5.html#extra-resources",
    "title": "Week 5:",
    "section": " Extra resources",
    "text": "Extra resources\nCreate {gganimate} plot - Illustration of concept unbiased estimator:\n\n\nRepo - unbiased-estimator-viz\n\n\nExtra credit: Create an animated plot that illustrates a concept relevant to EDS 241 (i.e, causal inference, statistics, design-based methods). Take a look at the examples made by Kristoffer Magnusson for inspiration!\n\n\nCool animations - visualize statistical concepts: ‚ÄòR Psychologist‚Äô - Kristoffer Magnusson\nReproducibility: Lowndes et al., 2017 - Our path to better science in less time using open data science tools"
  },
  {
    "objectID": "course-materials/week6.html",
    "href": "course-materials/week6.html",
    "title": "Week 6:",
    "section": "",
    "text": "Lecture slides\nLab\nAnswer key\n\n\n\n\nLecture 6\nNA\nNA"
  },
  {
    "objectID": "course-materials/week6.html#lecture-materials",
    "href": "course-materials/week6.html#lecture-materials",
    "title": "Week 6:",
    "section": "",
    "text": "Lecture slides\nLab\nAnswer key\n\n\n\n\nLecture 6\nNA\nNA"
  },
  {
    "objectID": "course-materials/week6.html#assignment-reminders",
    "href": "course-materials/week6.html#assignment-reminders",
    "title": "Week 6:",
    "section": " Assignment Reminders",
    "text": "Assignment Reminders\n\n\n\n\n\n\n\n\n\nAssignment Type\nAssignment Title\nDate Assigned\nDate Due\n\n\n\n\nEnd of Class Survey\nEOC-Week6\n02/12/2025\n02/12/2025, 11:55pm\n\n\nAssignment 2\nInvestigating Causal Inference Designs Applied in Environmental Science\n02/10/2025\n02/23/2025, 11:59pm\n\n\nReading - Mastering ‚ÄôMetrics\nA&P: Chapter 6\n02/12/2025\n02/16/2025\n\n\nReading - Article\nStokes, 2015 - Electoral Backlash Against Climate Policy\n01/29/2025"
  },
  {
    "objectID": "course-materials/week6.html#extra-resources",
    "href": "course-materials/week6.html#extra-resources",
    "title": "Week 6:",
    "section": " Extra resources",
    "text": "Extra resources\n\nRDD Lab (optional): Handout - Estimating Regression Discontinuity , RDD Lab Repository"
  },
  {
    "objectID": "course-materials/week3.html",
    "href": "course-materials/week3.html",
    "title": "Week 3: Difference-in-Difference",
    "section": "",
    "text": "Lecture slides\nLab\nResources\n\n\n\n\nLecture (TUES): Difference-in-Difference , Lecture (THURS)\n ,"
  },
  {
    "objectID": "course-materials/week3.html#lecture-materials",
    "href": "course-materials/week3.html#lecture-materials",
    "title": "Week 3: Difference-in-Difference",
    "section": "",
    "text": "Lecture slides\nLab\nResources\n\n\n\n\nLecture (TUES): Difference-in-Difference , Lecture (THURS)\n ,"
  },
  {
    "objectID": "course-materials/week3.html#assignment-reminders",
    "href": "course-materials/week3.html#assignment-reminders",
    "title": "Week 3: Difference-in-Difference",
    "section": " Assignment Reminders",
    "text": "Assignment Reminders\n\n\n\n\n\n\n\n\n\nAssignment Type\nAssignment Title\nDate Assigned\nDate Due\n\n\n\n\nFinish Reading Study\nBuntaine et al.¬†(2024) , ADD HERE -&gt; Qs for Prof.¬†Buntaine\n01/16\n01/22\n\n\nReading - Intro. to Natural Experiments\nLarsen et al., 2019\n01/16\n01/22\n\n\nReading - Mastering ‚ÄôMetrics\nA&P: Chapter 5\n01/16\n01/22"
  },
  {
    "objectID": "course-materials/week3.html#extra-resources",
    "href": "course-materials/week3.html#extra-resources",
    "title": "Week 3: Difference-in-Difference",
    "section": " Extra resources",
    "text": "Extra resources\n\nVideo - Interpreting regression coefficients & interactions - Assignment 1"
  },
  {
    "objectID": "course-materials/week1.html",
    "href": "course-materials/week1.html",
    "title": "Week 1:",
    "section": "",
    "text": "Welcome to the page for all things Week 1!"
  },
  {
    "objectID": "course-materials/week1.html#lecture-materials",
    "href": "course-materials/week1.html#lecture-materials",
    "title": "Week 1:",
    "section": " Lecture Materials",
    "text": "Lecture Materials\n\n\n\n\n\n\n\nLecture slides\nResources\n\n\n\n\nLecture 1 (TUE): Intro. to Casual Inference\n\n\n\nLecture 2 (THURS): Potenital Outcomes"
  },
  {
    "objectID": "course-materials/week1.html#assignment-reminders",
    "href": "course-materials/week1.html#assignment-reminders",
    "title": "Week 1:",
    "section": " Assignment Reminders",
    "text": "Assignment Reminders\n\n\n\n\n\n\n\n\n\nAssignment Type\nAssignment Title\nDate Assigned\nDate Due\n\n\n\n\nAssignment 1\nHandout: HW1-LOBSTRS , Repository: HW1-LOBSTRS\n01/08\n01/17\n\n\nMini-Assignment\nTopics of Interest - Spreadsheet (Add Topic/Study Here)\n01/08\n01/12\n\n\nReading - Mastering ‚ÄôMetrics\nA&P: Introduction & Chapter 1\n01/02\n01/07\n\n\nReading - Applied Policy Study\nPages 1-3: Buntaine et al.¬†(2024) , Glossary of Econometrics & Statistics Terms\n01/02\n01/07\n\n\nIn Class (Thursday)\nReading Quiz\n01/08\n01/08\n\n\nWeek 2 - Reading - Mastering ‚ÄôMetrics\nA&P: Chapter 2\n01/08\n01/15\n\n\nWeek 2 - Reading - Generalized Linear Models\nCarruthers et al., 2008\n01/13\n01/13\n\n\n\n\n\n\n\n\n\n\nAssignment Submission and Rubric\n\n\n\n\nSubmission: Submit assignments through GradeScope\nQuestion-specific guide: Each question has detailed grading criteria available on GradeScope. Be sure to review these guidelines before submitting your work!\nGeneral rubric outline: For an overview of the grading criteria, refer to the assignment rubric - general outline"
  },
  {
    "objectID": "course-materials/week1.html#discussion-materials",
    "href": "course-materials/week1.html#discussion-materials",
    "title": "Week 1:",
    "section": " Extra Resources",
    "text": "Extra Resources\n\n\nKey Concepts (videos)\nThese short videos that accompany Mastering ‚ÄôMetrics are a great resource for Chapter-1. The style is a bit extra, but they explain the key concepts clearly. If any of these concepts are unclear, I strongly recommend watching them (they may be relevant to the reading quiz tomorrow):\n\nCeteris Paribus & Counterfactuals - 6 minutes\nOmmitted Variable Bias & Selection Bias - 9 minutes\nExperimental Design & Random Assignment - 10 minutes\nHow to Read Econometrics Papers - 12 minutes\n\nReferences\n\nGelman, A., & Loken, E. (2013). The garden of forking paths: Why multiple comparisons can be a problem, even when there is no ‚Äúfishing expedition‚Äù or ‚Äúp-hacking‚Äù and the research hypothesis was posited ahead of time. Department of Statistics, Columbia University, 348(1-17), 3.\nBuntaine, M. T., Komakech, P., & Shen, S. V. (2024). Social competition drives collective action to reduce informal waste burning in Uganda. Proceedings of the National Academy of Sciences, 121(23), e2319712121."
  },
  {
    "objectID": "course-materials/week9.html",
    "href": "course-materials/week9.html",
    "title": "Week 9:",
    "section": "",
    "text": "Lecture slides\nLab\nAnswer key\n\n\n\n\nRepository - W9-Lab-EDS241\nRDD Lab Handout\n\n\n\n\nFixed Effects Lab Handout"
  },
  {
    "objectID": "course-materials/week9.html#lecture-materials",
    "href": "course-materials/week9.html#lecture-materials",
    "title": "Week 9:",
    "section": "",
    "text": "Lecture slides\nLab\nAnswer key\n\n\n\n\nRepository - W9-Lab-EDS241\nRDD Lab Handout\n\n\n\n\nFixed Effects Lab Handout"
  },
  {
    "objectID": "course-materials/week9.html#assignment-reminders",
    "href": "course-materials/week9.html#assignment-reminders",
    "title": "Week 9:",
    "section": " Assignment Reminders",
    "text": "Assignment Reminders\n\n\n\n\n\n\n\n\n\nAssignment Type\nAssignment Title\nDate Assigned\nDate Due\n\n\n\n\nEnd of Class Survey\nEOC-Week9\n03/05/2025\n03/05/2025, 11:55pm\n\n\nReading - Article\nNeal, 2024 - Estimating the Effectiveness of Forest Protection Using Regression Discontinuity\n03/05/2025"
  },
  {
    "objectID": "course-materials/week9.html#extra-resources",
    "href": "course-materials/week9.html#extra-resources",
    "title": "Week 9:",
    "section": " Extra resources",
    "text": "Extra resources"
  },
  {
    "objectID": "index.html#course-description",
    "href": "index.html#course-description",
    "title": "Environmental Policy Evaluation (EDS241 / ESM244)",
    "section": "Course Description",
    "text": "Course Description\nThis course will present state of the art program evaluation techniques necessary to evaluate the impact of environmental policies. The program evaluation methods presented will aim at identifying and measuring the causal effect of policies, regulations, and interventions on environmental outcomes of interest. Students will learn the research designs and methods for estimating causal effects with experimental and non-experimental data. This will prepare the students for interpreting and conducting high-quality empirical research, with applications in cross-sectional data and panel data settings."
  },
  {
    "objectID": "index.html#teaching-team",
    "href": "index.html#teaching-team",
    "title": "Environmental Policy Evaluation (EDS241 / ESM244)",
    "section": "Teaching Team",
    "text": "Teaching Team\n\n\n\n\n\n\nInstructor\n\n\n\n\n\n\n\n\n\n\n\nAdam Garber\nEmail: agarber@ucsb.edu\nOffice hours (subject to change):\nTuesday 12-2pm @ BH 4329 (Manzanita Room)\nAlternate instructor availability:\nMonday - Friday (by appointment)\n\n\n\n\n\n\nCo- Instructor\n\n\n\n\n\n\n\n\n\n\n\nAnnie Adams\nEmail: aradams@ucsb.edu\nOffice hours (subject to change):\nThursday 3 pm - 4 pm @ BH 3418 (Office)"
  },
  {
    "objectID": "index.html#meeting-time-location",
    "href": "index.html#meeting-time-location",
    "title": "Environmental Policy Evaluation (EDS241 / ESM244)",
    "section": "Meeting Time & Location",
    "text": "Meeting Time & Location\n\nTime\nTuesday & Thursday, 8:00-9:15 am\n\n\n\nLocation\nBren Hall 1414"
  },
  {
    "objectID": "index.html#acknowledgements",
    "href": "index.html#acknowledgements",
    "title": "Environmental Policy Evaluation (EDS241 / ESM244)",
    "section": "Acknowledgements",
    "text": "Acknowledgements\nThis website was developed from a template authored by Sam Csik. The EDS241/ESM244 course website houses materials which are heavily reused, adapted from, and inspired by Sam Csik‚Äôs original work. This website was made using Quarto and the repository used to make this website can be found here."
  },
  {
    "objectID": "assignments.html",
    "href": "assignments.html",
    "title": "Assignments",
    "section": "",
    "text": "Important: Email group presentation slides to instructor (Adam) before 7am on presentation day! (3/19)\n\n\n\n\nPresentation slides must be emailed to your instructor (Adam) before 7am (3/19)\nAcceptable submission formats include google slides or a shared link of the slides in pdf format (i.e., a Google Drive link)\nIf slides are available in advance please send early!\n\n\n\n\n\n\nFinal Project Instructions\nStudy Group Assignments\nFinal Presentation Grading Rubric\nPresentation Question Form\n\n\n\n\n\n\n\n\n\n\nPresentations will start at 8am sharp (please do not be late)\n\n\n\nPresentation order will follow the numerical order of your assigned group study.\n\n\n\nFirst 4 presentations (~60 minutes)\nShort break (~5-10 minutes)\nNext 3 presentations (~45 minutes)\nShort break (~5-10 minutes)\nFinal 3 presentations (~45 minutes)\n\nüçø Snacks are encouraged! Bring food for sustenance/survival or to share with the class! (totally optional)"
  },
  {
    "objectID": "assignments.html#final-project-links",
    "href": "assignments.html#final-project-links",
    "title": "Assignments",
    "section": "",
    "text": "Final Project Instructions\nStudy Group Assignments\nFinal Presentation Grading Rubric\nPresentation Question Form"
  },
  {
    "objectID": "assignments.html#presentation-day-schedule-8-11am-wednesday-319",
    "href": "assignments.html#presentation-day-schedule-8-11am-wednesday-319",
    "title": "Assignments",
    "section": "",
    "text": "Presentations will start at 8am sharp (please do not be late)\n\n\n\nPresentation order will follow the numerical order of your assigned group study.\n\n\n\nFirst 4 presentations (~60 minutes)\nShort break (~5-10 minutes)\nNext 3 presentations (~45 minutes)\nShort break (~5-10 minutes)\nFinal 3 presentations (~45 minutes)\n\nüçø Snacks are encouraged! Bring food for sustenance/survival or to share with the class! (totally optional)"
  },
  {
    "objectID": "course-materials/labs/lab1.1.html",
    "href": "course-materials/labs/lab1.1.html",
    "title": "Lab 1.1 TEMPLATE",
    "section": "",
    "text": "# load packages ----"
  },
  {
    "objectID": "course-materials/labs/lab1.1.html#setup",
    "href": "course-materials/labs/lab1.1.html#setup",
    "title": "Lab 1.1 TEMPLATE",
    "section": "",
    "text": "# load packages ----"
  },
  {
    "objectID": "course-materials/labs/lab1.1.html#tidy-data-review",
    "href": "course-materials/labs/lab1.1.html#tidy-data-review",
    "title": "Lab 1.1 TEMPLATE",
    "section": "Tidy Data Review",
    "text": "Tidy Data Review\nExample untidy / wide data:\n\n# create some untidy temperature data ----\n\n# print it out ----\n\nUsing pivot_longer() to ‚Äúlengthen‚Äù / tidy our data:\n\n# convert data from wide &gt; long ----\n\n# print it out ----"
  },
  {
    "objectID": "course-materials/week8.html",
    "href": "course-materials/week8.html",
    "title": "Week 8:",
    "section": "",
    "text": "Lecture slides\nLab\nAnswer key\n\n\n\n\nRepository - W8-Lab-EDS241\nLab-Handout\nRMD Updated!"
  },
  {
    "objectID": "course-materials/week8.html#lecture-materials",
    "href": "course-materials/week8.html#lecture-materials",
    "title": "Week 8:",
    "section": "",
    "text": "Lecture slides\nLab\nAnswer key\n\n\n\n\nRepository - W8-Lab-EDS241\nLab-Handout\nRMD Updated!"
  },
  {
    "objectID": "course-materials/week8.html#assignment-reminders",
    "href": "course-materials/week8.html#assignment-reminders",
    "title": "Week 8:",
    "section": " Assignment Reminders",
    "text": "Assignment Reminders\n\n\n\n\n\n\n\n\n\nAssignment Type\nAssignment Title\nDate Assigned\nDate Due\n\n\n\n\nEnd of Class Survey\nEOC-Week8\n02/26/2025\n02/26/2025, 11:55pm\n\n\nReading - Article\nStatistical matching for conservation science - Schleicher et al., 2019 -\n02/16/2025\n\n\n\nReview chapter - Mastering Metrics\nChapter 3 - Instrumental Variables\n02/26/2025"
  },
  {
    "objectID": "course-materials/discussion-materials/week1-discussion-slides.html#title-slide",
    "href": "course-materials/discussion-materials/week1-discussion-slides.html#title-slide",
    "title": "EDS 241",
    "section": "",
    "text": "EDS 240: Discussion 1\nToday‚Äôs topic here\n\nWeek 1 | January 7th, 2024"
  },
  {
    "objectID": "course-materials/discussion-materials/week1-discussion-slides.html#slide-slug-here",
    "href": "course-materials/discussion-materials/week1-discussion-slides.html#slide-slug-here",
    "title": "EDS 241",
    "section": "",
    "text": "Slide title"
  },
  {
    "objectID": "course-materials/week2.html",
    "href": "course-materials/week2.html",
    "title": "Week 2:",
    "section": "",
    "text": "Lecture slides\nResources\n\n\n\n\nLecture (TUES): Generalized Linear Models\n , \n\n\nLecture (THURS): Modeling Count Outcomes"
  },
  {
    "objectID": "course-materials/week2.html#lecture-materials",
    "href": "course-materials/week2.html#lecture-materials",
    "title": "Week 2:",
    "section": "",
    "text": "Lecture slides\nResources\n\n\n\n\nLecture (TUES): Generalized Linear Models\n , \n\n\nLecture (THURS): Modeling Count Outcomes"
  },
  {
    "objectID": "course-materials/week2.html#assignment-reminders",
    "href": "course-materials/week2.html#assignment-reminders",
    "title": "Week 2:",
    "section": " Assignment Reminders",
    "text": "Assignment Reminders\n\n\n\n\n\n\n\n\n\nAssignment Type\nAssignment Title\nDate Assigned\nDate Due\n\n\n\n\nMini-Assignment\nTopics of Interest - Spreadsheet (Add Topic/Study Here)\n01/08\n01/12\n\n\nReading - Mastering ‚ÄôMetrics\nA&P: Chapter 2\n01/08\n01/15\n\n\nReading - Generalized Linear Models\nCarruthers et al., 2008\n01/13\n01/13\n\n\nIn Class (Thursday)\nReading Quiz\n01/15\n01/15\n\n\nAssignment 1\nHandout: HW1-LOBSTRS , Repository: HW1-LOBSTRS\n01/08\n01/17\n\n\n\n\n\n\n\n\n\n\nTipAssignment Submission and Rubric\n\n\n\n\nSubmission: Submit assignments through GradeScope\nQuestion-specific guide: Each question has detailed grading criteria available on GradeScope. Be sure to review these guidelines before submitting your work!\nGeneral rubric outline: For an overview of the grading criteria, refer to the assignment rubric - general outline"
  },
  {
    "objectID": "course-materials/week7.html",
    "href": "course-materials/week7.html",
    "title": "Week 7:",
    "section": "",
    "text": "Lecture slides\nLab\nAnswer key\n\n\n\n\nLecture 7\nNA\nNA"
  },
  {
    "objectID": "course-materials/week7.html#lecture-materials",
    "href": "course-materials/week7.html#lecture-materials",
    "title": "Week 7:",
    "section": "",
    "text": "Lecture slides\nLab\nAnswer key\n\n\n\n\nLecture 7\nNA\nNA"
  },
  {
    "objectID": "course-materials/week7.html#assignment-reminders",
    "href": "course-materials/week7.html#assignment-reminders",
    "title": "Week 7:",
    "section": " Assignment Reminders",
    "text": "Assignment Reminders\n\n\n\n\n\n\n\n\n\nAssignment Type\nAssignment Title\nDate Assigned\nDate Due\n\n\n\n\nEnd of Class Survey\nEOC-Week7\n02/19/2025\n02/19/2025, 11:55pm\n\n\nAssignment 2\nInvestigating Causal Inference Designs Applied in Environmental Science\n02/10/2025\n02/23/2025, 11:59pm\n\n\nReading - Article\nStatistical Matching for Conservation Science - Schleicher et al., 2019 -\n02/16/2025\n\n\n\nReading - Mastering ‚ÄôMetrics\nA&P: Chapter 6\n02/12/2025\n02/19/2025"
  },
  {
    "objectID": "course-materials/week4.html",
    "href": "course-materials/week4.html",
    "title": "Week 4:",
    "section": "",
    "text": "Lecture slides\nLab\nAnswer key\n\n\n\n\nLecture 4 - DiD\nLab 2 - Repo\nLab Key - Handout"
  },
  {
    "objectID": "course-materials/week4.html#lecture-materials",
    "href": "course-materials/week4.html#lecture-materials",
    "title": "Week 4:",
    "section": "",
    "text": "Lecture slides\nLab\nAnswer key\n\n\n\n\nLecture 4 - DiD\nLab 2 - Repo\nLab Key - Handout"
  },
  {
    "objectID": "course-materials/week4.html#assignment-reminders",
    "href": "course-materials/week4.html#assignment-reminders",
    "title": "Week 4:",
    "section": " Assignment Reminders",
    "text": "Assignment Reminders\n\n\n\n\n\n\n\n\n\nAssignment Type\nAssignment Title\nDate Assigned\nDate Due\n\n\n\n\nEnd of Class Survey\nEOC-Week4\n01/29/2025\n01/29/2025, 11:55pm\n\n\nReading - Mastering ‚ÄôMetrics\nA&P: Chapter 4 - Regression Discontinuity\n01/29/2025\n\n\n\nReading - Lobster and cod benefit from small-scale northern marine protected areas: Inference from an empirical before‚Äìafter control-impact study\nMoland et al., 2013\n01/29/2025"
  },
  {
    "objectID": "course-materials/keys/key1.1.html",
    "href": "course-materials/keys/key1.1.html",
    "title": "Lab 1.1 KEY",
    "section": "",
    "text": "# load packages ----\nlibrary(tidyverse)\nlibrary(palmerpenguins)"
  },
  {
    "objectID": "course-materials/keys/key1.1.html#setup",
    "href": "course-materials/keys/key1.1.html#setup",
    "title": "Lab 1.1 KEY",
    "section": "",
    "text": "# load packages ----\nlibrary(tidyverse)\nlibrary(palmerpenguins)"
  },
  {
    "objectID": "course-materials/keys/key1.1.html#tidy-data-review",
    "href": "course-materials/keys/key1.1.html#tidy-data-review",
    "title": "Lab 1.1 KEY",
    "section": "Tidy Data Review",
    "text": "Tidy Data Review\nExample untidy / wide data:\n\n# create some untidy temperature data ----\ntemp_data_wide &lt;- tribble(\n  ~date, ~station1, ~station2,  ~station3,\n  \"2023-10-01\", 30.1, 29.8,  31.2,\n  \"2023-11-01\", 28.6, 29.1,  33.4,\n  \"2023-12-01\", 29.9, 28.5,  32.3\n)\n\n# print it out ----\nprint(temp_data_wide)\n\n# A tibble: 3 √ó 4\n  date       station1 station2 station3\n  &lt;chr&gt;         &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;\n1 2023-10-01     30.1     29.8     31.2\n2 2023-11-01     28.6     29.1     33.4\n3 2023-12-01     29.9     28.5     32.3\n\n\nUsing pivot_longer() to ‚Äúlengthen‚Äù / tidy our data:\n\n# convert data from wide &gt; long ----\ntemp_data_long &lt;- temp_data_wide %&gt;%  \n  pivot_longer(cols = starts_with(\"station\"),\n               names_to = \"station_id\",\n               values_to = \"temp_c\")\n\n# print it out ----\nprint(temp_data_long)\n\n# A tibble: 9 √ó 3\n  date       station_id temp_c\n  &lt;chr&gt;      &lt;chr&gt;       &lt;dbl&gt;\n1 2023-10-01 station1     30.1\n2 2023-10-01 station2     29.8\n3 2023-10-01 station3     31.2\n4 2023-11-01 station1     28.6\n5 2023-11-01 station2     29.1\n6 2023-11-01 station3     33.4\n7 2023-12-01 station1     29.9\n8 2023-12-01 station2     28.5\n9 2023-12-01 station3     32.3"
  },
  {
    "objectID": "course-materials/week10.html",
    "href": "course-materials/week10.html",
    "title": "Week 10:",
    "section": "",
    "text": "Lecture slides\nLab\nAnswer key\n\n\n\n\nWeek 10 Slides (Updated after class)"
  },
  {
    "objectID": "course-materials/week10.html#lecture-materials",
    "href": "course-materials/week10.html#lecture-materials",
    "title": "Week 10:",
    "section": "",
    "text": "Lecture slides\nLab\nAnswer key\n\n\n\n\nWeek 10 Slides (Updated after class)"
  },
  {
    "objectID": "course-materials/week10.html#assignment-reminders",
    "href": "course-materials/week10.html#assignment-reminders",
    "title": "Week 10:",
    "section": " Assignment Reminders",
    "text": "Assignment Reminders\n\n\n\n\n\n\n\n\n\nAssignment Type\nAssignment Title\nDate Assigned\nDate Due\n\n\n\n\n\nReminder: Please fill out the EDS 241 course evaluations. ¬†No EOC survey this week!\n03/12/2025\n03/12/2025, 11:55pm\n\n\nReading - Article\nGertler, 2004 - High Impact Randomized Experiment - PROGRESA Intervention\n03/12/2025"
  },
  {
    "objectID": "course-materials/resources.html",
    "href": "course-materials/resources.html",
    "title": "All Course Materials + Additional Readings & Resources Organized by Topic",
    "section": "",
    "text": "Lecture 1\nLecture 2\nLecture 3\nLecture 4\nLecture 5\nLecture 6\nLecture 7\nLecture 8\nLecture 9 (Part 1, Part 2)\nLecture 10\n\n\n\n\n\nLab 1: Generalize Fast! Replication of analyses from Buntaine et al., 2024\nLab 2: Code-along ü¶û ‚Üí Tables, Figures, & Lobsters (OH MY!)\nAdditional Lab - Illustration of ‚ÄòUnbiased Estimator‚Äô Using {gganimate} | View GIF\nWeek 8 - Lab\nWeek 9 - Lab Part 1\nWeek 9 - Lab Part 2\n\n\n\n\n\nAssignment 1: California Spiny Lobster Abundance (Panulirus Interruptus)\nAssignment 2: Investigating Causal Inference Designs Applied in Environmental Science\n\n\n\n\n\nFinal Project Instructions\nStudy Group Assignments\nFinal Presentation Grading Rubric\nPresentation Question Form"
  },
  {
    "objectID": "course-materials/resources.html#all-course-materials",
    "href": "course-materials/resources.html#all-course-materials",
    "title": "All Course Materials + Additional Readings & Resources Organized by Topic",
    "section": "",
    "text": "Lecture 1\nLecture 2\nLecture 3\nLecture 4\nLecture 5\nLecture 6\nLecture 7\nLecture 8\nLecture 9 (Part 1, Part 2)\nLecture 10\n\n\n\n\n\nLab 1: Generalize Fast! Replication of analyses from Buntaine et al., 2024\nLab 2: Code-along ü¶û ‚Üí Tables, Figures, & Lobsters (OH MY!)\nAdditional Lab - Illustration of ‚ÄòUnbiased Estimator‚Äô Using {gganimate} | View GIF\nWeek 8 - Lab\nWeek 9 - Lab Part 1\nWeek 9 - Lab Part 2\n\n\n\n\n\nAssignment 1: California Spiny Lobster Abundance (Panulirus Interruptus)\nAssignment 2: Investigating Causal Inference Designs Applied in Environmental Science\n\n\n\n\n\nFinal Project Instructions\nStudy Group Assignments\nFinal Presentation Grading Rubric\nPresentation Question Form"
  },
  {
    "objectID": "course-materials/resources.html#applied-studies-referenced-in-eds-241---organized-by-topic",
    "href": "course-materials/resources.html#applied-studies-referenced-in-eds-241---organized-by-topic",
    "title": "All Course Materials + Additional Readings & Resources Organized by Topic",
    "section": "Applied studies referenced in EDS 241 - organized by topic!",
    "text": "Applied studies referenced in EDS 241 - organized by topic!\n\nData science - general topics\nReproducibility\n\nLowndes et al., 2017 - Our path to better science in less time using open data science tools\nGehlman & Loken, 2013 - The garden of forking paths: Why multiple comparisons can be a problem, even when there is no ‚Äúfishing expedition‚Äù or ‚Äúp-hacking‚Äù\n\nData viz\n\nAnimated Visualizations - ‚ÄòR Psychologist‚Äô - Kristoffer Magnusson\nLarsen, Meng, & Kendall 2018 - Causal analysis in control‚Äìimpact ecological studies with observational data\n\n\n\nRegression\nOrdinary Least Squares (OLS)\nGeneralized Linear Models (GLMs)\n\nCarruthers et al., 2008 - Generalized linear models: model selection, diagnostics, and overdispersion\n\n\n\nCausal inference - Econometrics\nCausal inference research conducted at Bren & UCSB\n\nSarah Anderson - Monitoring ‚Üí Compliance\nMark Buntaine - Citizen participation ‚Üí Env. Policy\nJoan Dudney - Climate change ‚Üí Infectious tree disease\nLeah Stokes - Voting ‚Üí Climate policy\nOlivier Deschenes - Climate change ‚Üí Agricultural output\nMatto Mildenberger - Wildfires ‚Üí Voting\nKyle Meng - Marine Reserves, Inequality, Ground water rights\n\n\n\nExperimental Design\n\nBuntaine et al., 2024 - Social competition drives collective action to reduce informal waste burning in Uganda\n\nEDS 241 - Glossary of Econometric and Statistical Terms\nJ-PAL power calculation tool\nEGAP power calculation tool\n\n\n\n\nDifference-in-Difference Design\n\nMoland et al., 2013 - Lobster and cod benefit from small-scale northern marine protected areas: inference from an empirical before‚Äì after control-impact study\n\n\n\nRegression Discontinuity Design\n\n\n\n\n\n\n\nInstrumental Variable Design"
  },
  {
    "objectID": "course-materials/assignments.html",
    "href": "course-materials/assignments.html",
    "title": "Assignments",
    "section": "",
    "text": "Important: Email group presentation slides to instructor (Adam) before 7am on presentation day! (3/19)\n\n\n\n\nPresentation slides must be emailed to your instructor (Adam) before 7am (3/19)\nAcceptable submission formats include google slides or a shared link of the slides in pdf format (i.e., a Google Drive link)\nIf slides are available in advance please send early!\n\n\n\n\n\n\nFinal Project Instructions\nStudy Group Assignments\nFinal Presentation Grading Rubric\nPresentation Question Form\n\n\n\n\n\n\n\n\n\n\nPresentations will start at 8am sharp (please do not be late)\n\n\n\nPresentation order will follow the numerical order of your assigned group study.\n\n\n\nFirst 4 presentations (~60 minutes)\nShort break (~5-10 minutes)\nNext 3 presentations (~45 minutes)\nShort break (~5-10 minutes)\nFinal 3 presentations (~45 minutes)\n\nüçø Snacks are encouraged! Bring food for sustenance/survival or to share with the class! (totally optional)"
  },
  {
    "objectID": "course-materials/assignments.html#final-project-links",
    "href": "course-materials/assignments.html#final-project-links",
    "title": "Assignments",
    "section": "",
    "text": "Final Project Instructions\nStudy Group Assignments\nFinal Presentation Grading Rubric\nPresentation Question Form"
  },
  {
    "objectID": "course-materials/assignments.html#presentation-day-schedule-8-11am-wednesday-319",
    "href": "course-materials/assignments.html#presentation-day-schedule-8-11am-wednesday-319",
    "title": "Assignments",
    "section": "",
    "text": "Presentations will start at 8am sharp (please do not be late)\n\n\n\nPresentation order will follow the numerical order of your assigned group study.\n\n\n\nFirst 4 presentations (~60 minutes)\nShort break (~5-10 minutes)\nNext 3 presentations (~45 minutes)\nShort break (~5-10 minutes)\nFinal 3 presentations (~45 minutes)\n\nüçø Snacks are encouraged! Bring food for sustenance/survival or to share with the class! (totally optional)"
  },
  {
    "objectID": "index.html#section",
    "href": "index.html#section",
    "title": "Environmental Policy Evaluation (EDS241 / ESM244)",
    "section": "",
    "text": "////////   $$$$$$$$   ||\\      /|        @@@@@@     ^      ^    ^      ^   \n  /          $          || \\    / |       @      @    ^      ^    ^      ^   \n  /          $          ||  \\  /  |              @    ^      ^    ^      ^   \n  /          $          ||   \\/  ||             @     ^      ^    ^      ^   \n  ////////   $$$$$$$$   ||       ||            @      ^^^^^^^^    ^^^^^^^^   \n  /                 $   ||       ||           @              ^           ^   \n  /                 $   ||       ||          @               ^           ^   \n  /                 $   ||       ||         @                ^           ^   \n   ////////   $$$$$$$$   ||       ||       @@@@@@@            ^           ^"
  },
  {
    "objectID": "course-materials/labs/week1.html",
    "href": "course-materials/labs/week1.html",
    "title": "Lab 1",
    "section": "",
    "text": "There are many different ways to create tidy tables in R. You might be familiar with the kable function from {knitr} that creates tables for rectangular data. Kable tables don‚Äôt have a ton of flexibility, but are great at producing clean, simple tables. As we move into creating tables for the many different statistical models we will learn in this course, we will need to move beyond a simple kable table. That is where {gt} comes in! A {gt} table allows for the following structure, making it ideal for displaying different statistical outcomes.\n\n\n\n\n\n\n\n\n\nWe are going to use our class survey data to create some tables!\n\n# Load packages\nlibrary(tidyverse)\nlibrary(gtsummary)\nlibrary(gt)\nlibrary(janitor)\n\n\n\n# Read in class survey data and split into two random groups\nclass_data &lt;- read_csv(\"https://raw.github.com/garberadamc/W26-Policy-Eval/main/course-materials/labs/data/W26_class_survey.csv\") %&gt;%\n  mutate(random_groups = sample(rep(c(\"control\", \"treatment\"), each = n()/2)))\n\nLet‚Äôs create a base table that we will use with both kable and gt.\n\n# Create the base summary object\nbalance_summary &lt;- class_data %&gt;%\n  gtsummary::tbl_summary(\n    # Select how you want to group columns\n    by = random_groups,\n    # Variables to include\n    include = c(height, pets, dominant_hand, fav_number), \n    # Display mean and standard deviation for all continuous variables\n    statistic = list(all_continuous() ~ \"{mean} ({sd})\")\n  ) %&gt;% \n  # Add p value\n  add_p()\n\nbalance_summary\n\n\n\n\n\n\n\n\n\n\n\n\n\nCharacteristic\ncontrol\nN = 171\ntreatment\nN = 171\np-value2\n\n\n\n\nheight\n66.4 (4.2)\n63.5 (3.1)\n0.022\n\n\npets\n8 (47%)\n14 (82%)\n0.031\n\n\ndominant_hand\n\n\n\n\n0.6\n\n\n¬†¬†¬†¬†Left\n3 (18%)\n1 (5.9%)\n\n\n\n\n¬†¬†¬†¬†Right\n14 (82%)\n16 (94%)\n\n\n\n\nfav_number\n\n\n\n\n0.9\n\n\n¬†¬†¬†¬†2\n1 (5.9%)\n4 (24%)\n\n\n\n\n¬†¬†¬†¬†3\n2 (12%)\n2 (12%)\n\n\n\n\n¬†¬†¬†¬†4\n3 (18%)\n1 (5.9%)\n\n\n\n\n¬†¬†¬†¬†5\n3 (18%)\n2 (12%)\n\n\n\n\n¬†¬†¬†¬†6\n1 (5.9%)\n1 (5.9%)\n\n\n\n\n¬†¬†¬†¬†7\n4 (24%)\n4 (24%)\n\n\n\n\n¬†¬†¬†¬†8\n1 (5.9%)\n1 (5.9%)\n\n\n\n\n¬†¬†¬†¬†9\n2 (12%)\n1 (5.9%)\n\n\n\n\n¬†¬†¬†¬†10\n0 (0%)\n1 (5.9%)\n\n\n\n\n\n1 Mean (SD); n (%)\n\n\n2 Wilcoxon rank sum test; Pearson‚Äôs Chi-squared test; Fisher‚Äôs exact test\n\n\n\n\n\n\n\n\nNow lets output our balance_summary with a kable table!\n\nbalance_summary %&gt;%\n  as_kable_extra(caption = \"Class Survey Balance Table\") %&gt;% \n  kableExtra::kable_styling(\n    bootstrap_options = c(\"striped\", \"condensed\", \"hover\")\n  )\n\n\nClass Survey Balance Table\n\n\n\n\n\n\n\n\nCharacteristic\ncontrol\nN = 17\ntreatment\nN = 17\np-value\n\n\n\n\nheight\n66.4 (4.2)\n63.5 (3.1)\n0.022\n\n\npets\n8 (47%)\n14 (82%)\n0.031\n\n\ndominant_hand\n\n\n0.6\n\n\nLeft\n3 (18%)\n1 (5.9%)\n\n\n\nRight\n14 (82%)\n16 (94%)\n\n\n\nfav_number\n\n\n0.9\n\n\n2\n1 (5.9%)\n4 (24%)\n\n\n\n3\n2 (12%)\n2 (12%)\n\n\n\n4\n3 (18%)\n1 (5.9%)\n\n\n\n5\n3 (18%)\n2 (12%)\n\n\n\n6\n1 (5.9%)\n1 (5.9%)\n\n\n\n7\n4 (24%)\n4 (24%)\n\n\n\n8\n1 (5.9%)\n1 (5.9%)\n\n\n\n9\n2 (12%)\n1 (5.9%)\n\n\n\n10\n0 (0%)\n1 (5.9%)\n\n\n\n\n1 Mean (SD); n (%)\n\n\n\n\n\n2 Wilcoxon rank sum test; Pearson's Chi-squared test; Fisher's exact test\n\n\n\n\n\n\n\n\n\nIt looks fine‚Ä¶ But we can make it a lot nicer with {gt}!\n\n# Convert our balance summary table to a gt table\nbalance_summary %&gt;%\n  as_gt() %&gt;%\n  \n  # Add a Title and Subtitle\n  tab_header(\n    title = \"Class Survey Balance Table\",\n    subtitle = \"With Randomly Assigned Groups\"\n  ) %&gt;%\n  \n  # Add a Spanner to group the data columns\n  tab_spanner(\n    label = \"Randomized Groups\",\n    columns = c(stat_1, stat_2)\n  ) %&gt;%\n  \n  # Change column labels\n  cols_label(\n    label = \"Variable\",\n    p.value = \"P-Value\"\n  ) %&gt;%\n  \n  # Add a source note at the bottom\n  tab_source_note(\n    source_note = \"Note: Data from the Winter 2026 Class Survey.\"\n  )\n\n\n\n\n\n\n\n\n\n\n\n\n\nClass Survey Balance Table\n\n\nWith Randomly Assigned Groups\n\n\nVariable\n\nRandomized Groups\n\nP-Value2\n\n\ncontrol\nN = 171\ntreatment\nN = 171\n\n\n\n\nheight\n66.4 (4.2)\n63.5 (3.1)\n0.022\n\n\npets\n8 (47%)\n14 (82%)\n0.031\n\n\ndominant_hand\n\n\n\n\n0.6\n\n\n¬†¬†¬†¬†Left\n3 (18%)\n1 (5.9%)\n\n\n\n\n¬†¬†¬†¬†Right\n14 (82%)\n16 (94%)\n\n\n\n\nfav_number\n\n\n\n\n0.9\n\n\n¬†¬†¬†¬†2\n1 (5.9%)\n4 (24%)\n\n\n\n\n¬†¬†¬†¬†3\n2 (12%)\n2 (12%)\n\n\n\n\n¬†¬†¬†¬†4\n3 (18%)\n1 (5.9%)\n\n\n\n\n¬†¬†¬†¬†5\n3 (18%)\n2 (12%)\n\n\n\n\n¬†¬†¬†¬†6\n1 (5.9%)\n1 (5.9%)\n\n\n\n\n¬†¬†¬†¬†7\n4 (24%)\n4 (24%)\n\n\n\n\n¬†¬†¬†¬†8\n1 (5.9%)\n1 (5.9%)\n\n\n\n\n¬†¬†¬†¬†9\n2 (12%)\n1 (5.9%)\n\n\n\n\n¬†¬†¬†¬†10\n0 (0%)\n1 (5.9%)\n\n\n\n\n\n1 Mean (SD); n (%)\n\n\n2 Wilcoxon rank sum test; Pearson‚Äôs Chi-squared test; Fisher‚Äôs exact test\n\n\nNote: Data from the Winter 2026 Class Survey.\n\n\n\n\n\n\n\n\nThe {gt} table looks a lot cleaner! Let‚Äôs move on with creating some more {gt} tables!\nWe are going to use data from the Moland et al.¬†2013 study on Lobster MPAS.\nThe data we will be working with has the following variables:\n\n\n\n\n\n\n\n\nVariable\nData Type\nDescriptions\n\n\n\n\nyear\nNumeric (5-levels)\nYears measured from 2006 to 2010\n\n\nregion\nCharacter (3-levels)\nbol= Bol√¶rne , kve = Kvernskj√¶r , flo = Fl√∏devigen\n\n\ntreat\nCharacter (2-levels)\nmpa = treatment , con = control\n\n\ncpue\nNumeric\nCatch per unit effort\n\n\n\nLet‚Äôs read in our data to get started!\n\nlobsters &lt;- read_csv(\"https://raw.github.com/garberadamc/Lab2-EDS241-Moland13/main/data/moland13_lobsters.csv\")\n\nWe will start with creating a table for the total CPUE for each year and region.\nTo create a table with a column for each region, we need to untidy our data! We will do so by pivoting our data into wide format, with a column for each region, and the CPU for each year in that specific region.\nThis will be a 2 way table, since we are displaying data for two variables.\n\ntbl_2way &lt;- lobsters %&gt;%\n  \n  # Calculate total cpue for each year/region\n  group_by(year, region) %&gt;%\n  summarize(\n      total_cpue = sum(cpue, na.rm = TRUE),\n            .groups = \"drop\") %&gt;% # Same as `ungroup()`\n  \n  # Pivot to create column for each region\n  pivot_wider(\n      names_from = region, \n      values_from = total_cpue) %&gt;%\n  arrange(year) %&gt;%\n  \n  # Add a row for total cpue \n  adorn_totals(\"row\")\n\nTime to use {gt} to make this into a nice looking table!!\n\ntbl_2way %&gt;% \n  gt(rowname_col = \"year\") %&gt;%\n  tab_header(\n    title = \"European Lobster Catch by Region and Year\",\n    subtitle = \"Total Catch Per Unit Effort (CPUE) by year and region\") %&gt;% \n  cols_label(\n    bol = \"Bol√¶rne\",\n    flo = \"Kvernskj√¶r\",\n    kve = \"Fl√∏devigen\") %&gt;%\n  tab_source_note(\n      \"Source: Moland et al., 2013\")\n\n\n\n\n\n\n\nEuropean Lobster Catch by Region and Year\n\n\nTotal Catch Per Unit Effort (CPUE) by year and region\n\n\n\nBol√¶rne\nKvernskj√¶r\nFl√∏devigen\n\n\n\n\n2006\n127\n122\n177\n\n\n2007\n269\n93\n276\n\n\n2008\n249\n151\n367\n\n\n2009\n484\n168\n466\n\n\n2010\n463\n175\n449\n\n\nTotal\n1592\n709\n1735\n\n\n\nSource: Moland et al., 2013\n\n\n\n\n\n\n\n\nLet‚Äôs now add our treat variable into the table, so we can see how lobster catch varied within our control and MPA groups. This will be a 3 way table!\n\ntbl_3way &lt;- lobsters %&gt;%\n  # Calculate total cpue for each year/region/treatment group\n  group_by(year, region, treat) %&gt;%\n  summarize(total_cpue = sum(cpue, na.rm = TRUE),\n            .groups = \"drop\") %&gt;%\n  \n  # Pivot to create column for each trt/ control group within each region\n  pivot_wider(names_from = c(region, treat), values_from = total_cpue) %&gt;%\n  \n  arrange(year)\n\nTime to use {gt} to make this into a nice looking table!!\n\nfancy_table &lt;- tbl_3way %&gt;%\n  gt(rowname_col = \"year\") %&gt;%\n  tab_header(\n    title = \"European Lobster Catch by Year, Region and Treatment\",\n    subtitle = \"Total Catch Per Unit Effort (CPUE)\"\n  ) %&gt;%\n  tab_spanner(\n    label = \"Bol√¶rne\",\n    columns = c(\"bol_con\", \"bol_mpa\")\n  ) %&gt;%\n  tab_spanner(\n    label = \"Fl√∏devigen\",\n    columns = c(\"flo_con\", \"flo_mpa\")\n  ) %&gt;%\n  tab_spanner(\n    label = \"Kvernskj√¶r\",\n    columns = c(\"kve_con\", \"kve_mpa\")\n  ) %&gt;%\n  cols_label(\n    bol_con = \"Control\",\n    bol_mpa = \"MPA\",\n    flo_con = \"Control\",\n    flo_mpa = \"MPA\",\n    kve_con = \"Control\",\n    kve_mpa = \"MPA\")\n\nfancy_table\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nEuropean Lobster Catch by Year, Region and Treatment\n\n\nTotal Catch Per Unit Effort (CPUE)\n\n\n\n\nBol√¶rne\n\n\nFl√∏devigen\n\n\nKvernskj√¶r\n\n\n\nControl\nMPA\nControl\nMPA\nControl\nMPA\n\n\n\n\n2006\n52\n75\n54\n68\n125\n52\n\n\n2007\n98\n171\n33\n60\n114\n162\n\n\n2008\n78\n171\n55\n96\n178\n189\n\n\n2009\n187\n297\n51\n117\n244\n222\n\n\n2010\n148\n315\n64\n111\n198\n251\n\n\n\n\n\n\n\n\n\nWe can add plots within our table as well! While maybe not completely necessary in this instance, it can be a helpful tool to have!\n\ntable_w_plots &lt;- lobsters %&gt;%\n  # Calculate total cpue for each year\n  group_by(year) %&gt;%\n  summarize(\n      total_cpue = sum(cpue, na.rm = TRUE),\n      dist_cpue = list(cpue),\n      .groups = \"drop\") %&gt;% \n  arrange(year) %&gt;% \n  # Create gt table\n    gt() %&gt;% \n     tab_header(\n    title = \"European Lobster Catch Totals and Distribution (2006-2010)\",\n    subtitle = \"Total Catch Per Unit Effort (CPUE)\") %&gt;% \n    cols_label(\n    year = \"Year\",\n    total_cpue = \"Total CPUE\",\n    dist_cpue = \"Density CPUE\") %&gt;%\n    # Add in line density plots\n    gtExtras::gt_plt_dist( \n        dist_cpue,\n        type = \"density\", \n        line_color = \"blue\", \n        fill_color = \"red\")\n\n\ntable_w_plots \n\n\n\n\n\n\n\n\n\n\n\n\nEuropean Lobster Catch Totals and Distribution (2006-2010)\n\n\nTotal Catch Per Unit Effort (CPUE)\n\n\nYear\nTotal CPUE\nDensity CPUE\n\n\n\n\n2006\n426\n\n\n\n   \n\n\n\n2007\n638\n\n\n\n   \n\n\n\n2008\n767\n\n\n\n   \n\n\n\n2009\n1118\n\n\n\n   \n\n\n\n2010\n1087\n\n\n\n   \n\n\n\n\n\n\n\n\n\n#install.packages(\"praise\")\n#install.packages(\"cowsay\")\n#install.packages(\"beepr\")\nlibrary(praise)\nlibrary(cowsay)\nlibrary(beepr)\nsay(\"All done making some beautiful tables! :) \", \"whale\"); beep(3)  \n\n\n ___________________________________________ \n&lt; All done making some beautiful tables! :) &gt;\n ------------------------------------------- \n  \\\n   \\\n\n     .-'\n'--./ /     _.---.\n'-,  (__..-`       \\\n   \\          .     |\n    `,.__.   ,__.--/\n     '._/_.'___.-`"
  },
  {
    "objectID": "course-materials/labs/week1.html#activity-1-creating-tidy-tables",
    "href": "course-materials/labs/week1.html#activity-1-creating-tidy-tables",
    "title": "Lab 1",
    "section": "",
    "text": "There are many different ways to create tidy tables in R. You might be familiar with the kable function from {knitr} that creates tables for rectangular data. Kable tables don‚Äôt have a ton of flexibility, but are great at producing clean, simple tables. As we move into creating tables for the many different statistical models we will learn in this course, we will need to move beyond a simple kable table. That is where {gt} comes in! A {gt} table allows for the following structure, making it ideal for displaying different statistical outcomes.\n\n\n\n\n\n\n\n\n\nWe are going to use our class survey data to create some tables!\n\n# Load packages\nlibrary(tidyverse)\nlibrary(gtsummary)\nlibrary(gt)\nlibrary(janitor)\n\n\n\n# Read in class survey data and split into two random groups\nclass_data &lt;- read_csv(\"https://raw.github.com/garberadamc/W26-Policy-Eval/main/course-materials/labs/data/W26_class_survey.csv\") %&gt;%\n  mutate(random_groups = sample(rep(c(\"control\", \"treatment\"), each = n()/2)))\n\nLet‚Äôs create a base table that we will use with both kable and gt.\n\n# Create the base summary object\nbalance_summary &lt;- class_data %&gt;%\n  gtsummary::tbl_summary(\n    # Select how you want to group columns\n    by = random_groups,\n    # Variables to include\n    include = c(height, pets, dominant_hand, fav_number), \n    # Display mean and standard deviation for all continuous variables\n    statistic = list(all_continuous() ~ \"{mean} ({sd})\")\n  ) %&gt;% \n  # Add p value\n  add_p()\n\nbalance_summary\n\n\n\n\n\n\n\n\n\n\n\n\n\nCharacteristic\ncontrol\nN = 171\ntreatment\nN = 171\np-value2\n\n\n\n\nheight\n66.4 (4.2)\n63.5 (3.1)\n0.022\n\n\npets\n8 (47%)\n14 (82%)\n0.031\n\n\ndominant_hand\n\n\n\n\n0.6\n\n\n¬†¬†¬†¬†Left\n3 (18%)\n1 (5.9%)\n\n\n\n\n¬†¬†¬†¬†Right\n14 (82%)\n16 (94%)\n\n\n\n\nfav_number\n\n\n\n\n0.9\n\n\n¬†¬†¬†¬†2\n1 (5.9%)\n4 (24%)\n\n\n\n\n¬†¬†¬†¬†3\n2 (12%)\n2 (12%)\n\n\n\n\n¬†¬†¬†¬†4\n3 (18%)\n1 (5.9%)\n\n\n\n\n¬†¬†¬†¬†5\n3 (18%)\n2 (12%)\n\n\n\n\n¬†¬†¬†¬†6\n1 (5.9%)\n1 (5.9%)\n\n\n\n\n¬†¬†¬†¬†7\n4 (24%)\n4 (24%)\n\n\n\n\n¬†¬†¬†¬†8\n1 (5.9%)\n1 (5.9%)\n\n\n\n\n¬†¬†¬†¬†9\n2 (12%)\n1 (5.9%)\n\n\n\n\n¬†¬†¬†¬†10\n0 (0%)\n1 (5.9%)\n\n\n\n\n\n1 Mean (SD); n (%)\n\n\n2 Wilcoxon rank sum test; Pearson‚Äôs Chi-squared test; Fisher‚Äôs exact test\n\n\n\n\n\n\n\n\nNow lets output our balance_summary with a kable table!\n\nbalance_summary %&gt;%\n  as_kable_extra(caption = \"Class Survey Balance Table\") %&gt;% \n  kableExtra::kable_styling(\n    bootstrap_options = c(\"striped\", \"condensed\", \"hover\")\n  )\n\n\nClass Survey Balance Table\n\n\n\n\n\n\n\n\nCharacteristic\ncontrol\nN = 17\ntreatment\nN = 17\np-value\n\n\n\n\nheight\n66.4 (4.2)\n63.5 (3.1)\n0.022\n\n\npets\n8 (47%)\n14 (82%)\n0.031\n\n\ndominant_hand\n\n\n0.6\n\n\nLeft\n3 (18%)\n1 (5.9%)\n\n\n\nRight\n14 (82%)\n16 (94%)\n\n\n\nfav_number\n\n\n0.9\n\n\n2\n1 (5.9%)\n4 (24%)\n\n\n\n3\n2 (12%)\n2 (12%)\n\n\n\n4\n3 (18%)\n1 (5.9%)\n\n\n\n5\n3 (18%)\n2 (12%)\n\n\n\n6\n1 (5.9%)\n1 (5.9%)\n\n\n\n7\n4 (24%)\n4 (24%)\n\n\n\n8\n1 (5.9%)\n1 (5.9%)\n\n\n\n9\n2 (12%)\n1 (5.9%)\n\n\n\n10\n0 (0%)\n1 (5.9%)\n\n\n\n\n1 Mean (SD); n (%)\n\n\n\n\n\n2 Wilcoxon rank sum test; Pearson's Chi-squared test; Fisher's exact test\n\n\n\n\n\n\n\n\n\nIt looks fine‚Ä¶ But we can make it a lot nicer with {gt}!\n\n# Convert our balance summary table to a gt table\nbalance_summary %&gt;%\n  as_gt() %&gt;%\n  \n  # Add a Title and Subtitle\n  tab_header(\n    title = \"Class Survey Balance Table\",\n    subtitle = \"With Randomly Assigned Groups\"\n  ) %&gt;%\n  \n  # Add a Spanner to group the data columns\n  tab_spanner(\n    label = \"Randomized Groups\",\n    columns = c(stat_1, stat_2)\n  ) %&gt;%\n  \n  # Change column labels\n  cols_label(\n    label = \"Variable\",\n    p.value = \"P-Value\"\n  ) %&gt;%\n  \n  # Add a source note at the bottom\n  tab_source_note(\n    source_note = \"Note: Data from the Winter 2026 Class Survey.\"\n  )\n\n\n\n\n\n\n\n\n\n\n\n\n\nClass Survey Balance Table\n\n\nWith Randomly Assigned Groups\n\n\nVariable\n\nRandomized Groups\n\nP-Value2\n\n\ncontrol\nN = 171\ntreatment\nN = 171\n\n\n\n\nheight\n66.4 (4.2)\n63.5 (3.1)\n0.022\n\n\npets\n8 (47%)\n14 (82%)\n0.031\n\n\ndominant_hand\n\n\n\n\n0.6\n\n\n¬†¬†¬†¬†Left\n3 (18%)\n1 (5.9%)\n\n\n\n\n¬†¬†¬†¬†Right\n14 (82%)\n16 (94%)\n\n\n\n\nfav_number\n\n\n\n\n0.9\n\n\n¬†¬†¬†¬†2\n1 (5.9%)\n4 (24%)\n\n\n\n\n¬†¬†¬†¬†3\n2 (12%)\n2 (12%)\n\n\n\n\n¬†¬†¬†¬†4\n3 (18%)\n1 (5.9%)\n\n\n\n\n¬†¬†¬†¬†5\n3 (18%)\n2 (12%)\n\n\n\n\n¬†¬†¬†¬†6\n1 (5.9%)\n1 (5.9%)\n\n\n\n\n¬†¬†¬†¬†7\n4 (24%)\n4 (24%)\n\n\n\n\n¬†¬†¬†¬†8\n1 (5.9%)\n1 (5.9%)\n\n\n\n\n¬†¬†¬†¬†9\n2 (12%)\n1 (5.9%)\n\n\n\n\n¬†¬†¬†¬†10\n0 (0%)\n1 (5.9%)\n\n\n\n\n\n1 Mean (SD); n (%)\n\n\n2 Wilcoxon rank sum test; Pearson‚Äôs Chi-squared test; Fisher‚Äôs exact test\n\n\nNote: Data from the Winter 2026 Class Survey.\n\n\n\n\n\n\n\n\nThe {gt} table looks a lot cleaner! Let‚Äôs move on with creating some more {gt} tables!\nWe are going to use data from the Moland et al.¬†2013 study on Lobster MPAS.\nThe data we will be working with has the following variables:\n\n\n\n\n\n\n\n\nVariable\nData Type\nDescriptions\n\n\n\n\nyear\nNumeric (5-levels)\nYears measured from 2006 to 2010\n\n\nregion\nCharacter (3-levels)\nbol= Bol√¶rne , kve = Kvernskj√¶r , flo = Fl√∏devigen\n\n\ntreat\nCharacter (2-levels)\nmpa = treatment , con = control\n\n\ncpue\nNumeric\nCatch per unit effort\n\n\n\nLet‚Äôs read in our data to get started!\n\nlobsters &lt;- read_csv(\"https://raw.github.com/garberadamc/Lab2-EDS241-Moland13/main/data/moland13_lobsters.csv\")\n\nWe will start with creating a table for the total CPUE for each year and region.\nTo create a table with a column for each region, we need to untidy our data! We will do so by pivoting our data into wide format, with a column for each region, and the CPU for each year in that specific region.\nThis will be a 2 way table, since we are displaying data for two variables.\n\ntbl_2way &lt;- lobsters %&gt;%\n  \n  # Calculate total cpue for each year/region\n  group_by(year, region) %&gt;%\n  summarize(\n      total_cpue = sum(cpue, na.rm = TRUE),\n            .groups = \"drop\") %&gt;% # Same as `ungroup()`\n  \n  # Pivot to create column for each region\n  pivot_wider(\n      names_from = region, \n      values_from = total_cpue) %&gt;%\n  arrange(year) %&gt;%\n  \n  # Add a row for total cpue \n  adorn_totals(\"row\")\n\nTime to use {gt} to make this into a nice looking table!!\n\ntbl_2way %&gt;% \n  gt(rowname_col = \"year\") %&gt;%\n  tab_header(\n    title = \"European Lobster Catch by Region and Year\",\n    subtitle = \"Total Catch Per Unit Effort (CPUE) by year and region\") %&gt;% \n  cols_label(\n    bol = \"Bol√¶rne\",\n    flo = \"Kvernskj√¶r\",\n    kve = \"Fl√∏devigen\") %&gt;%\n  tab_source_note(\n      \"Source: Moland et al., 2013\")\n\n\n\n\n\n\n\nEuropean Lobster Catch by Region and Year\n\n\nTotal Catch Per Unit Effort (CPUE) by year and region\n\n\n\nBol√¶rne\nKvernskj√¶r\nFl√∏devigen\n\n\n\n\n2006\n127\n122\n177\n\n\n2007\n269\n93\n276\n\n\n2008\n249\n151\n367\n\n\n2009\n484\n168\n466\n\n\n2010\n463\n175\n449\n\n\nTotal\n1592\n709\n1735\n\n\n\nSource: Moland et al., 2013\n\n\n\n\n\n\n\n\nLet‚Äôs now add our treat variable into the table, so we can see how lobster catch varied within our control and MPA groups. This will be a 3 way table!\n\ntbl_3way &lt;- lobsters %&gt;%\n  # Calculate total cpue for each year/region/treatment group\n  group_by(year, region, treat) %&gt;%\n  summarize(total_cpue = sum(cpue, na.rm = TRUE),\n            .groups = \"drop\") %&gt;%\n  \n  # Pivot to create column for each trt/ control group within each region\n  pivot_wider(names_from = c(region, treat), values_from = total_cpue) %&gt;%\n  \n  arrange(year)\n\nTime to use {gt} to make this into a nice looking table!!\n\nfancy_table &lt;- tbl_3way %&gt;%\n  gt(rowname_col = \"year\") %&gt;%\n  tab_header(\n    title = \"European Lobster Catch by Year, Region and Treatment\",\n    subtitle = \"Total Catch Per Unit Effort (CPUE)\"\n  ) %&gt;%\n  tab_spanner(\n    label = \"Bol√¶rne\",\n    columns = c(\"bol_con\", \"bol_mpa\")\n  ) %&gt;%\n  tab_spanner(\n    label = \"Fl√∏devigen\",\n    columns = c(\"flo_con\", \"flo_mpa\")\n  ) %&gt;%\n  tab_spanner(\n    label = \"Kvernskj√¶r\",\n    columns = c(\"kve_con\", \"kve_mpa\")\n  ) %&gt;%\n  cols_label(\n    bol_con = \"Control\",\n    bol_mpa = \"MPA\",\n    flo_con = \"Control\",\n    flo_mpa = \"MPA\",\n    kve_con = \"Control\",\n    kve_mpa = \"MPA\")\n\nfancy_table\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nEuropean Lobster Catch by Year, Region and Treatment\n\n\nTotal Catch Per Unit Effort (CPUE)\n\n\n\n\nBol√¶rne\n\n\nFl√∏devigen\n\n\nKvernskj√¶r\n\n\n\nControl\nMPA\nControl\nMPA\nControl\nMPA\n\n\n\n\n2006\n52\n75\n54\n68\n125\n52\n\n\n2007\n98\n171\n33\n60\n114\n162\n\n\n2008\n78\n171\n55\n96\n178\n189\n\n\n2009\n187\n297\n51\n117\n244\n222\n\n\n2010\n148\n315\n64\n111\n198\n251\n\n\n\n\n\n\n\n\n\nWe can add plots within our table as well! While maybe not completely necessary in this instance, it can be a helpful tool to have!\n\ntable_w_plots &lt;- lobsters %&gt;%\n  # Calculate total cpue for each year\n  group_by(year) %&gt;%\n  summarize(\n      total_cpue = sum(cpue, na.rm = TRUE),\n      dist_cpue = list(cpue),\n      .groups = \"drop\") %&gt;% \n  arrange(year) %&gt;% \n  # Create gt table\n    gt() %&gt;% \n     tab_header(\n    title = \"European Lobster Catch Totals and Distribution (2006-2010)\",\n    subtitle = \"Total Catch Per Unit Effort (CPUE)\") %&gt;% \n    cols_label(\n    year = \"Year\",\n    total_cpue = \"Total CPUE\",\n    dist_cpue = \"Density CPUE\") %&gt;%\n    # Add in line density plots\n    gtExtras::gt_plt_dist( \n        dist_cpue,\n        type = \"density\", \n        line_color = \"blue\", \n        fill_color = \"red\")\n\n\ntable_w_plots \n\n\n\n\n\n\n\n\n\n\n\n\nEuropean Lobster Catch Totals and Distribution (2006-2010)\n\n\nTotal Catch Per Unit Effort (CPUE)\n\n\nYear\nTotal CPUE\nDensity CPUE\n\n\n\n\n2006\n426\n\n\n\n   \n\n\n\n2007\n638\n\n\n\n   \n\n\n\n2008\n767\n\n\n\n   \n\n\n\n2009\n1118\n\n\n\n   \n\n\n\n2010\n1087\n\n\n\n   \n\n\n\n\n\n\n\n\n\n#install.packages(\"praise\")\n#install.packages(\"cowsay\")\n#install.packages(\"beepr\")\nlibrary(praise)\nlibrary(cowsay)\nlibrary(beepr)\nsay(\"All done making some beautiful tables! :) \", \"whale\"); beep(3)  \n\n\n ___________________________________________ \n&lt; All done making some beautiful tables! :) &gt;\n ------------------------------------------- \n  \\\n   \\\n\n     .-'\n'--./ /     _.---.\n'-,  (__..-`       \\\n   \\          .     |\n    `,.__.   ,__.--/\n     '._/_.'___.-`"
  },
  {
    "objectID": "course-materials/labs/week1.html#activity-2-buntaine-policy-study-reading-comprehension-check",
    "href": "course-materials/labs/week1.html#activity-2-buntaine-policy-study-reading-comprehension-check",
    "title": "Lab 1",
    "section": "Activity 2: Buntaine Policy Study Reading Comprehension Check",
    "text": "Activity 2: Buntaine Policy Study Reading Comprehension Check\nWith your group, answer the following questions from the Buntaine article.\n\nWhat prompted this study?\nHow were the treatment and control groups formed?\nDiscuss the matched-pairs design of the study. How were neighborhoods paired? Create a diagram if helpful!\nWas the social competition strategy effective?\nWhat was the primary metric for assessing the impact of the social competition?"
  },
  {
    "objectID": "course-materials/labs/week1_solution.html",
    "href": "course-materials/labs/week1_solution.html",
    "title": "Lab 1",
    "section": "",
    "text": "There are many different ways to create tidy tables in R. You might be familiar with the kable function from {knitr} that creates tables for rectangular data. Kable tables don‚Äôt have a ton of flexibility, but are great at producing clean, simple tables. As we move into creating tables for the many different statistical models we will learn in this course, we will need to move beyond a simple kable table. That is where {gt} comes in! A {gt} table allows for the following structure, making it ideal for displaying different statistical outcomes.\n\n\n\n\n\n\n\n\n\nWe are going to use our class survey data to create some tables!\n\n# Load packages\nlibrary(tidyverse)\n\n‚îÄ‚îÄ Attaching core tidyverse packages ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ tidyverse 2.0.0 ‚îÄ‚îÄ\n‚úî dplyr     1.1.4     ‚úî readr     2.1.5\n‚úî forcats   1.0.1     ‚úî stringr   1.5.2\n‚úî ggplot2   4.0.0     ‚úî tibble    3.3.0\n‚úî lubridate 1.9.4     ‚úî tidyr     1.3.1\n‚úî purrr     1.1.0     \n‚îÄ‚îÄ Conflicts ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ tidyverse_conflicts() ‚îÄ‚îÄ\n‚úñ dplyr::filter() masks stats::filter()\n‚úñ dplyr::lag()    masks stats::lag()\n‚Ñπ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(gtsummary)\n\nWarning: package 'gtsummary' was built under R version 4.5.2\n\nlibrary(gt)\n\nWarning: package 'gt' was built under R version 4.5.2\n\nlibrary(janitor)\n\n\nAttaching package: 'janitor'\n\nThe following objects are masked from 'package:stats':\n\n    chisq.test, fisher.test\n\n# Read in class survey data and split into two random groups\nclass_data &lt;- read_csv(\"https://raw.github.com/garberadamc/W26-Policy-Eval/main/course-materials/labs/data/W26_class_survey.csv\") %&gt;%\n  mutate(random_groups = sample(rep(c(\"control\", \"treatment\"), each = n()/2)))\n\nRows: 34 Columns: 12\n‚îÄ‚îÄ Column specification ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\nDelimiter: \",\"\nchr (9): Timestamp, wakeup, fav_season, pets, dominant_hand, bday_month, cho...\ndbl (3): fav_number, siblings, height\n\n‚Ñπ Use `spec()` to retrieve the full column specification for this data.\n‚Ñπ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\nLet‚Äôs create a base table that we will use with both kable and gt.\n\n# Create the base summary object\nbalance_summary &lt;- class_data %&gt;%\n  gtsummary::tbl_summary(\n    # Select how you want to group columns\n    by = random_groups,\n    # Variables to include\n    include = c(height, pets, dominant_hand, fav_number), \n    # Display mean and standard deviation for all continuous variables\n    statistic = list(all_continuous() ~ \"{mean} ({sd})\")\n  ) %&gt;% \n  # Add p value\n  add_p()\n\nThe following warnings were returned during `add_p()`:\n! For variable `height` (`random_groups`) and \"estimate\", \"statistic\",\n  \"p.value\", \"conf.low\", and \"conf.high\" statistics: cannot compute exact\n  p-value with ties\n! For variable `height` (`random_groups`) and \"estimate\", \"statistic\",\n  \"p.value\", \"conf.low\", and \"conf.high\" statistics: cannot compute exact\n  confidence intervals with ties\n\nbalance_summary\n\n\n\n\n\n\n\n\n\n\n\n\n\nCharacteristic\ncontrol\nN = 171\ntreatment\nN = 171\np-value2\n\n\n\n\nheight\n64.2 (3.7)\n65.6 (4.1)\n0.3\n\n\npets\n13 (76%)\n9 (53%)\n0.2\n\n\ndominant_hand\n\n\n\n\n0.6\n\n\n¬†¬†¬†¬†Left\n1 (5.9%)\n3 (18%)\n\n\n\n\n¬†¬†¬†¬†Right\n16 (94%)\n14 (82%)\n\n\n\n\nfav_number\n\n\n\n\n0.6\n\n\n¬†¬†¬†¬†2\n1 (5.9%)\n4 (24%)\n\n\n\n\n¬†¬†¬†¬†3\n2 (12%)\n2 (12%)\n\n\n\n\n¬†¬†¬†¬†4\n2 (12%)\n2 (12%)\n\n\n\n\n¬†¬†¬†¬†5\n2 (12%)\n3 (18%)\n\n\n\n\n¬†¬†¬†¬†6\n1 (5.9%)\n1 (5.9%)\n\n\n\n\n¬†¬†¬†¬†7\n6 (35%)\n2 (12%)\n\n\n\n\n¬†¬†¬†¬†8\n0 (0%)\n2 (12%)\n\n\n\n\n¬†¬†¬†¬†9\n2 (12%)\n1 (5.9%)\n\n\n\n\n¬†¬†¬†¬†10\n1 (5.9%)\n0 (0%)\n\n\n\n\n\n1 Mean (SD); n (%)\n\n\n2 Wilcoxon rank sum test; Pearson‚Äôs Chi-squared test; Fisher‚Äôs exact test\n\n\n\n\n\n\n\n\nNow lets output our balance_summary with a kable table!\n\nbalance_summary %&gt;%\n  as_kable_extra(caption = \"Class Survey Balance Table\") %&gt;% \n  kableExtra::kable_styling(\n    bootstrap_options = c(\"striped\", \"condensed\", \"hover\")\n  )\n\n\nClass Survey Balance Table\n\n\n\n\n\n\n\n\nCharacteristic\ncontrol\nN = 17\ntreatment\nN = 17\np-value\n\n\n\n\nheight\n64.2 (3.7)\n65.6 (4.1)\n0.3\n\n\npets\n13 (76%)\n9 (53%)\n0.2\n\n\ndominant_hand\n\n\n0.6\n\n\nLeft\n1 (5.9%)\n3 (18%)\n\n\n\nRight\n16 (94%)\n14 (82%)\n\n\n\nfav_number\n\n\n0.6\n\n\n2\n1 (5.9%)\n4 (24%)\n\n\n\n3\n2 (12%)\n2 (12%)\n\n\n\n4\n2 (12%)\n2 (12%)\n\n\n\n5\n2 (12%)\n3 (18%)\n\n\n\n6\n1 (5.9%)\n1 (5.9%)\n\n\n\n7\n6 (35%)\n2 (12%)\n\n\n\n8\n0 (0%)\n2 (12%)\n\n\n\n9\n2 (12%)\n1 (5.9%)\n\n\n\n10\n1 (5.9%)\n0 (0%)\n\n\n\n\n1 Mean (SD); n (%)\n\n\n\n\n\n2 Wilcoxon rank sum test; Pearson's Chi-squared test; Fisher's exact test\n\n\n\n\n\n\n\n\n\nIt looks fine‚Ä¶ But we can make it a lot nicer with {gt}!\n\n# Convert our balance summary table to a gt table\nbalance_summary %&gt;%\n  as_gt() %&gt;%\n  \n  # Add a Title and Subtitle\n  tab_header(\n    title = \"Class Survey Balance Table\",\n    subtitle = \"With Randomly Assigned Groups\"\n  ) %&gt;%\n  \n  # Add a Spanner to group the data columns\n  tab_spanner(\n    label = \"Randomized Groups\",\n    columns = everything()\n  ) %&gt;%\n  \n  # Change column labels\n  cols_label(\n    label = \"Variable\",\n    p.value = \"P-Value\"\n  ) %&gt;%\n  \n  # Add a source note at the bottom\n  tab_source_note(\n    source_note = \"Note: Data from the Winter 2026 Class Survey.\"\n  )\n\n\n\n\n\n\n\n\n\n\n\n\n\nClass Survey Balance Table\n\n\nWith Randomly Assigned Groups\n\n\n\nRandomized Groups\n\n\n\nVariable\ncontrol\nN = 171\ntreatment\nN = 171\nP-Value2\n\n\n\n\nheight\n64.2 (3.7)\n65.6 (4.1)\n0.3\n\n\npets\n13 (76%)\n9 (53%)\n0.2\n\n\ndominant_hand\n\n\n\n\n0.6\n\n\n¬†¬†¬†¬†Left\n1 (5.9%)\n3 (18%)\n\n\n\n\n¬†¬†¬†¬†Right\n16 (94%)\n14 (82%)\n\n\n\n\nfav_number\n\n\n\n\n0.6\n\n\n¬†¬†¬†¬†2\n1 (5.9%)\n4 (24%)\n\n\n\n\n¬†¬†¬†¬†3\n2 (12%)\n2 (12%)\n\n\n\n\n¬†¬†¬†¬†4\n2 (12%)\n2 (12%)\n\n\n\n\n¬†¬†¬†¬†5\n2 (12%)\n3 (18%)\n\n\n\n\n¬†¬†¬†¬†6\n1 (5.9%)\n1 (5.9%)\n\n\n\n\n¬†¬†¬†¬†7\n6 (35%)\n2 (12%)\n\n\n\n\n¬†¬†¬†¬†8\n0 (0%)\n2 (12%)\n\n\n\n\n¬†¬†¬†¬†9\n2 (12%)\n1 (5.9%)\n\n\n\n\n¬†¬†¬†¬†10\n1 (5.9%)\n0 (0%)\n\n\n\n\n\n1 Mean (SD); n (%)\n\n\n2 Wilcoxon rank sum test; Pearson‚Äôs Chi-squared test; Fisher‚Äôs exact test\n\n\nNote: Data from the Winter 2026 Class Survey.\n\n\n\n\n\n\n\n\nThe {gt} table looks a lot cleaner! Let‚Äôs move on with creating some more {gt} tables!\nWe are going to use data from the Moland et al.¬†2013 study on Lobster MPAS.\nThe data we will be working with has the following variables:\n\n\n\n\n\n\n\n\nVariable\nData Type\nDescriptions\n\n\n\n\nyear\nNumeric (5-levels)\nYears measured from 2006 to 2010\n\n\nregion\nCharacter (3-levels)\nbol= Bol√¶rne , kve = Kvernskj√¶r , flo = Fl√∏devigen\n\n\ntreat\nCharacter (2-levels)\nmpa = treatment , con = control\n\n\ncpue\nNumeric\nCatch per unit effort\n\n\n\nLet‚Äôs read in our data to get started!\n\nlobsters &lt;- read_csv(\"https://raw.github.com/garberadamc/Lab2-EDS241-Moland13/main/data/moland13_lobsters.csv\")\n\nWe will start with creating a table for the total CPUE for each year and region.\nTo create a table with a column for each region, we need to untidy our data! We will do so by pivoting our data into wide format, with a column for each region, and the CPU for each year in that specific region.\nThis will be a 2 way table, since we are displaying data for two variables.\n\ntbl_2way &lt;- lobsters %&gt;%\n  \n  # Calculate total cpue for each year/region\n  group_by(year, region) %&gt;%\n  summarize(\n      total_cpue = sum(cpue, na.rm = TRUE),\n            .groups = \"drop\") %&gt;% # Same as `ungroup()`\n  \n  # Pivot to create column for each region\n  pivot_wider(\n      names_from = region, \n      values_from = total_cpue) %&gt;%\n  arrange(year) %&gt;%\n  \n  # Add a row for total cpue \n  adorn_totals(\"row\")\n\nTime to use {gt} to make this into a nice looking table!!\n\ntbl_2way %&gt;% \n  gt(rowname_col = \"year\") %&gt;%\n  tab_header(\n    title = \"European Lobster Catch by Region and Year\",\n    subtitle = \"Total Catch Per Unit Effort (CPUE) by year and region\") %&gt;% \n  cols_label(\n    bol = \"Bol√¶rne\",\n    flo = \"Kvernskj√¶r\",\n    kve = \"Fl√∏devigen\") %&gt;%\n  tab_source_note(\n      \"Source: Moland et al., 2013\")\n\n\n\n\n\n\n\nEuropean Lobster Catch by Region and Year\n\n\nTotal Catch Per Unit Effort (CPUE) by year and region\n\n\n\nBol√¶rne\nKvernskj√¶r\nFl√∏devigen\n\n\n\n\n2006\n127\n122\n177\n\n\n2007\n269\n93\n276\n\n\n2008\n249\n151\n367\n\n\n2009\n484\n168\n466\n\n\n2010\n463\n175\n449\n\n\nTotal\n1592\n709\n1735\n\n\n\nSource: Moland et al., 2013\n\n\n\n\n\n\n\n\nLet‚Äôs now add our treat variable into the table, so we can see how lobster catch varied within our control and MPA groups. This will be a 3 way table!\n\ntbl_3way &lt;- lobsters %&gt;%\n  # Calculate total cpue for each year/region/treatment group\n  group_by(year, region, treat) %&gt;%\n  summarize(total_cpue = sum(cpue, na.rm = TRUE),\n            .groups = \"drop\") %&gt;%\n  \n  # Pivot to create column for each trt/ control group within each region\n  pivot_wider(names_from = c(region, treat), values_from = total_cpue) %&gt;%\n  \n  arrange(year)\n\nTime to use {gt} to make this into a nice looking table!!\n\nfancy_table &lt;- tbl_3way %&gt;%\n  gt(rowname_col = \"year\") %&gt;%\n  tab_header(\n    title = \"European Lobster Catch by Year, Region and Treatment\",\n    subtitle = \"Total Catch Per Unit Effort (CPUE)\"\n  ) %&gt;%\n  tab_spanner(\n    label = \"Bol√¶rne\",\n    columns = c(\"bol_con\", \"bol_mpa\")\n  ) %&gt;%\n  tab_spanner(\n    label = \"Fl√∏devigen\",\n    columns = c(\"flo_con\", \"flo_mpa\")\n  ) %&gt;%\n  tab_spanner(\n    label = \"Kvernskj√¶r\",\n    columns = c(\"kve_con\", \"kve_mpa\")\n  ) %&gt;%\n  cols_label(\n    bol_con = \"Control\",\n    bol_mpa = \"MPA\",\n    flo_con = \"Control\",\n    flo_mpa = \"MPA\",\n    kve_con = \"Control\",\n    kve_mpa = \"MPA\")\n\nfancy_table\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nEuropean Lobster Catch by Year, Region and Treatment\n\n\nTotal Catch Per Unit Effort (CPUE)\n\n\n\n\nBol√¶rne\n\n\nFl√∏devigen\n\n\nKvernskj√¶r\n\n\n\nControl\nMPA\nControl\nMPA\nControl\nMPA\n\n\n\n\n2006\n52\n75\n54\n68\n125\n52\n\n\n2007\n98\n171\n33\n60\n114\n162\n\n\n2008\n78\n171\n55\n96\n178\n189\n\n\n2009\n187\n297\n51\n117\n244\n222\n\n\n2010\n148\n315\n64\n111\n198\n251\n\n\n\n\n\n\n\n\n\nWe can add plots within our table as well! While maybe not completely necessary in this instance, it can be a helpful tool to have!\n\ntable_w_plots &lt;- lobsters %&gt;%\n  # Calculate total cpue for each year\n  group_by(year) %&gt;%\n  summarize(\n      total_cpue = sum(cpue, na.rm = TRUE),\n      dist_cpue = list(cpue),\n      .groups = \"drop\") %&gt;% \n  arrange(year) %&gt;% \n  # Create gt table\n    gt() %&gt;% \n     tab_header(\n    title = \"European Lobster Catch Totals and Distribution (2006-2010)\",\n    subtitle = \"Total Catch Per Unit Effort (CPUE)\") %&gt;% \n    cols_label(\n    year = \"Year\",\n    total_cpue = \"Total CPUE\",\n    dist_cpue = \"Density CPUE\") %&gt;%\n    # Add in line density plots\n    gtExtras::gt_plt_dist( \n        dist_cpue,\n        type = \"density\", \n        line_color = \"blue\", \n        fill_color = \"red\")\n\n\ntable_w_plots \n\n\n\n\n\n\n\n\n\n\n\n\nEuropean Lobster Catch Totals and Distribution (2006-2010)\n\n\nTotal Catch Per Unit Effort (CPUE)\n\n\nYear\nTotal CPUE\nDensity CPUE\n\n\n\n\n2006\n426\n\n\n\n   \n\n\n\n2007\n638\n\n\n\n   \n\n\n\n2008\n767\n\n\n\n   \n\n\n\n2009\n1118\n\n\n\n   \n\n\n\n2010\n1087\n\n\n\n   \n\n\n\n\n\n\n\n\n\n#install.packages(\"praise\")\n#install.packages(\"cowsay\")\n#install.packages(\"beepr\")\nlibrary(praise)\nlibrary(cowsay)\nlibrary(beepr)\nsay(\"All done making some beautiful tables! :) \", \"whale\"); beep(3)  \n\n\n ___________________________________________ \n&lt; All done making some beautiful tables! :) &gt;\n ------------------------------------------- \n  \\\n   \\\n\n     .-'\n'--./ /     _.---.\n'-,  (__..-`       \\\n   \\          .     |\n    `,.__.   ,__.--/\n     '._/_.'___.-`"
  },
  {
    "objectID": "course-materials/labs/week1_solution.html#activity-1-creating-tidy-tables",
    "href": "course-materials/labs/week1_solution.html#activity-1-creating-tidy-tables",
    "title": "Lab 1",
    "section": "",
    "text": "There are many different ways to create tidy tables in R. You might be familiar with the kable function from {knitr} that creates tables for rectangular data. Kable tables don‚Äôt have a ton of flexibility, but are great at producing clean, simple tables. As we move into creating tables for the many different statistical models we will learn in this course, we will need to move beyond a simple kable table. That is where {gt} comes in! A {gt} table allows for the following structure, making it ideal for displaying different statistical outcomes.\n\n\n\n\n\n\n\n\n\nWe are going to use our class survey data to create some tables!\n\n# Load packages\nlibrary(tidyverse)\n\n‚îÄ‚îÄ Attaching core tidyverse packages ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ tidyverse 2.0.0 ‚îÄ‚îÄ\n‚úî dplyr     1.1.4     ‚úî readr     2.1.5\n‚úî forcats   1.0.1     ‚úî stringr   1.5.2\n‚úî ggplot2   4.0.0     ‚úî tibble    3.3.0\n‚úî lubridate 1.9.4     ‚úî tidyr     1.3.1\n‚úî purrr     1.1.0     \n‚îÄ‚îÄ Conflicts ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ tidyverse_conflicts() ‚îÄ‚îÄ\n‚úñ dplyr::filter() masks stats::filter()\n‚úñ dplyr::lag()    masks stats::lag()\n‚Ñπ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(gtsummary)\n\nWarning: package 'gtsummary' was built under R version 4.5.2\n\nlibrary(gt)\n\nWarning: package 'gt' was built under R version 4.5.2\n\nlibrary(janitor)\n\n\nAttaching package: 'janitor'\n\nThe following objects are masked from 'package:stats':\n\n    chisq.test, fisher.test\n\n# Read in class survey data and split into two random groups\nclass_data &lt;- read_csv(\"https://raw.github.com/garberadamc/W26-Policy-Eval/main/course-materials/labs/data/W26_class_survey.csv\") %&gt;%\n  mutate(random_groups = sample(rep(c(\"control\", \"treatment\"), each = n()/2)))\n\nRows: 34 Columns: 12\n‚îÄ‚îÄ Column specification ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\nDelimiter: \",\"\nchr (9): Timestamp, wakeup, fav_season, pets, dominant_hand, bday_month, cho...\ndbl (3): fav_number, siblings, height\n\n‚Ñπ Use `spec()` to retrieve the full column specification for this data.\n‚Ñπ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\nLet‚Äôs create a base table that we will use with both kable and gt.\n\n# Create the base summary object\nbalance_summary &lt;- class_data %&gt;%\n  gtsummary::tbl_summary(\n    # Select how you want to group columns\n    by = random_groups,\n    # Variables to include\n    include = c(height, pets, dominant_hand, fav_number), \n    # Display mean and standard deviation for all continuous variables\n    statistic = list(all_continuous() ~ \"{mean} ({sd})\")\n  ) %&gt;% \n  # Add p value\n  add_p()\n\nThe following warnings were returned during `add_p()`:\n! For variable `height` (`random_groups`) and \"estimate\", \"statistic\",\n  \"p.value\", \"conf.low\", and \"conf.high\" statistics: cannot compute exact\n  p-value with ties\n! For variable `height` (`random_groups`) and \"estimate\", \"statistic\",\n  \"p.value\", \"conf.low\", and \"conf.high\" statistics: cannot compute exact\n  confidence intervals with ties\n\nbalance_summary\n\n\n\n\n\n\n\n\n\n\n\n\n\nCharacteristic\ncontrol\nN = 171\ntreatment\nN = 171\np-value2\n\n\n\n\nheight\n64.2 (3.7)\n65.6 (4.1)\n0.3\n\n\npets\n13 (76%)\n9 (53%)\n0.2\n\n\ndominant_hand\n\n\n\n\n0.6\n\n\n¬†¬†¬†¬†Left\n1 (5.9%)\n3 (18%)\n\n\n\n\n¬†¬†¬†¬†Right\n16 (94%)\n14 (82%)\n\n\n\n\nfav_number\n\n\n\n\n0.6\n\n\n¬†¬†¬†¬†2\n1 (5.9%)\n4 (24%)\n\n\n\n\n¬†¬†¬†¬†3\n2 (12%)\n2 (12%)\n\n\n\n\n¬†¬†¬†¬†4\n2 (12%)\n2 (12%)\n\n\n\n\n¬†¬†¬†¬†5\n2 (12%)\n3 (18%)\n\n\n\n\n¬†¬†¬†¬†6\n1 (5.9%)\n1 (5.9%)\n\n\n\n\n¬†¬†¬†¬†7\n6 (35%)\n2 (12%)\n\n\n\n\n¬†¬†¬†¬†8\n0 (0%)\n2 (12%)\n\n\n\n\n¬†¬†¬†¬†9\n2 (12%)\n1 (5.9%)\n\n\n\n\n¬†¬†¬†¬†10\n1 (5.9%)\n0 (0%)\n\n\n\n\n\n1 Mean (SD); n (%)\n\n\n2 Wilcoxon rank sum test; Pearson‚Äôs Chi-squared test; Fisher‚Äôs exact test\n\n\n\n\n\n\n\n\nNow lets output our balance_summary with a kable table!\n\nbalance_summary %&gt;%\n  as_kable_extra(caption = \"Class Survey Balance Table\") %&gt;% \n  kableExtra::kable_styling(\n    bootstrap_options = c(\"striped\", \"condensed\", \"hover\")\n  )\n\n\nClass Survey Balance Table\n\n\n\n\n\n\n\n\nCharacteristic\ncontrol\nN = 17\ntreatment\nN = 17\np-value\n\n\n\n\nheight\n64.2 (3.7)\n65.6 (4.1)\n0.3\n\n\npets\n13 (76%)\n9 (53%)\n0.2\n\n\ndominant_hand\n\n\n0.6\n\n\nLeft\n1 (5.9%)\n3 (18%)\n\n\n\nRight\n16 (94%)\n14 (82%)\n\n\n\nfav_number\n\n\n0.6\n\n\n2\n1 (5.9%)\n4 (24%)\n\n\n\n3\n2 (12%)\n2 (12%)\n\n\n\n4\n2 (12%)\n2 (12%)\n\n\n\n5\n2 (12%)\n3 (18%)\n\n\n\n6\n1 (5.9%)\n1 (5.9%)\n\n\n\n7\n6 (35%)\n2 (12%)\n\n\n\n8\n0 (0%)\n2 (12%)\n\n\n\n9\n2 (12%)\n1 (5.9%)\n\n\n\n10\n1 (5.9%)\n0 (0%)\n\n\n\n\n1 Mean (SD); n (%)\n\n\n\n\n\n2 Wilcoxon rank sum test; Pearson's Chi-squared test; Fisher's exact test\n\n\n\n\n\n\n\n\n\nIt looks fine‚Ä¶ But we can make it a lot nicer with {gt}!\n\n# Convert our balance summary table to a gt table\nbalance_summary %&gt;%\n  as_gt() %&gt;%\n  \n  # Add a Title and Subtitle\n  tab_header(\n    title = \"Class Survey Balance Table\",\n    subtitle = \"With Randomly Assigned Groups\"\n  ) %&gt;%\n  \n  # Add a Spanner to group the data columns\n  tab_spanner(\n    label = \"Randomized Groups\",\n    columns = everything()\n  ) %&gt;%\n  \n  # Change column labels\n  cols_label(\n    label = \"Variable\",\n    p.value = \"P-Value\"\n  ) %&gt;%\n  \n  # Add a source note at the bottom\n  tab_source_note(\n    source_note = \"Note: Data from the Winter 2026 Class Survey.\"\n  )\n\n\n\n\n\n\n\n\n\n\n\n\n\nClass Survey Balance Table\n\n\nWith Randomly Assigned Groups\n\n\n\nRandomized Groups\n\n\n\nVariable\ncontrol\nN = 171\ntreatment\nN = 171\nP-Value2\n\n\n\n\nheight\n64.2 (3.7)\n65.6 (4.1)\n0.3\n\n\npets\n13 (76%)\n9 (53%)\n0.2\n\n\ndominant_hand\n\n\n\n\n0.6\n\n\n¬†¬†¬†¬†Left\n1 (5.9%)\n3 (18%)\n\n\n\n\n¬†¬†¬†¬†Right\n16 (94%)\n14 (82%)\n\n\n\n\nfav_number\n\n\n\n\n0.6\n\n\n¬†¬†¬†¬†2\n1 (5.9%)\n4 (24%)\n\n\n\n\n¬†¬†¬†¬†3\n2 (12%)\n2 (12%)\n\n\n\n\n¬†¬†¬†¬†4\n2 (12%)\n2 (12%)\n\n\n\n\n¬†¬†¬†¬†5\n2 (12%)\n3 (18%)\n\n\n\n\n¬†¬†¬†¬†6\n1 (5.9%)\n1 (5.9%)\n\n\n\n\n¬†¬†¬†¬†7\n6 (35%)\n2 (12%)\n\n\n\n\n¬†¬†¬†¬†8\n0 (0%)\n2 (12%)\n\n\n\n\n¬†¬†¬†¬†9\n2 (12%)\n1 (5.9%)\n\n\n\n\n¬†¬†¬†¬†10\n1 (5.9%)\n0 (0%)\n\n\n\n\n\n1 Mean (SD); n (%)\n\n\n2 Wilcoxon rank sum test; Pearson‚Äôs Chi-squared test; Fisher‚Äôs exact test\n\n\nNote: Data from the Winter 2026 Class Survey.\n\n\n\n\n\n\n\n\nThe {gt} table looks a lot cleaner! Let‚Äôs move on with creating some more {gt} tables!\nWe are going to use data from the Moland et al.¬†2013 study on Lobster MPAS.\nThe data we will be working with has the following variables:\n\n\n\n\n\n\n\n\nVariable\nData Type\nDescriptions\n\n\n\n\nyear\nNumeric (5-levels)\nYears measured from 2006 to 2010\n\n\nregion\nCharacter (3-levels)\nbol= Bol√¶rne , kve = Kvernskj√¶r , flo = Fl√∏devigen\n\n\ntreat\nCharacter (2-levels)\nmpa = treatment , con = control\n\n\ncpue\nNumeric\nCatch per unit effort\n\n\n\nLet‚Äôs read in our data to get started!\n\nlobsters &lt;- read_csv(\"https://raw.github.com/garberadamc/Lab2-EDS241-Moland13/main/data/moland13_lobsters.csv\")\n\nWe will start with creating a table for the total CPUE for each year and region.\nTo create a table with a column for each region, we need to untidy our data! We will do so by pivoting our data into wide format, with a column for each region, and the CPU for each year in that specific region.\nThis will be a 2 way table, since we are displaying data for two variables.\n\ntbl_2way &lt;- lobsters %&gt;%\n  \n  # Calculate total cpue for each year/region\n  group_by(year, region) %&gt;%\n  summarize(\n      total_cpue = sum(cpue, na.rm = TRUE),\n            .groups = \"drop\") %&gt;% # Same as `ungroup()`\n  \n  # Pivot to create column for each region\n  pivot_wider(\n      names_from = region, \n      values_from = total_cpue) %&gt;%\n  arrange(year) %&gt;%\n  \n  # Add a row for total cpue \n  adorn_totals(\"row\")\n\nTime to use {gt} to make this into a nice looking table!!\n\ntbl_2way %&gt;% \n  gt(rowname_col = \"year\") %&gt;%\n  tab_header(\n    title = \"European Lobster Catch by Region and Year\",\n    subtitle = \"Total Catch Per Unit Effort (CPUE) by year and region\") %&gt;% \n  cols_label(\n    bol = \"Bol√¶rne\",\n    flo = \"Kvernskj√¶r\",\n    kve = \"Fl√∏devigen\") %&gt;%\n  tab_source_note(\n      \"Source: Moland et al., 2013\")\n\n\n\n\n\n\n\nEuropean Lobster Catch by Region and Year\n\n\nTotal Catch Per Unit Effort (CPUE) by year and region\n\n\n\nBol√¶rne\nKvernskj√¶r\nFl√∏devigen\n\n\n\n\n2006\n127\n122\n177\n\n\n2007\n269\n93\n276\n\n\n2008\n249\n151\n367\n\n\n2009\n484\n168\n466\n\n\n2010\n463\n175\n449\n\n\nTotal\n1592\n709\n1735\n\n\n\nSource: Moland et al., 2013\n\n\n\n\n\n\n\n\nLet‚Äôs now add our treat variable into the table, so we can see how lobster catch varied within our control and MPA groups. This will be a 3 way table!\n\ntbl_3way &lt;- lobsters %&gt;%\n  # Calculate total cpue for each year/region/treatment group\n  group_by(year, region, treat) %&gt;%\n  summarize(total_cpue = sum(cpue, na.rm = TRUE),\n            .groups = \"drop\") %&gt;%\n  \n  # Pivot to create column for each trt/ control group within each region\n  pivot_wider(names_from = c(region, treat), values_from = total_cpue) %&gt;%\n  \n  arrange(year)\n\nTime to use {gt} to make this into a nice looking table!!\n\nfancy_table &lt;- tbl_3way %&gt;%\n  gt(rowname_col = \"year\") %&gt;%\n  tab_header(\n    title = \"European Lobster Catch by Year, Region and Treatment\",\n    subtitle = \"Total Catch Per Unit Effort (CPUE)\"\n  ) %&gt;%\n  tab_spanner(\n    label = \"Bol√¶rne\",\n    columns = c(\"bol_con\", \"bol_mpa\")\n  ) %&gt;%\n  tab_spanner(\n    label = \"Fl√∏devigen\",\n    columns = c(\"flo_con\", \"flo_mpa\")\n  ) %&gt;%\n  tab_spanner(\n    label = \"Kvernskj√¶r\",\n    columns = c(\"kve_con\", \"kve_mpa\")\n  ) %&gt;%\n  cols_label(\n    bol_con = \"Control\",\n    bol_mpa = \"MPA\",\n    flo_con = \"Control\",\n    flo_mpa = \"MPA\",\n    kve_con = \"Control\",\n    kve_mpa = \"MPA\")\n\nfancy_table\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nEuropean Lobster Catch by Year, Region and Treatment\n\n\nTotal Catch Per Unit Effort (CPUE)\n\n\n\n\nBol√¶rne\n\n\nFl√∏devigen\n\n\nKvernskj√¶r\n\n\n\nControl\nMPA\nControl\nMPA\nControl\nMPA\n\n\n\n\n2006\n52\n75\n54\n68\n125\n52\n\n\n2007\n98\n171\n33\n60\n114\n162\n\n\n2008\n78\n171\n55\n96\n178\n189\n\n\n2009\n187\n297\n51\n117\n244\n222\n\n\n2010\n148\n315\n64\n111\n198\n251\n\n\n\n\n\n\n\n\n\nWe can add plots within our table as well! While maybe not completely necessary in this instance, it can be a helpful tool to have!\n\ntable_w_plots &lt;- lobsters %&gt;%\n  # Calculate total cpue for each year\n  group_by(year) %&gt;%\n  summarize(\n      total_cpue = sum(cpue, na.rm = TRUE),\n      dist_cpue = list(cpue),\n      .groups = \"drop\") %&gt;% \n  arrange(year) %&gt;% \n  # Create gt table\n    gt() %&gt;% \n     tab_header(\n    title = \"European Lobster Catch Totals and Distribution (2006-2010)\",\n    subtitle = \"Total Catch Per Unit Effort (CPUE)\") %&gt;% \n    cols_label(\n    year = \"Year\",\n    total_cpue = \"Total CPUE\",\n    dist_cpue = \"Density CPUE\") %&gt;%\n    # Add in line density plots\n    gtExtras::gt_plt_dist( \n        dist_cpue,\n        type = \"density\", \n        line_color = \"blue\", \n        fill_color = \"red\")\n\n\ntable_w_plots \n\n\n\n\n\n\n\n\n\n\n\n\nEuropean Lobster Catch Totals and Distribution (2006-2010)\n\n\nTotal Catch Per Unit Effort (CPUE)\n\n\nYear\nTotal CPUE\nDensity CPUE\n\n\n\n\n2006\n426\n\n\n\n   \n\n\n\n2007\n638\n\n\n\n   \n\n\n\n2008\n767\n\n\n\n   \n\n\n\n2009\n1118\n\n\n\n   \n\n\n\n2010\n1087\n\n\n\n   \n\n\n\n\n\n\n\n\n\n#install.packages(\"praise\")\n#install.packages(\"cowsay\")\n#install.packages(\"beepr\")\nlibrary(praise)\nlibrary(cowsay)\nlibrary(beepr)\nsay(\"All done making some beautiful tables! :) \", \"whale\"); beep(3)  \n\n\n ___________________________________________ \n&lt; All done making some beautiful tables! :) &gt;\n ------------------------------------------- \n  \\\n   \\\n\n     .-'\n'--./ /     _.---.\n'-,  (__..-`       \\\n   \\          .     |\n    `,.__.   ,__.--/\n     '._/_.'___.-`"
  },
  {
    "objectID": "course-materials/labs/week1_solution.html#activity-2-buntaine-policy-study-reading-comprehension-check",
    "href": "course-materials/labs/week1_solution.html#activity-2-buntaine-policy-study-reading-comprehension-check",
    "title": "Lab 1",
    "section": "Activity 2: Buntaine Policy Study Reading Comprehension Check",
    "text": "Activity 2: Buntaine Policy Study Reading Comprehension Check\nWith your group, answer the following questions from the Buntaine article.\n\nWhat prompted this study?\nHow were the treatment and control groups formed?\nWas the social competition strategy effective?"
  },
  {
    "objectID": "course-materials/week1.html#lab-materials",
    "href": "course-materials/week1.html#lab-materials",
    "title": "Week 1:",
    "section": " Lab Materials",
    "text": "Lab Materials\n\n\n\n\n\n\n\nLab Materials to Download\nLab Solution\n\n\n\n\nWeek 1 Lab\nWeek 1 Lab Solution"
  },
  {
    "objectID": "course-materials/week2.html#lab-materials",
    "href": "course-materials/week2.html#lab-materials",
    "title": "Week 2:",
    "section": " Lab Materials",
    "text": "Lab Materials\n\n\n\n\n\n\n\nLab Materials to Download\nLab Solution\n\n\n\n\nWeek 2 Lab\nWeek 2 Lab Solution"
  },
  {
    "objectID": "course-materials/labs/week2.html",
    "href": "course-materials/labs/week2.html",
    "title": "Lab 2: Generalize Fast!",
    "section": "",
    "text": "Map of Treatment Assignment Zones in Nansana, Uganda\n\n\n\n\n\nFigure from Buntaine et al., 2024 (S5; Supplemental Materials)\n\n\n\n\n\n\nOpen source data & applied study:\nOpen access data was utilized for this class exercise from the study (Buntaine et al., 2024):\n\nBuntaine, M. T., Komakech, P., & Shen, S. V. (2024). Social competition drives collective action to reduce informal waste burning in Uganda. Proceedings of the National Academy of Sciences, 121(23). https://doi.org/10.1073/pnas.2319712121\n\n\n\nGoals of this exercise\n\nStart with a simple OLS model\nDemonstrate the rationale for estimating progressively complex models\nIllustrate a causal inference approach (i.e., identify robust estimators)\nConclusion: What is the take-home message of the Lab-1 exercise? . . .\nEstimating complex models is a relatively easy part of our work as data scientists‚Ä¶\n\nMaking informed specification decisions & communicating results is the hard part (i.e., the science side of data science)!\n\n\n\n\nLet‚Äôs get started!\nLoad packages and data:\n\nlibrary(tidyverse)   # Keeping things tidy \nlibrary(janitor)     # Housekeeping \nlibrary(here)        # Location, location, location \n#install.packages(\"jtools\")\nlibrary(jtools)      # Pretty regression output \nlibrary(gt)          # Tables \nlibrary(gtsummary)   # Table for checking balance \n#install.packages(\"performance\")\nlibrary(performance) # Check model diagnostics \n#install.packages(\"see\") # Visualization engine for performance package\n#install.packages(\"huxtable\") # Package required for export_summs\n\n\n\n# A tibble: 6 √ó 8\n  zoneid survey    waste_piles rain_0_48hrs rain_49_168hrs pre_count_avg pair_id\n   &lt;dbl&gt; &lt;chr&gt;           &lt;dbl&gt;        &lt;dbl&gt;          &lt;dbl&gt;         &lt;dbl&gt;   &lt;dbl&gt;\n1   1011 post_tre‚Ä¶          97         0              0.4           97      2021\n2   1012 post_tre‚Ä¶          12         0              0.4           12.5    2022\n3   1013 post_tre‚Ä¶          58         0              0.4           87.5    2022\n4   1014 post_tre‚Ä¶          88         0              0.4          110.     2021\n5   1015 post_tre‚Ä¶          53         0.04           0.36         110.     2017\n6   1016 post_tre‚Ä¶          18         0              0.4           23      2017\n# ‚Ñπ 1 more variable: treat &lt;dbl&gt;\n\n\n\n# Read in waste pile count data\ncounts_gpx &lt;- read_csv(here::here( \"data\", \"waste_pile_counts_gpx.csv\")) %&gt;% \n    rename(\"waste_piles\" = \"total\",\n           \"rain_0_48hrs\" = \"rf_0_to_48_hours\",\n           \"rain_49_168hrs\" = \"rf_49_to_168_hours\") %&gt;% \n    filter(survey%in%c(\n      \"post_treatment_1\",\"post_treatment_2\",\"post_treatment_3\",\n      \"post_treatment_4\",\"post_treatment_5\"))\n\n# Select subset of post-treatment periods\npost_treat_subset &lt;- counts_gpx %&gt;% \n   filter(survey == \"post_treatment_4\")\n\nhead(post_treat_subset)\n\nNow that we got our data loaded in, lets take a look at what these variables mean.\n\n\n\n\n\n\n\nLabel\nDescription\n\n\n\n\nzoneid\nZone or neighborhood ID: The observational unit (\\(N=44\\))\n\n\nwaste_piles\nThe outcome variable (\\(Y\\)): The number of waste pile burns recorded (Range: 5, 125)\n\n\ntreat\nThe treatment assignment variable (0 = Control Group; 1 = Treatment Group)\n\n\nrf_0_to_48_hours\nThe rainfall estimated between 0 and 48 hours prior to the start of the audit, derived from NASA MEERA-2 corrected precipitation estimates (mm)\n\n\nrf_49_to_168_hours\nThe rainfall estimated between 49 and 168 hours prior to the start of the audit, derived from NASA MEERA-2 corrected precipitation estimates (mm)\n\n\npre_count_avg\nThe average number of burnt piles in the pre-treatment audits, spread to zone level\n\n\npair_id\nThe unique ID assigned to each pair of zones for assignment to treatment\n\n\n\n\n\nModel 1: Simple OLS (Ordinary Least Squares) estimator\nOLS estimator (specification):\n\\[waste\\ piles =\\beta_0 + \\beta_1(treat) + \\epsilon\\]\n\nm1_ols &lt;- lm(\n    waste_piles ~ treat,\n    data = post_treat_subset)\n\nsumm(m1_ols, model.fit = FALSE)\n\n\n\n\n\nObservations\n44\n\n\nDependent variable\nwaste_piles\n\n\nType\nOLS linear regression\n\n\n\n\n  \n\n\n\n\n\nEst.\nS.E.\nt val.\np\n\n\n\n\n(Intercept)\n77.91\n4.16\n18.73\n0.00\n\n\ntreat\n-24.77\n5.88\n-4.21\n0.00\n\n\n\n Standard errors: OLS\n\n\n\n\n\n\n\n\n\n\n\n\nAre we making reasonable assumptions?\nRemember questioning (and testing) assumptions is the heart of causal inference!We want to make sure our assumptions our reasonable. To do so, let‚Äôstake a quick look at our outcome variable waste piles, and then look at the normality of our residuals.\n\npost_treat_subset %&gt;%\n  ggplot(aes(x = waste_piles)) +\n  geom_histogram(binwidth = 5, fill = \"blue\", color = \"white\", alpha = 0.7) +\n  labs(title = \"Histogram of Waste Piles\",\n    x = \"Waste Piles (counts)\",\n    y = \"Count\") +\n    theme_minimal()\n\n\n\n\n\n\n\n\nWhile our data isn‚Äôt a perfect bell curve, it also isn‚Äôt severely skewed. Let‚Äôs do another assumption check (normality of residuals) to help us decide if OLS is the right model for us.\nTo do so, we will make a QQ plot displaying residuals (y-axis) compared to the normal distribution (x-axis).\n\ncheck_model(m1_ols,  check = \"qq\" )\n\n\n\n\n\n\n\n\nHmm, it doesn‚Äôt seem like the OLS assumption, normality of residuals is a good fit for our data. Onto the next model!\n\n\n\n\n\n\nTipDiscussion\n\n\n\nSince we ruled OLS out, talk with your neighbor about another potential model to try. What kind of variable is our outcome variable? How might that inform what model we try next?\n\n\nWe can ‚Äúrelax‚Äù our assumptions by using a Generalized Linear model! Specifically, we will relax the assumption that the residuals are normally distributed.\n\n\n\n\n\n\nWarningWarning!\n\n\n\nWhen we relax an assumption and make our model more flexible, the statistics and interpretation become more complex!\n\n\nAs you discussed with your partner, our dependent variable, waste_piles is a count variable, not a continuous variable. Therefore, it‚Äôs best we pick a model that explicitly accounts for count outcomes. In walks the poisson regression model!\n\n\n\nModel 2 : Poisson Generalized Linear Regression Model\nWe are going to assume Y follows a Poisson distribution with non negative integers (counts!). To do so, we need to think about the assumptions the Poisson regression makes. This model assumes that variance (dispersion) is proportional to the mean. Does our data match the theoretical distribution proposed? Let‚Äôs check!\n\nlambda_hat &lt;- mean(post_treat_subset$waste_piles)\n\npoisson_curve &lt;- tibble(\n  x = seq(0, max(post_treat_subset$waste_piles), by = 1),  # Range of x values\n  density = dpois(seq(0, max(post_treat_subset$waste_piles), by = 1), \n                  lambda = mean(post_treat_subset$waste_piles)) \n)\n\nggplot(post_treat_subset, aes(x = waste_piles)) +\n  geom_histogram(aes(y = ..density..), binwidth = 5, color = \"white\", fill = \"blue\", alpha = 0.7) + \n  geom_line(data = poisson_curve, aes(x = x, y = density), color = \"red\", size = 1) +  # Poisson curve\n  geom_density(color = \"green\", size = 1, adjust = 1.5) +  # KDE for a smoother curve\n  labs(\n    title = \"Plot of Empirical (Green) v. Theoretical (Red) Distributions\",\n    x = \"Waste Pile Counts\",\n    y = \"Density\"\n  ) +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\nm2_poisson &lt;- glm(waste_piles ~ treat,\n                  family = poisson(link = \"log\"),\n                  data = post_treat_subset)\n\nsumm(m2_poisson, model.fit = FALSE)\n\n\n\n\n\nObservations\n44\n\n\nDependent variable\nwaste_piles\n\n\nType\nGeneralized linear model\n\n\nFamily\npoisson\n\n\nLink\nlog\n\n\n\n\n  \n\n\n\n\n\nEst.\nS.E.\nz val.\np\n\n\n\n\n(Intercept)\n4.36\n0.02\n180.32\n0.00\n\n\ntreat\n-0.38\n0.04\n-10.09\n0.00\n\n\n\n Standard errors: MLE\n\n\n\n\n\n\n\n\n\n\n\nJust like OLS, we have some assumptions we need to check!\nWe first need to check for over dispersion. This is when our variance exceeds the mean.\n\ncheck_overdispersion(m2_poisson)\n\n# Overdispersion test\n\n       dispersion ratio =   5.873\n  Pearson's Chi-Squared = 246.680\n                p-value = &lt; 0.001\n\n\nIf our dispersion ratio was equal to 1, this would mean our mean is equal to our variance. Since our dispersion ratio is 5.873, and far greater than 1, we can see that our data has significant overdispersion. Poisson isn‚Äôt the model for us!\n\n\n\n\n\n\nTipDiscussion\n\n\n\nSince we ruled OLS and Poisson Regression out, talk with your neighbor about another potential model to try.\n\n\n\n\nModel 3: Log- OLS Regression\nLog Linear Regression is an alternative way to model count outcomes using regular OLS. The only difference is that we are transforming our outcome variable to be on the log scale.\n\nm3_log &lt;- lm(\n    log(waste_piles) ~ treat,\n    data = post_treat_subset)\n\nsumm(m3_log, model.fit = FALSE)\n\n\n\n\n\nObservations\n44\n\n\nDependent variable\nlog(waste_piles)\n\n\nType\nOLS linear regression\n\n\n\n\n  \n\n\n\n\n\nEst.\nS.E.\nt val.\np\n\n\n\n\n(Intercept)\n4.31\n0.08\n51.97\n0.00\n\n\ntreat\n-0.42\n0.12\n-3.57\n0.00\n\n\n\n Standard errors: OLS\n\n\n\n\n\n\n\n\n\n\n\nWe can compare our observed data to simulated data based on the model we just created (m3_log).\n\ncheck_predictions(m3_log, verbose = FALSE)\n\nWarning: Minimum value of original data is not included in the\n  replicated data.\n  Model may not capture the variation of the data.\n\n\n\n\n\n\n\n\n\n\n\nModel Recap\nLet‚Äôs take a look at the three models we have created thus far and compare their results.\n\nexport_summs(m1_ols, m2_poisson, m3_log,\n             model.names = c(\"OLS\",\"Poisson GLM\",\"Log(Outcome) OLS\"),\n             statistics = \"none\")\n\n\n\n\nOLSPoisson GLMLog(Outcome) OLS\n\n\n\n(Intercept)77.91 ***4.36 ***4.31 ***\n\n(4.16)¬†¬†¬†(0.02)¬†¬†¬†(0.08)¬†¬†¬†\n\ntreat-24.77 ***-0.38 ***-0.42 ***\n\n(5.88)¬†¬†¬†(0.04)¬†¬†¬†(0.12)¬†¬†¬†\n\n*** p &lt; 0.001; ** p &lt; 0.01; * p &lt; 0.05.\n\n\n\n\n\nLet‚Äôs do some quick math to compare treatment effect estimates\n\\[\\% \\Delta Y = \\left( e^{\\beta_1} - 1 \\right) \\times 100\\]\n\n# Calculate percent change in waste piles in each model:\n\nm1_change = (-24.77/77.91)*100   ## % change OLS = -31.8\nm2_change = (exp(-.38) - 1)*100  ## % change POIS = -31.6\nm3_change = (exp(-0.42) - 1)*100 ## % change LOG-OLS = -34.3\n\n\n\n\n\n\n\nTipTreatment Estimate Check In\n\n\n\nInterpret the treatment estimate for the log OLS.\n\n\nThe treatment estimate for log-OLS can be interpret as follows:\n\nThe model estimated that the treatment group had a 34% reduction in waste piles relative to the control group.\n\nLet‚Äôs also take a look at the covariate balance table across treatment conditions.\n\npost_treat_subset %&gt;% \n    select(treat, waste_piles, pre_count_avg) %&gt;% \n    tbl_summary(\n        by = treat,\n        statistic = list(all_continuous() ~ \"{mean} ({sd})\")) %&gt;% \n    modify_header(label ~ \"**Variable**\") %&gt;%\n    modify_spanning_header(c(\"stat_1\", \"stat_2\") ~ \"**Treatment**\") \n\n\n\n\n\n\n\n\n\n\n\n\nVariable\n\nTreatment\n\n\n\n0\nN = 221\n1\nN = 221\n\n\n\n\nwaste_piles\n78 (21)\n53 (18)\n\n\npre_count_avg\n94 (18)\n84 (26)\n\n\n\n1 Mean (SD)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTipDiscussion\n\n\n\nWhat potential issues do you notice with the pre_count_avg variable?\n\n\nThis takes us to our next model - controlling for pre_count_avg!\n\n\n\nModel 4: Log- OLS Regression with a control variable\n\nm4_control &lt;- lm(\n    log(waste_piles) ~ \n        treat +\n        log(pre_count_avg), ## Add a control\n    data = post_treat_subset)\n\nsumm(m4_control, model.fit = FALSE)\n\n\n\n\n\nObservations\n44\n\n\nDependent variable\nlog(waste_piles)\n\n\nType\nOLS linear regression\n\n\n\n\n  \n\n\n\n\n\nEst.\nS.E.\nt val.\np\n\n\n\n\n(Intercept)\n0.47\n0.35\n1.34\n0.19\n\n\ntreat\n-0.27\n0.06\n-4.44\n0.00\n\n\nlog(pre_count_avg)\n0.85\n0.08\n11.09\n0.00\n\n\n\n Standard errors: OLS\n\n\n\n\n\n\n\n\n\n\n\nLet‚Äôs compare our two log OLS Regression Models.\n\nexport_summs(m3_log, m4_control,\n             model.names = c(\"Log OLS Regression\",\"Log OLS Regression with a control variable\"),\n             statistics = \"none\")\n\n\n\n\nLog OLS RegressionLog OLS Regression with a control variable\n\n\n\n(Intercept)4.31 ***0.47¬†¬†¬†¬†\n\n(0.08)¬†¬†¬†(0.35)¬†¬†¬†\n\ntreat-0.42 ***-0.27 ***\n\n(0.12)¬†¬†¬†(0.06)¬†¬†¬†\n\nlog(pre_count_avg)¬†¬†¬†¬†¬†¬†¬†0.85 ***\n\n¬†¬†¬†¬†¬†¬†¬†(0.08)¬†¬†¬†\n\n*** p &lt; 0.001; ** p &lt; 0.01; * p &lt; 0.05.\n\n\n\n\nWe have now controlled for pre_count_avg, but let‚Äôs think about what other variables we could control for to improve our model. Other variables in our data include:\n\n\n\n\n\n\n\nLabel\nDescription\n\n\n\n\nrf_0_to_48_hours\nThe rainfall estimated between 0 and 48 hours prior to the start of the audit, derived from NASA MEERA-2 corrected precipitation estimates (mm)\n\n\nrf_49_to_168_hours\nThe rainfall estimated between 49 and 168 hours prior to the start of the audit, derived from NASA MEERA-2 corrected precipitation estimates (mm)\n\n\n\nThe environment doesn‚Äôt sit still during our experiment, so let‚Äôs control for it! We should account for events that might also affect trash burning during the treatment period (like rainfall)\nLet‚Äôs start by looking at a covariate balance table for across treatment conditions for average rain events.\n\npost_treat_subset %&gt;% \n    select(treat, waste_piles, rain_0_48hrs, rain_49_168hrs) %&gt;% \n    tbl_summary(\n        by = treat,\n        statistic = list(all_continuous() ~ \"{mean} ({sd})\")) %&gt;% \n    modify_header(label ~ \"**Variable**\") %&gt;% \n    modify_spanning_header(c(\"stat_1\", \"stat_2\") ~ \"**Treatment**\") \n\n\n\n\n\n\n\n\n\n\n\n\nVariable\n\nTreatment\n\n\n\n0\nN = 221\n1\nN = 221\n\n\n\n\nwaste_piles\n78 (21)\n53 (18)\n\n\nrain_0_48hrs\n0.18 (0.15)\n0.20 (0.16)\n\n\nrain_49_168hrs\n0.17 (0.17)\n0.17 (0.17)\n\n\n\n1 Mean (SD)\n\n\n\n\n\n\n\n\n\nm5_control2 &lt;- lm(\n    log(waste_piles) ~ \n        treat +\n        log(pre_count_avg) +\n        rain_0_48hrs + rain_49_168hrs,\n    data = post_treat_subset)\n\nsumm(m5_control2,\n     model.fit = FALSE,\n     robust = \"HC2\")  # Heteroskedasticity robust standard error adj.\n\n\n\n\n\nObservations\n44\n\n\nDependent variable\nlog(waste_piles)\n\n\nType\nOLS linear regression\n\n\n\n\n  \n\n\n\n\n\nEst.\nS.E.\nt val.\np\n\n\n\n\n(Intercept)\n0.43\n0.45\n0.95\n0.35\n\n\ntreat\n-0.27\n0.06\n-4.29\n0.00\n\n\nlog(pre_count_avg)\n0.86\n0.07\n12.51\n0.00\n\n\nrain_0_48hrs\n-0.02\n0.85\n-0.03\n0.98\n\n\nrain_49_168hrs\n0.07\n0.79\n0.09\n0.93\n\n\n\n Standard errors: Robust, type = HC2\n\n\n\n\n\n\n\n\n\n\n\nNow that we have two fixed effects models, lets look at our results side by side.\n\nexport_summs(m4_control, m5_control2, robust = \"HC2\",\n             model.names = c(\"M4 - Pre-treat\",\"M5 - Add Rain\"),\n             statistics = \"none\")\n\n\n\n\nM4 - Pre-treatM5 - Add Rain\n\n\n\n(Intercept)0.47¬†¬†¬†¬†0.43¬†¬†¬†¬†\n\n(0.33)¬†¬†¬†(0.45)¬†¬†¬†\n\ntreat-0.27 ***-0.27 ***\n\n(0.06)¬†¬†¬†(0.06)¬†¬†¬†\n\nlog(pre_count_avg)0.85 ***0.86 ***\n\n(0.07)¬†¬†¬†(0.07)¬†¬†¬†\n\nrain_0_48hrs¬†¬†¬†¬†¬†¬†¬†-0.02¬†¬†¬†¬†\n\n¬†¬†¬†¬†¬†¬†¬†(0.85)¬†¬†¬†\n\nrain_49_168hrs¬†¬†¬†¬†¬†¬†¬†0.07¬†¬†¬†¬†\n\n¬†¬†¬†¬†¬†¬†¬†(0.79)¬†¬†¬†\n\nStandard errors are heteroskedasticity robust. *** p &lt; 0.001; ** p &lt; 0.01; * p &lt; 0.05.\n\n\n\n\n\n\n\n\n\n\nTipAre neighborhoods independent?\n\n\n\nWe have another issue which may bias our treatment effect - the neighborhoods were paired together. We have to account for this paired data feature which is called clustered data. Clustered data violates the regression assumption that observations are independent and identically distributed (i.i.d). In-other-words, the data has within group dependencies, paired neighborhoods will likely be more similar than un-paired neighborhoods.\n\n\nLets account for these paired neighborhoods by adding a standard error adjustment based on our clusters. We can make this adjustment with the fixed effects model we just created (m5_control2) .\n\nsumm(m5_control2,\n     robust = \"HC2\",\n     cluster = \"pair_id\", # Add SE adj. based on pair-clusters\n     model.fit = FALSE)\n\n\n\n\n\nObservations\n44\n\n\nDependent variable\nlog(waste_piles)\n\n\nType\nOLS linear regression\n\n\n\n\n  \n\n\n\n\n\nEst.\nS.E.\nt val.\np\n\n\n\n\n(Intercept)\n0.43\n0.49\n0.88\n0.39\n\n\ntreat\n-0.27\n0.07\n-3.63\n0.00\n\n\nlog(pre_count_avg)\n0.86\n0.07\n12.44\n0.00\n\n\nrain_0_48hrs\n-0.02\n0.97\n-0.03\n0.98\n\n\nrain_49_168hrs\n0.07\n0.94\n0.07\n0.94\n\n\n\n Standard errors: Cluster-robust, type = HC2\n\n\n\n\n\n\n\n\n\n\n\nWhat are the key takeaways from this summary? Discuss with your neighbor!"
  },
  {
    "objectID": "course-materials/labs/week2_test.html",
    "href": "course-materials/labs/week2_test.html",
    "title": "week3.qmd",
    "section": "",
    "text": "Figure: Map of Treatment Assignment Zones in Nansana, Uganda\n* SOURCE: Figure from Buntaine et al., 2024 (S5; Supplemental Materials)\n\n\n\nOpen source data & applied study:\nOpen access data was utilized for this class exercise from the study (Buntaine et al., 2024):\n\nBuntaine, M. T., Komakech, P., & Shen, S. V. (2024). Social competition drives collective action to reduce informal waste burning in Uganda. Proceedings of the National Academy of Sciences, 121(23). https://doi.org/10.1073/pnas.2319712121\n\n\n\nGoals of this exercise\n\nStart with a simple OLS model\nDemonstrate the rationale for estimating progressively complex models\nIllustrate a causal inference approach (i.e., identify robust estimators)\nConclusion: What is the take-home message of the Lab-1 exercise? . . .\nEstimating complex models is a relatively easy part of our work as data scientists‚Ä¶\n\nMaking informed specification decisions & communicating results is the hard part (i.e., the science side of data science)!\n\n\n\n\nLet‚Äôs get started!\nLoad packages and data:\n\nlibrary(tidyverse)   # Keeping things tidy \n\n‚îÄ‚îÄ Attaching core tidyverse packages ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ tidyverse 2.0.0 ‚îÄ‚îÄ\n‚úî dplyr     1.1.4     ‚úî readr     2.1.5\n‚úî forcats   1.0.1     ‚úî stringr   1.5.2\n‚úî ggplot2   4.0.0     ‚úî tibble    3.3.0\n‚úî lubridate 1.9.4     ‚úî tidyr     1.3.1\n‚úî purrr     1.1.0     \n‚îÄ‚îÄ Conflicts ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ tidyverse_conflicts() ‚îÄ‚îÄ\n‚úñ dplyr::filter() masks stats::filter()\n‚úñ dplyr::lag()    masks stats::lag()\n‚Ñπ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(janitor)     # Housekeeping \n\n\nAttaching package: 'janitor'\n\nThe following objects are masked from 'package:stats':\n\n    chisq.test, fisher.test\n\nlibrary(here)        # Location, location, location \n\nhere() starts at /Users/annalieseadams/Git/W26-Policy-Eval\n\n#install.packages(\"jtools\")\nlibrary(jtools)      # Pretty regression output \nlibrary(gt)          # Tables \n\nWarning: package 'gt' was built under R version 4.5.2\n\nlibrary(gtsummary)   # Table for checking balance \n\nWarning: package 'gtsummary' was built under R version 4.5.2\n\n#install.packages(\"performance\")\nlibrary(performance) # Check model diagnostics \n\n\n# Read in waste pile count data\n\ncounts_gpx &lt;- read_csv(\"https://raw.github.com/garberadamc/Lab1-EDS241-Buntaine24/main/data/waste_pile_counts_gpx.csv\") %&gt;% \n    rename(\"waste_piles\" = \"total\",\n           \"rain_0_48hrs\" = \"rf_0_to_48_hours\",\n           \"rain_49_168hrs\" = \"rf_49_to_168_hours\") %&gt;% \n    filter(survey%in%c(\n      \"post_treatment_1\",\"post_treatment_2\",\"post_treatment_3\",\n      \"post_treatment_4\",\"post_treatment_5\"))\n\nRows: 308 Columns: 8\n‚îÄ‚îÄ Column specification ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\nDelimiter: \",\"\nchr (1): survey\ndbl (7): zoneid, total, rf_0_to_48_hours, rf_49_to_168_hours, pre_count_avg,...\n\n‚Ñπ Use `spec()` to retrieve the full column specification for this data.\n‚Ñπ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n# Select subset of post-treatment periods (remove time point 5)\npost_treat_subset &lt;- counts_gpx %&gt;% \n   filter(survey == \"post_treatment_4\")\n\nNow that we got our data loaded in, lets take a look at what these variables mean."
  },
  {
    "objectID": "course-materials/labs/week2_notes.html",
    "href": "course-materials/labs/week2_notes.html",
    "title": "Lab 2: Generalize Fast!",
    "section": "",
    "text": "Map of Treatment Assignment Zones in Nansana, Uganda\n\n* SOURCE: Figure from Buntaine et al., 2024 (S5; Supplemental Materials)\n\n\nOpen source data & applied study:\nOpen access data was utilized for this class exercise from the study (Buntaine et al., 2024):\n\nBuntaine, M. T., Komakech, P., & Shen, S. V. (2024). Social competition drives collective action to reduce informal waste burning in Uganda. Proceedings of the National Academy of Sciences, 121(23). https://doi.org/10.1073/pnas.2319712121\n\n\n\nGoals of this exercise\n\nStart with a simple OLS model\nDemonstrate the rationale for estimating progressively complex models\nIllustrate a causal inference approach (i.e., identify robust estimators)\nConclusion: What is the take-home message of the Lab-1 exercise? . . .\nEstimating complex models is a relatively easy part of our work as data scientists‚Ä¶\n\nMaking informed specification decisions & communicating results is the hard part (i.e., the science side of data science)!\n\n\n\n\nLet‚Äôs get started!\nLoad packages and data:\n\nlibrary(tidyverse)   # Keeping things tidy \nlibrary(janitor)     # Housekeeping \nlibrary(here)        # Location, location, location \n#install.packages(\"jtools\")\nlibrary(jtools)      # Pretty regression output \nlibrary(gt)          # Tables \nlibrary(gtsummary)   # Table for checking balance \n#install.packages(\"performance\")\nlibrary(performance) # Check model diagnostics \n#install.packages(\"see\") # Visualization engine for performance package\n#install.packages(\"huxtable\") # Package required for export_summs\n\n\n# Read in waste pile count data\ncounts_gpx &lt;- read_csv(here::here(\"course-materials\", \"labs\", \"data\", \"waste_pile_counts_gpx.csv\")) %&gt;% \n    rename(\"waste_piles\" = \"total\",\n           \"rain_0_48hrs\" = \"rf_0_to_48_hours\",\n           \"rain_49_168hrs\" = \"rf_49_to_168_hours\") %&gt;% \n    filter(survey%in%c(\n      \"post_treatment_1\",\"post_treatment_2\",\"post_treatment_3\",\n      \"post_treatment_4\",\"post_treatment_5\"))\n\n# Select subset of post-treatment periods \npost_treat_subset &lt;- counts_gpx %&gt;% \n   filter(survey == \"post_treatment_4\")\n\nhead(post_treat_subset)\n\n# A tibble: 6 √ó 8\n  zoneid survey    waste_piles rain_0_48hrs rain_49_168hrs pre_count_avg pair_id\n   &lt;dbl&gt; &lt;chr&gt;           &lt;dbl&gt;        &lt;dbl&gt;          &lt;dbl&gt;         &lt;dbl&gt;   &lt;dbl&gt;\n1   1011 post_tre‚Ä¶          97         0              0.4           97      2021\n2   1012 post_tre‚Ä¶          12         0              0.4           12.5    2022\n3   1013 post_tre‚Ä¶          58         0              0.4           87.5    2022\n4   1014 post_tre‚Ä¶          88         0              0.4          110.     2021\n5   1015 post_tre‚Ä¶          53         0.04           0.36         110.     2017\n6   1016 post_tre‚Ä¶          18         0              0.4           23      2017\n# ‚Ñπ 1 more variable: treat &lt;dbl&gt;\n\n\nNow that we got our data loaded in, lets take a look at what these variables mean.\n\n\n\n\n\n\n\nLabel\nDescription\n\n\n\n\nzoneid\nZone or neighborhood ID: The observational unit (\\(N=44\\))\n\n\nwaste_piles\nThe outcome variable (\\(Y\\)): The number of waste pile burns recorded (Range: 5, 125)\n\n\ntreat\nThe treatment assignment variable (0 = Control Group; 1 = Treatment Group)\n\n\nrf_0_to_48_hours\nThe rainfall estimated between 0 and 48 hours prior to the start of the audit, derived from NASA MEERA-2 corrected precipitation estimates (mm)\n\n\nrf_49_to_168_hours\nThe rainfall estimated between 49 and 168 hours prior to the start of the audit, derived from NASA MEERA-2 corrected precipitation estimates (mm)\n\n\npre_count_avg\nThe average number of burnt piles in the pre-treatment audits, spread to zone level\n\n\npair_id\nThe unique ID assigned to each pair of zones for assignment to treatment\n\n\n\n\n\nModel 1: Simple OLS (Ordinary Least Squares) estimator\nOLS estimator (specification):\n\\[waste\\ piles =\\beta_0 + \\beta_1(treat) + \\epsilon\\]\n\nm1_ols &lt;- lm(\n    waste_piles ~ treat,\n    data = post_treat_subset)\n\nsumm(m1_ols, model.fit = FALSE) # model.fit = TRUE explains accuracy reports like R2\n\n\n\n\n\nObservations\n44\n\n\nDependent variable\nwaste_piles\n\n\nType\nOLS linear regression\n\n\n\n\n  \n\n\n\n\n\nEst.\nS.E.\nt val.\np\n\n\n\n\n(Intercept)\n77.91\n4.16\n18.73\n0.00\n\n\ntreat\n-24.77\n5.88\n-4.21\n0.00\n\n\n\n Standard errors: OLS\n\n\n\n\n\n\n\n\n\n\n\nInterpretatation: - interecept coeff: On average, the control group had 78 waste piles.\n\ntreat coeff: On average, the treatment groups had 25 less waste piles than the control group. or : The treatment reduced the average number of trash piles by approximately 25 piles\ninterecept SE: The true control average likely falls within 4.16 trash piles of 78.\ntreat SE: the true treatment effect likely falls within about +- 5.88 trash piles of -24.77\n\nt value (est/SE): - larger t value = stronger evidence the effect is real.\n\nAre we making reasonable assumptions?\nRemember questioning (and testing) assumptions is the heart of causal inference!We want to make sure our assumptions our reasonable. To do so, let‚Äôstake a quick look at our outcome variable waste piles, and then look at the normality of our residuals.\n\npost_treat_subset %&gt;%\n  ggplot(aes(x = waste_piles)) +\n  geom_histogram(binwidth = 5, fill = \"blue\", color = \"white\", alpha = 0.7) +\n  labs(title = \"Histogram of Waste Piles\",\n    x = \"Waste Piles (counts)\",\n    y = \"Count\") +\n    theme_minimal()\n\n\n\n\n\n\n\n\nWhile our data isn‚Äôt a perfect bell curve, it also isn‚Äôt severely skewed. Let‚Äôs do another assumption check (normality of residuals) to help us decide if OLS is the right model for us.\nTo do so, we will make a QQ plot displaying residuals (y-axis) compared to the normal distribution (x-axis).\n\ncheck_model(m1_ols,  check = \"qq\" )\n\n\n\n\n\n\n\n# de trended qq plot, where rather than y = x ( goal is a diag line), we want y - x = 0 (goal is a horizontal line). Makes errors easier to see\n\nHmm, it doesn‚Äôt seem like the OLS assumption, normality of residuals is a good fit for our data. Onto the next model!\n\n\n\n\n\n\nTipDiscussion\n\n\n\nSince we ruled OLS out, talk with your neighbor about another potential model to try. What kind of variable is our outcome variable? How might that inform what model we try next?\n\n\nWe can ‚Äúrelax‚Äù our assumptions by using a Generalized Linear model! Specifically, we will relax the assumption that the residuals are normally distributed.\n\n\n\n\n\n\nWarningWarning!\n\n\n\nWhen we relax an assumption and make our model more flexible, the statistics and interpretation become more complex!\n\n\nAs you discussed with your partner, our dependent variable, waste_piles is a count variable, not a continuous variable. Therefore, it‚Äôs best we pick a model that explicitly accounts for count outcomes. In walks the poisson regression model!\n\n\n\nModel 2 : Poisson Generalized Linear Regression Model\nWe are going to assume Y follows a Poisson distribution with non negative integers (counts!). To do so, we need to think about the assumptions the Poisson regression makes. This model assumes that variance (dispersion) is proportional to the mean. Does our data match the theoretical distribution proposed? Let‚Äôs check!\n\nlambda_hat &lt;- mean(post_treat_subset$waste_piles)\n\n# Generate theoretical poisson data\npoisson_curve &lt;- tibble(\n  # Create a sequence of integers from 0 to max waste pile count\n  x = seq(0, max(post_treat_subset$waste_piles), by = 1),  \n  \n  # Calculate the probability (density) for each x value using the poisson formula\n  density = dpois(seq(0, max(post_treat_subset$waste_piles), by = 1), \n                  lambda = mean(post_treat_subset$waste_piles)) \n)\n\nggplot(post_treat_subset, aes(x = waste_piles)) +\n  # ..desnity .. resizes histogram bars so they can be compared to probability curve\n  geom_histogram(aes(y = ..density..), binwidth = 5, color = \"white\", fill = \"blue\", alpha = 0.7) + \n  # red line is showing us what poisson dist w averge of our data should look like\n  geom_line(data = poisson_curve, aes(x = x, y = density), color = \"red\", size = 1) +  # Poisson curve\n  # smoothed density line of our histogram \n  geom_density(color = \"green\", size = 1, adjust = 1.5) +  # KDE for a smoother curve\n  labs(\n    title = \"Plot of Empirical (Green) v. Theoretical (Red) Distributions\",\n    x = \"Waste Pile Counts\",\n    y = \"Density\"\n  ) +\n  theme_minimal()\n\n\n\n\n\n\n\n\nRed is what our dist should look like for poisson. Green is our actual data. Not a great fit! Our variance is much larger than our mean (overdispersion).\n\nm2_poisson &lt;- glm(waste_piles ~ treat,\n                  family = poisson(link = \"log\"), # log bc it resticts outcome to positive values and changes model to be multiplicative ( percent change formula when we interpret)\n                  data = post_treat_subset)\n\nsumm(m2_poisson, model.fit = FALSE)\n\n\n\n\n\nObservations\n44\n\n\nDependent variable\nwaste_piles\n\n\nType\nGeneralized linear model\n\n\nFamily\npoisson\n\n\nLink\nlog\n\n\n\n\n  \n\n\n\n\n\nEst.\nS.E.\nz val.\np\n\n\n\n\n(Intercept)\n4.36\n0.02\n180.32\n0.00\n\n\ntreat\n-0.38\n0.04\n-10.09\n0.00\n\n\n\n Standard errors: MLE\n\n\n\n\n\n\n\n\n\n\n\nTo get percent change: (e^beta_1 -1) * 100 - z test here bc variance is assumed/known from dist. IN OLS it is estimated from data but not known. - p value for int: H0: is log(control mean) = 0, alt: not euqal to zero. p = 0 means we reject null hypothesis that control has 1 waste pile\nInterpret:\nIntercept: On average, our control group has 78 (exp(4.36)) waste piles. Treat coeff: exp(-.038) = .68. .68 - 1 = -.32. This model estimates that treatment reduced waste burning by 32% in treated neighborhoods.\nJust like OLS, we have some assumptions we need to check!\nWe first need to check for over dispersion. This is when our variance exceeds the mean.\n\ncheck_overdispersion(m2_poisson)\n\n# Overdispersion test\n\n       dispersion ratio =   5.873\n  Pearson's Chi-Squared = 246.680\n                p-value = &lt; 0.001\n\n\nIf our dispersion ratio was equal to 1, this would mean our mean is equal to our variance. Since our dispersion ratio is 5.873, and far greater than 1, we can see that our data has significant overdispersion. Poisson isn‚Äôt the model for us!\n\n\n\n\n\n\nTipDiscussion\n\n\n\nSince we ruled OLS and Poisson Regression out, talk with your neighbor about another potential model to try.\n\n\n** Note, mabe negative binomial would be good here. Negative binomial assumes variance &gt;mean\n\n\nModel 3: Log- OLS Regression\nLog Linear Regression is an alternative way to model count outcomes using regular OLS. The only difference is that we are transforming our outcome variable to be on the log scale.\nWhy ? Helps to fix non normality issue! Prevents negative outcomes! percent change interpretation which is often better than direct counts ( more interpretable)\n\nm3_log &lt;- lm(\n    log(waste_piles) ~ treat,\n    data = post_treat_subset)\n\nsumm(m3_log, model.fit = FALSE)\n\n\n\n\n\nObservations\n44\n\n\nDependent variable\nlog(waste_piles)\n\n\nType\nOLS linear regression\n\n\n\n\n  \n\n\n\n\n\nEst.\nS.E.\nt val.\np\n\n\n\n\n(Intercept)\n4.31\n0.08\n51.97\n0.00\n\n\ntreat\n-0.42\n0.12\n-3.57\n0.00\n\n\n\n Standard errors: OLS\n\n\n\n\n\n\n\n\n\n\n\ninterpretation:\nintercept: exp(4.31) = 74.44. There are 74 waste piles on average in the control group.\ntreat coeff: exp(-0.42) = .656. .657 - 1 = .35. This model estimates that treatment reduced waste burning by an average of 35% in treated neighborhoods.\nWe can compare our observed data to simulated data based on the model we just created (m3_log).\n\ncheck_predictions(m3_log, verbose = FALSE)\n\nWarning: Minimum value of original data is not included in the\n  replicated data.\n  Model may not capture the variation of the data.\n\n\n\n\n\n\n\n\n\ngenerating fake waste pile count data and using our model.\n\n\nModel Recap\nLet‚Äôs take a look at the three models we have created thus far and compare their results.\n\nexport_summs(m1_ols, m2_poisson, m3_log,\n             model.names = c(\"OLS\",\"Poisson GLM\",\"Log(Outcome) OLS\"),\n             statistics = \"none\")\n\n\n\n\nOLSPoisson GLMLog(Outcome) OLS\n\n\n\n(Intercept)77.91 ***4.36 ***4.31 ***\n\n(4.16)¬†¬†¬†(0.02)¬†¬†¬†(0.08)¬†¬†¬†\n\ntreat-24.77 ***-0.38 ***-0.42 ***\n\n(5.88)¬†¬†¬†(0.04)¬†¬†¬†(0.12)¬†¬†¬†\n\n*** p &lt; 0.001; ** p &lt; 0.01; * p &lt; 0.05.\n\n\n\n\n\nLet‚Äôs do some quick math to compare treatment effect estimates\n\\[\\% \\Delta Y = \\left( e^{\\beta_1} - 1 \\right) \\times 100\\]\n\n# Calculate percent change in waste piles in each model:\n\nm1_change = (-24.77/77.91)*100   ## % change OLS = -31.8\nm2_change = (exp(-.38) - 1)*100  ## % change POIS = -31.6\nm3_change = (exp(-0.42) - 1)*100 ## % change LOG-OLS = -34.3\n\nCould convert everything to trash pile counts, or to percentages.\nTo percentages: OLS -&gt; percentage ( -24.77/77.91)\nTo trash pile counts:\n\npoisson -&gt; counts: exp(4.36), exp(-0.38)\nlog OLS -&gt; counts: exp(4.31), exp(-0.42)\n\n\n\n\n\n\n\nTipTreatment Estimate Check In\n\n\n\nInterpret the treatment estimate for the log OLS.\n\n\nThe treatment estimate for log-OLS can be interpret as follows:\n\nThe model estimated that the treatment group had, on average, a 34% reduction in waste piles relative to the control group.\n\nLet‚Äôs also take a look at the covariate balance table across treatment conditions.\n\npost_treat_subset %&gt;% \n    select(treat, waste_piles, pre_count_avg) %&gt;% \n    tbl_summary(\n        by = treat,\n        statistic = list(all_continuous() ~ \"{mean} ({sd})\")) %&gt;% \n    modify_header(label ~ \"**Variable**\") %&gt;%\n    modify_spanning_header(c(\"stat_1\", \"stat_2\") ~ \"**Treatment**\") \n\n\n\n\n\n\n\n\n\n\n\n\nVariable\n\nTreatment\n\n\n\n0\nN = 221\n1\nN = 221\n\n\n\n\nwaste_piles\n78 (21)\n53 (18)\n\n\npre_count_avg\n94 (18)\n84 (26)\n\n\n\n1 Mean (SD)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTipDiscussion\n\n\n\nWhat potential issues do you notice with the pre_count_avg variable?\n\n\nThis takes us to our next model - controlling for pre_count_avg!\n\n\n\nModel 4: Log- OLS Regression with a control variable\n\nm4_control &lt;- lm(\n    log(waste_piles) ~ \n        treat +\n        log(pre_count_avg), ## Add a control\n    data = post_treat_subset)\n\nsumm(m4_control, model.fit = FALSE)\n\n\n\n\n\nObservations\n44\n\n\nDependent variable\nlog(waste_piles)\n\n\nType\nOLS linear regression\n\n\n\n\n  \n\n\n\n\n\nEst.\nS.E.\nt val.\np\n\n\n\n\n(Intercept)\n0.47\n0.35\n1.34\n0.19\n\n\ntreat\n-0.27\n0.06\n-4.44\n0.00\n\n\nlog(pre_count_avg)\n0.85\n0.08\n11.09\n0.00\n\n\n\n Standard errors: OLS\n\n\n\n\n\n\n\n\n\n\n\nInterpretation:\n\nInterecept: Our model estimates the expected waste piles for control areas at a pre-treatment average of 1 pile to be exp(0.47) ‚âà 1.6 waste piles, though this is not statistically significant (p = 0.19).\ntreat: The treatment coefficient of -0.27 indicates that, after controlling for baseline waste levels, the treatment reduces waste piles by an average of 24%.\nlog(pre_count_aveage): The coefficient of 0.85 on log(pre_count_avg) means that a 1% increase in baseline waste ( pre count avg) is associated with a 0.85% increase in post-treatment waste.( in log - log models, the coefficient tells you the percent directly! )\n\nLet‚Äôs compare our two log OLS Regression Models.\n\nexport_summs(m3_log, m4_control,\n             model.names = c(\"Log OLS Regression\",\"Log OLS Regression with a control variable\"),\n             statistics = \"none\")\n\n\n\n\nLog OLS RegressionLog OLS Regression with a control variable\n\n\n\n(Intercept)4.31 ***0.47¬†¬†¬†¬†\n\n(0.08)¬†¬†¬†(0.35)¬†¬†¬†\n\ntreat-0.42 ***-0.27 ***\n\n(0.12)¬†¬†¬†(0.06)¬†¬†¬†\n\nlog(pre_count_avg)¬†¬†¬†¬†¬†¬†¬†0.85 ***\n\n¬†¬†¬†¬†¬†¬†¬†(0.08)¬†¬†¬†\n\n*** p &lt; 0.001; ** p &lt; 0.01; * p &lt; 0.05.\n\n\n\n\nWe have now controlled for pre_count_avg, but let‚Äôs think about what other variables we could control for to improve our model. Other variables in our data include:\n\n\n\n\n\n\n\nLabel\nDescription\n\n\n\n\nrf_0_to_48_hours\nThe rainfall estimated between 0 and 48 hours prior to the start of the audit, derived from NASA MEERA-2 corrected precipitation estimates (mm)\n\n\nrf_49_to_168_hours\nThe rainfall estimated between 49 and 168 hours prior to the start of the audit, derived from NASA MEERA-2 corrected precipitation estimates (mm)\n\n\n\nThe environment doesn‚Äôt sit still during our experiment, so let‚Äôs control for it! We should account for events that might also affect trash burning during the treatment period (like rainfall)\nLet‚Äôs start by looking at a covariate balance table for across treatment conditions for average rain events.\n\npost_treat_subset %&gt;% \n    select(treat, waste_piles, rain_0_48hrs, rain_49_168hrs) %&gt;% \n    tbl_summary(\n        by = treat,\n        statistic = list(all_continuous() ~ \"{mean} ({sd})\")) %&gt;% \n    modify_header(label ~ \"**Variable**\") %&gt;% \n    modify_spanning_header(c(\"stat_1\", \"stat_2\") ~ \"**Treatment**\") \n\n\n\n\n\n\n\n\n\n\n\n\nVariable\n\nTreatment\n\n\n\n0\nN = 221\n1\nN = 221\n\n\n\n\nwaste_piles\n78 (21)\n53 (18)\n\n\nrain_0_48hrs\n0.18 (0.15)\n0.20 (0.16)\n\n\nrain_49_168hrs\n0.17 (0.17)\n0.17 (0.17)\n\n\n\n1 Mean (SD)\n\n\n\n\n\n\n\n\n\nm5_control2 &lt;- lm(\n    log(waste_piles) ~ \n        treat +\n        log(pre_count_avg) +\n        rain_0_48hrs + rain_49_168hrs, # dont take log of rain bc rain can be 0, diff relationship -&gt; additive makes more sense for rainfall( each mm of rain adds certain amount of waste)\n    data = post_treat_subset)\n\nsumm(m5_control2,\n     model.fit = FALSE,\n     robust = \"HC2\")  # Heteroskedasticity robust standard error adj.\n\n\n\n\n\nObservations\n44\n\n\nDependent variable\nlog(waste_piles)\n\n\nType\nOLS linear regression\n\n\n\n\n  \n\n\n\n\n\nEst.\nS.E.\nt val.\np\n\n\n\n\n(Intercept)\n0.43\n0.45\n0.95\n0.35\n\n\ntreat\n-0.27\n0.06\n-4.29\n0.00\n\n\nlog(pre_count_avg)\n0.86\n0.07\n12.51\n0.00\n\n\nrain_0_48hrs\n-0.02\n0.85\n-0.03\n0.98\n\n\nrain_49_168hrs\n0.07\n0.79\n0.09\n0.93\n\n\n\n Standard errors: Robust, type = HC2\n\n\n\n\n\n\n\n\n\n\n\nIntercept (0.43): Our model estimates the expected waste piles for control areas with a pre-treatment average of 1 pile and no rainfall to be exp(0.43) ‚âà 1.5 waste piles, though this is not statistically significant (p = 0.35).\nTreatment coefficient (-0.27): The treatment coefficient of -0.27 indicates that, after controlling for baseline waste levels and rainfall, the treatment reduces waste piles by an average of 24% (p &lt; 0.001).\nlog(pre_count_avg) coefficient (0.86): The coefficient of 0.86 on log(pre_count_avg) means that a 1% increase in baseline waste is associated with a 0.86% increase in post-treatment waste.\nrain_0_48hrs coefficient (-0.02): The coefficient of -0.02 suggests that each additional millimeter of rain in the 0-48 hours before measurement is associated with a 2% decrease in waste piles, though this effect is not statistically significant (p = 0.98).\nrain_49_168hrs coefficient (0.07): The coefficient of 0.07 suggests that each additional millimeter of rain in the 49-168 hours before measurement is associated with a 7% increase in waste piles, though this effect is not statistically significant (p = 0.93).\nNow that we have two fixed effects models, lets look at our results side by side.\n\nexport_summs(m4_control, m5_control2, robust = \"HC2\",\n             model.names = c(\"M4 - Pre-treat\",\"M5 - Add Rain\"),\n             statistics = \"none\")\n\n\n\n\nM4 - Pre-treatM5 - Add Rain\n\n\n\n(Intercept)0.47¬†¬†¬†¬†0.43¬†¬†¬†¬†\n\n(0.33)¬†¬†¬†(0.45)¬†¬†¬†\n\ntreat-0.27 ***-0.27 ***\n\n(0.06)¬†¬†¬†(0.06)¬†¬†¬†\n\nlog(pre_count_avg)0.85 ***0.86 ***\n\n(0.07)¬†¬†¬†(0.07)¬†¬†¬†\n\nrain_0_48hrs¬†¬†¬†¬†¬†¬†¬†-0.02¬†¬†¬†¬†\n\n¬†¬†¬†¬†¬†¬†¬†(0.85)¬†¬†¬†\n\nrain_49_168hrs¬†¬†¬†¬†¬†¬†¬†0.07¬†¬†¬†¬†\n\n¬†¬†¬†¬†¬†¬†¬†(0.79)¬†¬†¬†\n\nStandard errors are heteroskedasticity robust. *** p &lt; 0.001; ** p &lt; 0.01; * p &lt; 0.05.\n\n\n\n\n\n\n\n\n\n\nTipAre neighborhoods independent?\n\n\n\nWe have another issue which may bias our treatment effect - the neighborhoods were paired together. We have to account for this paired data feature which is called clustered data. Clustered data violates the regression assumption that observations are independent and identically distributed (i.i.d). In-other-words, the data has within group dependencies, paired neighborhoods will likely be more similar than un-paired neighborhoods.\n\n\nLets account for these paired neighborhoods by adding a standard error adjustment based on our clusters. We can make this adjustment with the fixed effects model we just created (m5_control2) .\n\nsumm(m5_control2,\n     robust = \"HC2\", # HC2 is a specific type of robust standard errors (bias-corrected)\n     cluster = \"pair_id\", # Add SE adj. based on pair-clusters\n     model.fit = FALSE)\n\n\n\n\n\nObservations\n44\n\n\nDependent variable\nlog(waste_piles)\n\n\nType\nOLS linear regression\n\n\n\n\n  \n\n\n\n\n\nEst.\nS.E.\nt val.\np\n\n\n\n\n(Intercept)\n0.43\n0.49\n0.88\n0.39\n\n\ntreat\n-0.27\n0.07\n-3.63\n0.00\n\n\nlog(pre_count_avg)\n0.86\n0.07\n12.44\n0.00\n\n\nrain_0_48hrs\n-0.02\n0.97\n-0.03\n0.98\n\n\nrain_49_168hrs\n0.07\n0.94\n0.07\n0.94\n\n\n\n Standard errors: Cluster-robust, type = HC2\n\n\n\n\n\n\n\n\n\n\n\nWhat are the key takeaways from this summary? Discuss with your neighbor!"
  }
]